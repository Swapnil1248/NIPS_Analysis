{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>EventType</th>\n",
       "      <th>PdfName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "      <th>AbstractClean</th>\n",
       "      <th>PaperTextClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5677</td>\n",
       "      <td>Double or Nothing: Multiplicative Incentive Me...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5677-double-or-nothing-multiplicative-incentiv...</td>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "      <td>crowdsourcing gained immense popularity machin...</td>\n",
       "      <td>double nothing multiplicative incentive mechan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5941</td>\n",
       "      <td>Learning with Symmetric Label Noise: The Impor...</td>\n",
       "      <td>Spotlight</td>\n",
       "      <td>5941-learning-with-symmetric-label-noise-the-i...</td>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "      <td>convex potential minimisation de facto approac...</td>\n",
       "      <td>learning symmetric label noise importance unhi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6019</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalization</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6019-algorithmic-stability-and-uniform-general...</td>\n",
       "      <td>One of the central questions in statistical le...</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalizati...</td>\n",
       "      <td>one central questions statistical learning the...</td>\n",
       "      <td>algorithmic stability uniform generalization i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6035</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6035-adaptive-low-complexity-sequential-infere...</td>\n",
       "      <td>We develop a sequential low-complexity inferen...</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "      <td>develop sequential lowcomplexity inference pro...</td>\n",
       "      <td>adaptive lowcomplexity sequential inference di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Id                                              Title  \\\n",
       "0           0  5677  Double or Nothing: Multiplicative Incentive Me...   \n",
       "1           1  5941  Learning with Symmetric Label Noise: The Impor...   \n",
       "2           2  6019   Algorithmic Stability and Uniform Generalization   \n",
       "3           3  6035  Adaptive Low-Complexity Sequential Inference f...   \n",
       "\n",
       "   EventType                                            PdfName  \\\n",
       "0     Poster  5677-double-or-nothing-multiplicative-incentiv...   \n",
       "1  Spotlight  5941-learning-with-symmetric-label-noise-the-i...   \n",
       "2     Poster  6019-algorithmic-stability-and-uniform-general...   \n",
       "3     Poster  6035-adaptive-low-complexity-sequential-infere...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "2  One of the central questions in statistical le...   \n",
       "3  We develop a sequential low-complexity inferen...   \n",
       "\n",
       "                                           PaperText  \\\n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...   \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...   \n",
       "2  Algorithmic Stability and Uniform Generalizati...   \n",
       "3  Adaptive Low-Complexity Sequential Inference f...   \n",
       "\n",
       "                                       AbstractClean  \\\n",
       "0  crowdsourcing gained immense popularity machin...   \n",
       "1  convex potential minimisation de facto approac...   \n",
       "2  one central questions statistical learning the...   \n",
       "3  develop sequential lowcomplexity inference pro...   \n",
       "\n",
       "                                      PaperTextClean  \n",
       "0  double nothing multiplicative incentive mechan...  \n",
       "1  learning symmetric label noise importance unhi...  \n",
       "2  algorithmic stability uniform generalization i...  \n",
       "3  adaptive lowcomplexity sequential inference di...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id=list(df['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import numpy.core.defchararray as npd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test1:\n",
    "    def _iter_nouns(self, sent):\n",
    "        '''\n",
    "        INPUT: SentCustomProperties\n",
    "        OUTPUT: str\n",
    "\n",
    "        Iterates through each token of spacy sentence and collects lemmas of all nouns into a set.\n",
    "        '''        \n",
    "        wordset = set()\n",
    "        noun_tag = set(['NN', 'NNP', 'NNS'])\n",
    "        nonaspects=set()\n",
    "        c=0\n",
    "        for token in doc:#nlp(sent):\n",
    "            c+=1\n",
    "            if c <100:\n",
    "                try:\n",
    "                    #print(\"test\")\n",
    "                    root = nlp.vocab[token.lemma].prob \n",
    "                    root1= nlp.vocab[token.lemma].vector_norm\n",
    "                    print(root1,root,token.tag_)\n",
    "                    # filter to only consider nouns, valid aspects, and uncommon words\n",
    "                    if token.tag_ in noun_tag :#and (root < -3.5 and token.lemma_ not in nonaspects):\n",
    "                        #print(root1,root,token.tag_)\n",
    "                        wordset.add(token)#, token.lemma_)\n",
    "                except:\n",
    "                    continue               \n",
    "            else:\n",
    "                break\n",
    "        #print(wordset)\n",
    "        #for token in doc: \n",
    "        #    print(token.text, token.lemma_, token.pos_, token.tag_)\n",
    "        #return \" \".join(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jjnn_pairs(phrase):\n",
    "    '''\n",
    "    Iterate over pairs of JJ-NN.\n",
    "    '''\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(phrase))\n",
    "    def ngramise(sequence):\n",
    "        \"\"\"\n",
    "        generate bigrams\n",
    "        \"\"\"\n",
    "        for bigram in nltk.ngrams(sequence, 2):\n",
    "            yield bigram\n",
    "\n",
    "    for ngram in ngramise(tagged):\n",
    "        tokens, tags = zip(*ngram)\n",
    "        if tags == ('JJ', 'NN'):\n",
    "            print(tokens,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sweet', 'chili') ('JJ', 'NN')\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "s = [\"thai iced tea spicy fried chicken sweet chili pork thai chicken curry\"]\n",
    "for phrase in s:\n",
    "    #print(noun_notnoun(phrase))\n",
    "    print(jjnn_pairs(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def _iter_nouns(self, sent):\n",
    "        '''\n",
    "        INPUT: SentCustomProperties\n",
    "        OUTPUT: str\n",
    "        Iterates through each token of spacy sentence and collects lemmas of all nouns into a set.\n",
    "        '''\n",
    "        # dictionary of unigram_nouns\n",
    "        unigram_wordset = dict()\n",
    "        # Identify noun tags\n",
    "        noun_tag = set(['NN', 'NNP', 'NNS'])\n",
    "        # List of keywords we want to exclude \n",
    "        nonaspects=set()\n",
    "            \n",
    "        for token in sent:            \n",
    "            # filter to only consider nouns, valid aspects, and uncommon words\n",
    "            if token.tag_ in noun_tag and token.lemma_ not in nonaspects:\n",
    "                if token.lemma_ in unigram_wordset:\n",
    "                    unigram_wordset[token.lemma_]+=1\n",
    "                else:\n",
    "                    unigram_wordset[token.lemma_]= 1\n",
    "\n",
    "        # Note: We need to normalize the count of nouns with respect to the total noun counts \n",
    "        # as in certain scenarios there will be few nouns and we can't pick nouns based on count\n",
    "        nouns_count=sum([val for val in unigram_wordset.values()])\n",
    "        \n",
    "        unigram_wordset_norm={key: val/nouns_count for (key,val) in unigram_wordset.items()}\n",
    "        top_aspects=[aspect for aspect in unigram_wordset_norm.keys() if unigram_wordset_norm[aspect]>0.01]\n",
    "        print(top_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mechanism', 'crowdsourcing', 'amount', 'datum', 'payment', 'worker', 'question', 'experiment', 'task', 'algorithm', 'answer', 'confidence', 'g']\n"
     ]
    }
   ],
   "source": [
    "for i in paper_id[:1]:\n",
    "    nlp = spacy.load('en_core_web_sm') # this is for core english\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    doc=nlp(str(df[df.Id==i]['PaperTextClean'][0])) #Spacy Doc type\n",
    "    #print(type(doc))\n",
    "    a._iter_nouns(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate FastText word embeddings\n",
    "1. support a user-defined number of dimensions \n",
    "2. have the ability to turn on/off at least one text-preprocessing step.\n",
    "\n",
    "### Step 1: Preprocessing data\n",
    "- data be in str format for python 3 \n",
    "space\n",
    "tab\n",
    "vertical tab\n",
    "carriage return\n",
    "formfeed\n",
    "the null character\n",
    "\n",
    "#### text preprocessing guidelines - \n",
    "https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "bible = gutenberg.sents('bible-kjv.txt')\n",
    "from string import punctuation\n",
    "remove_terms = punctuation + '0123456789'\n",
    "!python -m spacy link en_core_web_sm en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#from pycontractions import Contractions\n",
    "import unicodedata\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "#nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, #entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Abstract[:10].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=True)\n",
    "#df.Abstract[:10].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "#lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Abstract[:10].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(doc, html_stripping=False, accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    #normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    #for doc in corpus:\n",
    "    # strip HTML\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "\n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        doc = remove_accented_chars(doc)\n",
    "\n",
    "    # lowercase the text    \n",
    "    if text_lower_case:\n",
    "        doc = doc.lower()\n",
    "\n",
    "    # remove extra newlines\n",
    "    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "\n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        doc = lemmatize_text(doc)\n",
    "\n",
    "    # remove special characters and\\or digits    \n",
    "    if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "    # remove extra whitespace\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "    # remove stopwords\n",
    "    # http://www.cs.cornell.edu/~xanda/stopwords2017.pdf\n",
    "    if stopword_removal:\n",
    "        doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "\n",
    "    #normalized_corpus.append(doc)\n",
    "    return doc\n",
    "    #return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html_stripping=False, \n",
    "#accented_char_removal=True, text_lower_case=True,text_lemmatization=False, special_char_removal=True, stopword_removal=False, remove_digits=True))\n",
    "#df['AbstractClean']=df.Abstract.apply(normalize_corpus, args=(False, True, True, True, True, True, True))\n",
    "#    True, True, True,True,True, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['PaperTextClean']=df.PaperText.apply(normalize_corpus, args=(False, True, True, True, True, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>EventType</th>\n",
       "      <th>PdfName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "      <th>AbstractClean</th>\n",
       "      <th>PaperTextClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5677</td>\n",
       "      <td>Double or Nothing: Multiplicative Incentive Me...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5677-double-or-nothing-multiplicative-incentiv...</td>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "      <td>crowdsourcing gained immense popularity machin...</td>\n",
       "      <td>double nothing multiplicative incentive mechan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5941</td>\n",
       "      <td>Learning with Symmetric Label Noise: The Impor...</td>\n",
       "      <td>Spotlight</td>\n",
       "      <td>5941-learning-with-symmetric-label-noise-the-i...</td>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "      <td>convex potential minimisation de facto approac...</td>\n",
       "      <td>learning symmetric label noise importance unhi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Id                                              Title  \\\n",
       "0           0  5677  Double or Nothing: Multiplicative Incentive Me...   \n",
       "1           1  5941  Learning with Symmetric Label Noise: The Impor...   \n",
       "\n",
       "   EventType                                            PdfName  \\\n",
       "0     Poster  5677-double-or-nothing-multiplicative-incentiv...   \n",
       "1  Spotlight  5941-learning-with-symmetric-label-noise-the-i...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "\n",
       "                                           PaperText  \\\n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...   \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...   \n",
       "\n",
       "                                       AbstractClean  \\\n",
       "0  crowdsourcing gained immense popularity machin...   \n",
       "1  convex potential minimisation de facto approac...   \n",
       "\n",
       "                                      PaperTextClean  \n",
       "0  double nothing multiplicative incentive mechan...  \n",
       "1  learning symmetric label noise importance unhi...  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.kdnuggets.com/2018/05/implementing-deep-learning-methods-feature-engineering-text-data-fasttext.html\n",
    "# ref: https://radimrehurek.com/gensim/models/fasttext.html\n",
    "from gensim.models.fasttext import FastText\n",
    "import nltk\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "#tokenized_corpus = [wpt.tokenize(document) for document in norm_bible[:1000]]\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in list(df.AbstractClean)]#[:10])]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 50   # Context window size                                                                                    \n",
    "min_word_count = 5    # Minimum word count                        \n",
    "sample = 1e-3         # Downsample setting for frequent words\n",
    "mode=1               # sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "ft_model = FastText(tokenized_corpus, size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count,sample=sample, sg=mode, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'crowdsourcing': ['loss',\n",
       "  'learning',\n",
       "  'algorithmic',\n",
       "  'stability',\n",
       "  'logarithmic',\n",
       "  'clustering',\n",
       "  'propose',\n",
       "  'mechanism',\n",
       "  'data',\n",
       "  'convex'],\n",
       " 'classification': ['clustering',\n",
       "  'number',\n",
       "  'problem',\n",
       "  'propose',\n",
       "  'logarithmic',\n",
       "  'mechanism',\n",
       "  'loss',\n",
       "  'data',\n",
       "  'size',\n",
       "  'learning'],\n",
       " 'regularised': ['propose',\n",
       "  'large',\n",
       "  'number',\n",
       "  'problem',\n",
       "  'logarithmic',\n",
       "  'loss',\n",
       "  'size',\n",
       "  'learning',\n",
       "  'data',\n",
       "  'show'],\n",
       " 'experiments': ['number',\n",
       "  'clustering',\n",
       "  'learning',\n",
       "  'large',\n",
       "  'problem',\n",
       "  'approach',\n",
       "  'algorithmic',\n",
       "  'mechanism',\n",
       "  'show',\n",
       "  'stability']}"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view similar words based on gensim's FastText model\n",
    "similar_words = {search_term: [item[0] for item in ft_model.wv.most_similar([search_term], topn=10)]\n",
    "                  for search_term in ['crowdsourcing','classification','regularised','experiments']}\n",
    "similar_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAAJPCAYAAAAUtsmcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzs3Xl4VNXB+PHvmUmAECBsKosIYgCRAApRqZTFhRdcqqK4URW3utS1Wt+iLQou7WtFtFp/vmJtq9YVtC1aUeuKWrEECgp1ISCtIlSI7BBCMuf3R2BeoLg1uUbg+3mePM3cOXPvmdEH+fbOuTfEGJEkSZKk2paq6wlIkiRJ2jEZG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEpFT1xP4T7Rs2TJ26NChrqchSZKkHdj06dOXxhh3qet5bM+2y9jo0KEDJSUldT0NSZIk7cBCCP+o6zls7/walSRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGxnbgobcfosudXUhfl6bLnV146O2H6npKkiRJ0hfKqesJ6PM99PZDXP7C5Qw7ZBjntTmP+R/P5/IXLgdgePfhdTw7SZIk6bPVypmNEMKQEMJ7IYTSEMLIbTxfP4Tw6Mbn3wwhdNi4vUMIYV0IYebGn/+tjfnsSMZMGcOwQ4bRqV0n0uk0ndp1YtghwxgzZUxdT02SJEn6XDWOjRBCGrgTOBzYBzglhLDPVsPOBpbFGAuBW4GbNntuXoxx340/59d0Pjua0rJSOrbpuMW2jm06UlpWWkczkiRJkr6c2jizcQBQGmOcH2OsAB4BjtlqzDHAfRt/nwgcGkIItXDsHV5hi0Lmfzx/i23zP55PYYvCOpqRJEmS9OXURmy0BT7c7PFHG7dtc0yMsRJYAbTY+NyeIYS/hRBeCSH0q4X57FCu7X8tE1+cyNwP51JVVcXcD+cy8cWJXNv/2rqemiRJkvS56nqB+CJgjxhjWQihN/CHEEK3GOPKrQeGEM4FzgXYY489vuZp1p1Ni8DHTBlDaVkphS0KGXfoOBeHS5Ik6RuvNmJjIdBus8e7b9y2rTEfhRBygAKgLMYYgfUAMcbpIYR5QGegZOuDxBjHA+MBiouLYy3Me7sxvPtw40KSJEnbndr4GtU0oFMIYc8QQj3gZGDSVmMmASM2/j4MeDHGGEMIu2xcYE4IoSPQCZiPJEmSpO1ejc9sxBgrQwgXAc8CaeDXMcY5IYTrgJIY4yTgXuCBEEIp8CnVQQLQH7guhLAByADnxxg/remcJEmSJNW9UP1Npu1LcXFxLCn5t29aSZIkSbUmhDA9xlhc1/PYntXKTf0kSZIkaWvGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUpErcRGCGFICOG9EEJpCGHkNp6vH0J4dOPzb4YQOmz1/B4hhNUhhB/WxnwkSZIk1b0ax0YIIQ3cCRwO7AOcEkLYZ6thZwPLYoyFwK3ATVs9Pw6YXNO5SJIkSfrmqI0zGwcApTHG+THGCuAR4JitxhwD3Lfx94nAoSGEABBCOBb4AJhTC3ORJEmS9A1RG7HRFvhws8cfbdy2zTExxkpgBdAihNAI+BEwphbmIUmSJOkbpK4XiI8Gbo0xrv6igSGEc0MIJSGEkiVLliQ/M0mSJEk1klML+1gItNvs8e4bt21rzEchhBygACgDDgSGhRB+DjQFMiGE8hjjL7c+SIxxPDAeoLi4ONbCvCVJkiQlqDZiYxrQKYSwJ9VRcTIwfKsxk4ARwBvAMODFGGME+m0aEEIYDazeVmhIkiRJ2v7UODZijJUhhIuAZ4E08OsY45wQwnVASYxxEnAv8EAIoRT4lOogkSRJkrQDC9UnGLYvxcXFsaSkpK6nIUmSpB1YCGF6jLG4ruexPavrBeKSJEmSdlDGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUpErcRGCGFICOG9EEJpCGHkNp6vH0J4dOPzb4YQOmzcfkAIYebGn1khhKG1MR9JkiRJda/GsRFCSAN3AocD+wCnhBD22WrY2cCyGGMhcCtw08bts4HiGOO+wBDg7hBCTk3nJEmSJKnu1caZjQOA0hjj/BhjBfAIcMxWY44B7tv4+0Tg0BBCiDGujTFWbtzeAIi1MB9JkiRJ3wC1ERttgQ83e/zRxm3bHLMxLlYALQBCCAeGEOYAbwPnbxYfkiRJkrZjdb5APMb4ZoyxG7A/cFUIocG2xoUQzg0hlIQQSpYsWfL1TlKSJEnSV1YbsbEQaLfZ4903btvmmI1rMgqAss0HxBjfAVYDRds6SIxxfIyxOMZYvMsuu9TCtCVJkiQlqTZiYxrQKYSwZwihHnAyMGmrMZOAERt/Hwa8GGOMG1+TAxBCaA/sDSyohTlJkiRJqmM1vvJTjLEyhHAR8CyQBn4dY5wTQrgOKIkxTgLuBR4IIZQCn1IdJADfBkaGEDYAGeD7McalNZ2TJEmSpLoXYtz+LgBVXFwcS0pK6noakiRJ2oGFEKbHGIvreh7bszpfIC5JkiRpx2RsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmStumhtx+iy51dSF+XpsudXXjo7YfqekraztRKbIQQhoQQ3gshlIYQRm7j+fohhEc3Pv9mCKHDxu2DQgjTQwhvb/zfQ2pjPpIkSaqZh95+iMtfuJxB/QZx8/dvZlC/QVz+wuUGh76SGsdGCCEN3AkcDuwDnBJC2GerYWcDy2KMhcCtwE0bty8FvhNj7A6MAB6o6XwkSZJUc2OmjGHYIcPo1K4T6XSaTu06MeyQYYyZMqaup6btSG2c2TgAKI0xzo8xVgCPAMdsNeYY4L6Nv08EDg0hhBjj32KMH2/cPgfICyHUr4U5SZIkqQZKy0rp2KbjFts6tulIaVlpHc1I26PaiI22wIebPf5o47ZtjokxVgIrgBZbjTkemBFjXF8Lc5IkSVINFLYoZP7H87fYNv/j+RS2KKyjGWl79I1YIB5C6Eb1V6vO+5wx54YQSkIIJUuWLPn6JidJkrQTurb/tUx8cSJzP5xLVVUVcz+cy8QXJ3Jt/2vremrajuTUwj4WAu02e7z7xm3bGvNRCCEHKADKAEIIuwO/B06PMc77rIPEGMcD4wGKi4tjLcxbkiRJn2F49+FA9dqN0rJSClsUMu7Qcdnt0pdRG7ExDegUQtiT6qg4Gdj638JJVC8AfwMYBrwYY4whhKbAn4CRMcbXa2EukiRJqiXDuw83LlQjNf4a1cY1GBcBzwLvAI/FGOeEEK4LIRy9cdi9QIsQQilwObDp8rgXAYXANSGEmRt/dq3pnCRJkiTVvRDj9veNpOLi4lhSUlLX05AkSdIOLIQwPcZYXNfz2J59IxaIS5IkSdrxGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJEmSpEQYG5IkSZISYWxIkiRJSoSxIUmSJCkRxoYkSZKkRBgbkiRJkhJhbEiSJElKhLEhSZIkKRHGhiRJkqREGBuSJEmSEmFsSJIkSUqEsSFJkiQpEcaGJO3EcnJyGDNmTF1PQ5K0gzI2JEn/kdWrV9f1FCRJ33DGhiRtByZMmEAIgaZNm5JKpcjNzWXhwoVbnJmYOnUqIQQA+vfvT4MGDcjNzSWEwH777UdRURGpVIp0Os306dOz+/7lL39JKpUilUoxatQoAD744AMKCgpIp9OkUimGDRuW3W/9+vXJzc2ladOmX/OnIEna3hgbkrQdueqqq8hkMuTm5nL66ad/7tj169czZ84cXnvtNWbOnEleXh6ZTIYWLVpwxhlnZMdt2LCBTCbDxRdfzI033ghAv379KC4upqqqipkzZ/L444/zwQcfAFBRUcHUqVOprKxM7H1KknYMxoYkbUd+9KMfAdCuXTsWLFjwuWMLCgro3Lkzffv2BcieAencuTOLFy/Ojjv55JMB+MUvfkGMkbfeeotFixbx0ksvkUql2HfffQF45plnAGjcuDG9e/eu1fclSdoxGRuStJ3Y9BUpgFQqRVVVFSGE7BmGpUuXbjE+nU5v8bh58+bZ12YymS32tblUKkWMkbvvvptMJkMmkyHGyAUXXABAbm5u7b0pSdIOzdiQpO1Yw4YNmTx5MlD9Fav/xMMPPwzA5ZdfTgiBoqIi2rRpw8iRI6mqqgLgZz/7We1MWJK0U8mp6wlIkv5zP//5z7ngggtIpVK0bdv2P9pHTk5O9uzGj3/8YwDefPNNioqKsmcxcnNz/+OYkSTtvEKMsa7n8JUVFxfHkpKSup6GJEmSdmAhhOkxxuK6nsf2zK9RSZIkSUqEsSFJkiQpEcaGJEmSpEQYG5K0kxo4cCCuf5MkJcnYkKTtiHftliRtT4wNSUrQscceS+/evenWrRvjx48HoFGjRvzgBz+gW7duHHrooSxZsgSoPtNw6aWXUq9ePerXr89f//pXAEaPHs1pp51G3759Oe200ygvL+fMM8+ke/fu7Lfffrz00ksATJ48mRACrVu3plevXvzlL3/JzmPPPfckJyeHnj17MnLkyOz2CRMmcMABBxBC4Le//e3X9KlIknYWxoYkJejXv/4106dPp6SkhNtvv52ysjLWrFlDcXExc+bMYcCAAYwZMyY7fu3atbRp04ZWrVpx1llnZbf//e9/5/nnn+fhhx/mtttuI4TA22+/zcMPP8yIESMoLy+nrKwMgPPPP59HH32USy65BKiOkLKyMpo0acKsWbP47//+7+x+KyoqeO211/jTn/7E7373u6/pU5Ek7Sy8z4YkJWj06NH8/ve/B2DBggU8++yz9O3bl/Xr15OTk8P8+fPp0aMHa9euJcZITk4OlZWVhBDY9OfzgQceyJtvvkmLFi0oKytj1113Za+99uKNN97IHuf888/nt7/9LeXl5Vscv3nz5nz66acAhBDIZDJcf/31XHPNNdkx3/nOd3jyySdJp9Pk5ub+2z5CCOTk5HDZZZcxduxYAHbddVc++eQTMplM7X9okvQN4X02as4zG5KUkJdffpnnn3+eN954g1mzZrHffvv921/k77vvPtasWcNf/vIXBgwYwBlnnAFA06ZNadeuHQBvvfUWAPn5+QAcdNBBvPHGG7Rt25YYI/Xq1eOhhx7iqKOOAqBJkyZs2LABgE8//ZTLLruMvfbaixgjl156KTfeeCMA7dq14+c//zlPPvkkADFG1q9fD8Dhhx8OQCqV4uCDDyaVSnHzzTdz5ZVXkslksncclyTp8/hfC0lKyIoVK2jWrBkNGzbk3XffZerUqQBkMhkmTpwIwPjx48nPz6dPnz5A9dkPgMaNG1NQUACQDZTzzjsPgMLCQgA+/vhj9txzT+rVq8fKlStZu3YtAKeccgoPPPBAdh633XYb8+bNA6CkpCQbFB9++OEWX6mC6jMWAH/4wx8A6NixIwsWLKBZs2YA3HTTTQBcccUVNf+AJEk7PGNDkhIyZMgQKisr6dq1KyNHjswGRX5+Pn/9618pKipi9erVNG7cOPuaevXqAbB48WLuvfdeQgjZ5+rXrw/AySefDEDbtm1ZvHgxq1evBmDw4MEATJw4kXfffTf72rfffpsOHTqQSqVYs2ZNdn8DBgxg2rRpbP512nQ6TQghO4+cnByqqqq2mIckSV+WsSFJCalfvz6TJ0/mnXfe4Q9/+AMvv/wyAwcOBGDcuHHMnj2bn/70pyxevDh75ak999wTqL5i1QEHHECMMRsZ/+///T8AevfuDcBxxx3HunXrSKVSpNPpbMxUVVVx00030bVrVwDy8vJo164d+fn53H777dn9vfzyyxQXF2fPVuyyyy6f+V42xcePf/xjAG699dba+ZAkSTs0F4hL0tesUaNG2bMRUH2G4dVXXyXGSG5uLhs2bMguEN+0YByqY2DJkiXEGGnZsmX26lMADz74IMOHD9/iDEQIgZYtW7J06dLs2YsHHniABQsWMGrUqOwxmjZtyvLly9ltt91Ip9MsWrSITCZDCIG9996bdevWAbB69ersYvMWLVrw6aefUlVVlfjnJUl1xQXiNZdT1xOQpJ3N5qEB8Morr2R/nzBhAieddNI2r/IUQshegWrp0qXb3PeX/T+Qrr/+eqZNm0aPHj0+c8zW+5o3bx577bUXAJ07d/5Sx5Ek7dyMDUnaCW1aJP5VXHTRRTz77LMA5Obm8uKLL9b2tCRJO5haWbMRQhgSQngvhFAaQhi5jefrhxAe3fj8myGEDhu3twghvBRCWB1C+GVtzEWSkvZlFktPmDCBEAJNmjQhhEB+fj7f+973sguwR40axQcffEBBQQHpdJpUKsWwYcM44YQTWLNmDbm5uYQQSKVS7Lvvvtn9nnTSSaRSKVKpFHfffTcAo0aNyu4jnU5zzz33ANC/f38aNGhAvXr1sncW3/w9TJ06lQ8++ID69etn97npkrchBNq2bZvd5w033MALL7xAjJHevXuzfv16+vbtW5sfqyRpB1Tj2AghpIE7gcOBfYBTQgj7bDXsbGBZjLEQuBW4aeP2cmAU8MOazkOSvolGjx7N+vXrWbduHY899hgVFRUcf/zx3HzzzfTt25fi4mKqqqqYOXMmjz/+OB988AH9+vUjJyeHVatWkclkuPfee7P7a9asGZlMhv322y972drhw4ezYsUKMpkMZ511VvbO4VB9BmPGjBksWrSIxYsXM2nSpC3md9lll5GXl0cmkyGTyXDzzTdnn2vdujWZTIYWLVpwzTXXMHv2bF577TVcMydJ+rJq48zGAUBpjHF+jLECeAQ4ZqsxxwD3bfx9InBoCCHEGNfEGF+jOjokabuz6667EkIghJC9LwbAiSeeCFTfj6JLly7k5+ez//77k5OTw5NPPsn69etZtGgRL774IiEEevbsCcDFF1/MnDlzyM3NpWXLlgD06dOHvLw8AGbNmkXv3r0ZMmQIa9asoUGDBuyzzz40btyYEAK/+tWvtrhxYEFBAUVFRbRq1Yrc3FxefvnlLeZ/9NFHs2LFCtq0acNll11GUVFR9rnrrrsOqF6f0aRJEzp37pw9m7HpRoOSJH2e2oiNtsCHmz3+aOO2bY6JMVYCK4AWtXBsSaozgwcPZvny5WzYsIF169axatUqhg0bBpC92tPcuXNZsGABlZWV2WBo3bp1NlAA2rRpQ4yRdu3a8dxzz23zWBUVFQCMGDGCGTNmUK9ePaqqqqioqCA/P5899tgD+L9L526STqezv4cQsvvZ5Oyzz6akpISuXbvyy1/+covXN2/eHCD7VarNbbqBoCRJn2e7uc9GCOHcEEJJCKFkyZIldT0dSeK1115jw4YN5ObmkpeXR4yRmTNnAjB27FhijHTq1Akge/lagDFjxgDVkQFw4403AtCyZUsqKyspKipi1apV2atBxRiz98w4//zzt5hDYWEhlZWV7LvvvqRSKT766KOv9B6effZZ2rZtywsvvMCwYcNYtGjRV/0YJEn6TLURGwuBdps93n3jtm2OCSHkAAVAGV9BjHF8jLE4xlj8eTeekqSvy9q1a2nfvj0xxuxPaWkpQ4cOzd6bYvNg2KRhw4YAvPnmmwCceeaZpFIpZs2aRYyRl156iVQqRUVFBalUiqqqquyN+Lbl0ksvZdKkSdu8XO4Xefjhh2nTpg2pVIoJEyZw1VVXfeV9SJL0WWp8U7+N8fA+cCjVUTENGB5jnLPZmAuB7jHG80MIJwPHxRhP3Oz5M4DiGONFX+aY3tRPUl3adDO8TV+DmjVrFj169OD++++noKCAm266ialTp5LJZBgzZgyjR4/m6KOP5vHHHyc3N3eL8AghMHnyZIYMGcK5557LPffcQ4yRLl268I9//IPy8nJyc3Pp0aMH06dP3+L4vXv35m9/+xtr167l6aef5vjjj6dXr17ZcZKkmvGmfjVX4/tsxBgrQwgXAc8CaeDXMcY5IYTrgJIY4yTgXuCBEEIp8Clw8qbXhxAWAE2AeiGEY4H/ijH+vabzkqSvQ+vWrbOLuwFycnI477zzmDp16haXyH3qqaeyZzLq1atHvXr12H333bPPv/POOzz44IMA5Ofn06hRoy889uuvv05BQQF5eXnZNSCe+ZUkfZPUypqNGOPTMcbOMca9Yow3btx2zcbQIMZYHmM8IcZYGGM8IMY4f7PXdogxNo8xNoox7m5oSPqm2/zMxMcff8y6dev48MMPiTEye/Zs7r77biorK3n11VcBuOeee6iqqiIvL490Os0777zDJ598wqJFi+jevTtDhgzh8MMP5+abbybGyPPPP8/y5cuzV5XasGHDFmcrNh2/QYMGTJs2jRgjzz77LDFGzjnnnK/xk5Ak6fPV+GtUdcGvUUn6Jtj0daa1a9fSp08f5s6dC0B5eTmzZs1i5cqVHHzwwWzYsAGAq666iscee4x58+YBcPzxxzN37lzeeustUqnUFusyKioqWLhwIa1atfrCOWzSuXNn3nvvvdp+m5K00/JrVDVX469RSdLO7pIDtpWtAAAgAElEQVRLLmH58uWUlZXRsGFDcnJyWLlyJcC/XTL28yxatIimTZt+pWNvj/+HkSRp57HdXPpWkr6pPv30U5o2bUrDhg0ZN24cVVVV2xx3/PHH849//IMPPviA8vJyXnjhhexze+yxB9/97nezjx999NHE5y1JUtI8syFJNfSzn/2M4uJiGjRoQLt27ahXr942xxUXF3PyySfTpUsX6tWrx2677Ubjxo2B6vtdDBo0iLy8PDKZDHvttRcnnXTS1/k2JEmqdZ7ZkKT/0KavMHXp0oVVq1ZRXl7O3Llzueeee/j+97/PhRdeyAknnMCCBQs45JBD6NGjBwsWLKC0tJSlS5fy8ccfU15ezkEHHcThhx/OuHHjWLduHUOHDuXmm2/OHueMM85g4sSJVFVVceWVV7L//vvTo0cP7r77bgB+//vfc+ihhxJjZNGiRXTu3JnFixfXyWciSdLmjA1JqkVz5szhhhtu4MUXX2TWrFn84he/4OKLL2bEiBG89dZbfPTRR+y5557ZS9Z26NCB1157jaeeeoqRI0cCcNJJJ/HYY48B1QvFX3jhBY488kjuvfdeCgoKmDZtGtOmTeOee+7hgw8+YOjQobRu3Zo777yT733ve4wZM+YLF5ZLkvR18GtUklSLXnzxRU444QRatmwJQPPmzXnjjTd44oknAJg7dy6tW7dm6dKlnHHGGQwaNIhUKsU+++zDv/71LwAOP/xwLr30UtavX88zzzxD//79ycvL47nnnuOtt95i4sSJAKxYsYK5c+ey5557cscdd1BUVESfPn045ZRT6ubNS5K0FWNDkv4DOTk5zJkzhy5dunzp13Tp0oWf/exnW2zb/HK3m98/Y+DAgTz77LM8+uijnHzyydnn77jjDgYPHvxv+/7oo49IpVL861//IpPJkEp54lqSVPf8r5Ek1aJDDjmECRMmUFZWBlRfqeqggw7ikUce4b333mPlypX069fvC/dz0kkn8Zvf/IZXX32VIUOGADB48GDuuuuu7H073n//fdasWUNlZSVnnXUWDz/8MF27dmXcuHHJvUFJkr4Cz2xI+toNHDiQsWPHUlz89d4n6ZprrqF///4cdthhX+l1n3zyCUVFRaxatYoYI+eddx4A5557LtOmTSOTyfDEE09wxBFH0KBBAz755BN22WUXYoyEEMjNzeXJJ5/k9NNPp0uXLrz33nvk5FT/8XvfffcRQiCTybBhwwaaNm1KRUUFDRs2pKKiguOPPz57datzzjmH999/n169ehFjZJddduEPf/gDt9xyC/369ePb3/42PXv2ZP/99+fII4+ka9eutfsBSpL0FXlmQ9KXUllZWddT+FI+b57XXXfdVw4NgJtvvpnmzZuzbt06KisrGTp0KAC77LILa9eu5dhjj+UHP/gBAEOHDmW//fbjiiuuoGXLltSvX5/169dTWFhI/fr1GTx4MA0bNqSwsJATTzwRgKOOOooRI0YA1eswQgiUlZWxatUqxowZQwiBNm3akJOTw+zZszn11FN55513eOWVV9htt904+uijGTduHDk5OXTs2JH333+fffbZJ3vp3OnTp9OgQQNSqRTpdJpbbrkFgIsvvphUKpX9ef/99wHo2rUr6XSaVCpF+/btv/LnJUnSJsaGpKz777+fHj160LNnT0477TTOOOMMzj//fA488ED++7//m08//ZRjjz2WHj160KdPH9566y0AunfvzvLly4kx0qJFC+6//34ATj/9dP785z+zbt06Tj75ZLp27crQoUNZt24dAFVVVZxxxhkUFRXRvXt3br31VgBmzpxJnz596NGjB0OHDmXZsmVA9RmRkpISAJYuXUqHDh0A+O1vf8vRRx/NIYccwqGHHgrATTfdRPfu3enZs2f2Kk+bLiEL0KFDB6699lp69epF9+7deffddwFYsmQJgwYNolu3bpxzzjm0b9+eAw44gNLSUg488EAymQytW7cGyO530KBBLFmyBIDS0lKuvfZaxo4dy7JlyygvL+f6669n0KBBbNiwgalTp3LiiSfy0Ucf8Y9//IMQApMmTWLixIlUVFQAsH79+uw/k03rM5o0aUImk+GZZ57hqquu4vvf/z69e/cmxsigQYPo379/9uxIJpMByF7RavDgwbRu3ZpMJsP3vvc9rrzySgDGjx/P5ZdfTiaTYe7cubRp04azzjqLRYsW0ahRI8aNG8fixYu59NJLa/zvliRpJxVj3O5+evfuHSXVrtmzZ8dOnTrFJUuWxBhjLCsriyNGjIhHHnlkrKysjDHGeNFFF8XRo0fHGGN84YUXYs+ePWOMMZ533nnxqaeeim+//XYsLi6O55xzTowxxsLCwrh69ep4yy23xDPPPDPGGOOsWbNiOp2O06ZNiyUlJfGwww7LzmHZsmUxxhi7d+8eX3755RhjjKNGjYqXXnppjDHGAQMGxGnTpsUYY1yyZEls3759jDHG3/zmN7Ft27axrKwsxhjj008/Hb/1rW/FNWvWZN9LjDGOGDEiTpgwIcYYY/v27ePtt98eY4zxzjvvjGeffXaMMcYLL7ww/vSnP40xxtisWbMIxHr16sVjjjkmnn/++RGIBx54YEyn0/HAAw+Mubm5sX79+jGEEI888siYl5cXR40alX0dEPPz82OvXr0iENPpdKxXr15Mp9MRiI0bN46XX355BLI/ZWVlMZ1Ox1atWmW3XXXVVTHGGIuLi7OvS6fT8YgjjtjitZvex6bHs2bNys6lQYMG2ff097//PXbq1CmmUqnYo0eP+OSTT8YYY9xtt90iEEMIMYQQgXjQQQfV4N8sSdp+ASXxG/B33+35xzMbkoBtX7IV4IQTTiCdTgPw2muvcdpppwHVC6HLysqyC56nTJnClClTuOCCC3j77bdZuHAhzZo1Iz8/nylTpnDqqacC0KNHD3r06AFAx44dmT9/PhdffDHPPPMMTZo0YcWKFSxfvpwBAwYAMGLECKZMmfKF8x80aFB2zs8//zxnnnkmDRs23OK9bO24444DoHfv3ixYsCD7Hjdd/WnatGk0a9aMRx55hMmTJ3P55ZcTQmDevHnEGJk5cyaffPIJN910E9X/Tar+Gtf1118PwOjRo0mlUqxdu5YZM2YA1WdzLrzwwuzZh1WrVnH77bdvMa/evXtTVVW1xY35fve735GTk8Pf/vY3ANq2bUuzZs14+umnt3jtvffey8KFC7OPe/bsCcBhhx3GunXryMvLA6CoqIi5c+fStWtX5syZw3e+8x3S6TRVVVXsv//+pFIprr32WmKMFBYWZr9qlZub+4X/LCRJ2sTYkPS58vPzv3BM//79efXVV3n11VcZOHAgu+yyCxMnTvzCqy41a9aMWbNmMXDgQP73f/+Xc84553PH5+TkZP+SXl5e/pXnubVNl51Np9PbXOtx9tlns2zZMk444QQqKiro1q0bMUZ++MMfEmOke/fuNG3alGbNmmUvNdusWbPs66+++moymQwxRu65557s9l/96lfZONl87i1atADgn//8Z3ZeIQQAli9fvsXcBg8enA2Hli1bZoMwNzeXtm3bZsf95Cc/AWDy5MkArFy5Eqi+StZJJ53EnDlzOOWUU2jQoAEADRs2ZPr06dnXjx07lvvvv5+HH36YTCbD1KlTv/BzlSRpE2NDErDtS7ZurV+/fjz44IMAvPzyy7Rs2ZImTZrQrl07li5dyty5c+nYsSPf/va3GTt2LP379weqY+Shhx4CYPbs2dm1HkuXLiWTyXD88cdzww03MGPGDAoKCmjWrBmvvvoqAA888ED2LEeHDh2yfxHetPZiWwYNGsRvfvMb1q5d+5nv5bP07duXxx57jNtuu41p06YBsHjxYgoKCvj5z39OOp3m2GOP5ZhjjsmeObnuuuvIZDL86U9/yv5lHtjiXhf/9V//lb2q1KpVq4Dq+2k0bdo0+3jTZ78pMKqqqrJRsimydtttN7p06cIdd9zBhx9+CMCf//zn7HFCCNnjHnDAAUB1zMQYSaVSrF69mubNm1NQUMBzzz0HVJ81SaVStG7dmiZNmtCtWzeqqqoYPXo0V155JfXq1csuNu/du/eX/iwlSTI2JAHQrVs3fvzjHzNgwAB69uzJ5Zdf/m9jRo8ezfTp0+nRowcjR47kvvvuyz534IEH0rlzZ6A6ShYuXMi3v/1tAC644AJWr15N165dueaaa7J/YV24cCEDBw5k33335dRTT83e8O6+++7jyiuvpEePHsycOZNrrrkGgB/+8Ifcdddd7LfffixduvQz38uQIUM4+uijKS4uZt9992Xs2LFf+nO49tpree6557jxxhvJZDK0atWK119/nRUrVmwx7thjj2XatGlcffXVzJs3j5ycHAYMGEB5efkWsbDpErdHH3109kxEq1ataNmyJfXq1cv+xR7+L06qqqpIpVK8++67tG/fnhYtWmT3k5ubS2FhIVVVVdkrRe27777ZebVv3z4bJoMHD2b48OGsXbuWVCpFJpOhTZs27LrrrsD/RViMkTVr1pCTk0NlZSVvvfUW6XSa0aNHc+KJJ2aPLUnSV1bXi0b+kx8XiEtKSnl5edywYUNcsWJFLCgoiCGE2KpVq1hQUBBvvfXWmE6n47vvvhtjrF6wzsbF1G3atImnnXZa3HXXXWObNm22WLQNxIKCgpifnx+B2KdPn3jkkUdGIO6xxx4xlUr92/h0Oh2fe+65GEKILVq0iAUFBTGdTsf27dvHI488MsZYvci9+o/xmF2M3qlTpxhjjEAcNWpUjDHGgQMHRiA2aNAghhDi3nvvnX2/m16/aX+bnkun03H06NHxtddei0B85JFHYowxlpSUJPxPQJK+OXCBeI1/PLMhSZv55z//yf7770+/fv3o1KkTb775JosWLWL58uVcdtllVFZW0qVLFwAeeeQRioqKaN26NcuXL9/iMrVQffndyspKcnJyWLFiBWvXriWdTtOyZUueeOIJmjZtyj//+U9ijOTk5LDbbrvRuHFjdt99d6qqqhgyZAiNGjWiUaNG2TUbZ555Jk899VR2vnvvvTdA9i7jpaWl2QXiRxxxBFB9yeB0Os26devYY489vtLn0bdvX0499VROOeUUUqkUffr0+U8/WknSTihUR9v2pbi4OG661r4k1ZUOHTqwcOFCKisr6d+/P/fccw9dunRh77335t133+WVV16hZ8+etGzZkry8PFauXEnLli3p2rUrr776KvPmzaOwsJC77rqLCy64gHQ6TcOGDbdY9yFJqjshhOkxxuK6nsf2zNiQpBoqLCxk3rx5pNNp6tevzx577EF+fj7Tp08nlUrRrFkzKioqWLlyJT/84Q+55ZZbsus6Nv8zuEWLFtlxkqS6Z2zUnLEhSZIkbYOxUXOu2ZC0Qxs6dOjnXrlqk5ycHN57771/2z58+HDOPfdcoPqysldccQUAXbp0YdKkSUD1ZW0lSdK/83qGknZoTz75JGVlZdk7o39Vm+4PsrXNw2Tz+1xIkqT/45kNSTuMTz75hF133ZW8vDwaNGjAwQcfTFVVFT179sze2btbt27k5+fToEGD7M0CNznttNNo0KABjRo14oUXXgCqr+R01FFHbTFu030wQgjZtRd5eXm0atUquy2VSnHZZZfRp08fcnJySKVSpFIpunfv/jV8EpIkfTMYG5J2GDfffDPNmzdn3bp1lJeXc99995FOp5k1axbLli0DYNKkSaxZs4aVK1cya9asLe5EXlBQQHl5OcOHD2f48OGfeZzjjjuOGCPjx4/npZdeyu63efPmtGnThry8PNasWcO5556bvQt5JpMhk8lk78AuSdLOwNiQtMM47LDDKC0t5cADD+SXv/zlNu8pMWrUKBo2bEhBQQErV65kypQp2ed+/OMfA3DbbbexZMmSzzzOG2+8kb3y1MCBAwEYP348gwYN4uOPP6a8vJyzzjqLJ554ghYtWlBVVUXLli0ZMWJE9r4YkiTtDIwNSTuMwYMH895779GrVy9+8pOfcMghh2zx/JQpU5gwYQJz5sxh3bp1dOzYkbVr12afT6Vq9kfiL37xC5555hl23XVXHnnkEW644QYuvPBCSktL6du3L48++ii77LJLjY4hSdL2xNiQtMOYMWMGLVq04K677uKSSy7hnXfeIScnh3/9618ALF68mJycHNq1a8fs2bOZP3/+Fq//6U9/CsAVV1zxuVFw0EEHUVVVxfr163n99dcBOOOMM3j88cfp378/paWlAKxfv56DDz6YiooK/vjHP/I///M/rFq1Kom3LknSN5JXo5K0w5g8eTI33HADUH2W4q677mLSpEkcdthh5Ofns2zZMq6++moaNmxIo0aNaNWq1RavX7ZsGXl5eaRSqexlbbfl8ccfJzc3l3PPPZcQAk2aNOG4444jhEBFRUV2XGFhIVOnTmXkyJHZbd/97ndr+V1LkvTN5U39JKmWVVZW0rhxYyZNmsSgQYPqejqSpP+QN/WrOb9GJUm1aNKkSeTl5dGtWzdDQ5K00/NrVJJUi44++mg2bNhQ19OQJOkbwTMbkiTVskaNGn3l1xxxxBEsX768RsddsGABRUVFNdqHJNUmz2xIknZ6MUZijDW+/HFNjv30009/7ceWpKR5ZkOStFNasGABXbp04fTTT6eoqIgHHniAb33rW/Tq1YsTTjiB1atXA/D000+z995707t3by655BKOOuooAEaPHs3YsWOz+ysqKmLBggVbHGP16tUceuih9OrVi+7du/PHP/5xm8f+8MMP6dChA0uXLmXNmjUceeSR9OzZk6KiIh599FEApk+fzoABA+jduzeDBw9m0aJF2e09e/akZ8+e3HnnnUl/bJL0lRgbkqSd1ty5c/n+97/PK6+8wr333svzzz/PjBkzKC4uZty4cZSXl3PeeecxefJkpk+f/rl3lt+WBg0a8Pvf/54ZM2bw0ksvccUVV7DpKpCbjj1nzhzat2+ffc0zzzxDmzZtmDVrFrNnz2bIkCFs2LCBiy++mIkTJzJ9+nTOOuus7B3vzzzzTO644w5mzZpVex+MJNUSv0YlSdpptW/fnj59+vDUU0/x97//nb59+wJQUVHBt771Ld599106duzInnvuCcApp5zC+PHjv/T+Y4xcffXVTJkyhVQqxcKFC7M3mdx07K11796dK664gh/96EccddRR9OvXj9mzZzN79uzsFc6qqqpo3bo1y5cvZ/ny5fTv3x+A0047jcmTJ9foM5Gk2mRsSJJ2Wvn5+UB1FAwaNIiHH354i+dnzpz5ma/Nyckhk8lkH5eXl//bmAcffJAlS5Ywffp0cnNz6dChQ3bcpmNvrXPnzsyYMYOnn36an/zkJxx66KEMHTqUbt268cYbb2wxtqYLyiUpaX6NSpK00+vTpw+vv/46paWlAKxZs4b333+fLl26MP//t3f/sXHX9x3HX29MbCeO2oQMAmnCwPLFiMhRN44f2ua0CuScklEq4akQahvhqqy4QjQaG1WluqZ00F9pN80gUaLGNvIYMpqaDa9OmtDGnZYVJ2OxQod8O4ZiTFIIIVLc2iHmvT98ts7JBcc+f+97P54PKfJ9v/f5fvM++MTOK9/Pj0Riei7G1PwJSbrmmmt06NAhSdKhQ4f0xhtvnHffU6dO6YorrtCiRYv08ssv680335y1lpGRES1ZskRf+MIX9Mgjj+jQoUOqrq7WO++8Mx02PvjgAx05ckTLli3TsmXL9Ktf/UrSZLgBgFzCkw0AQNG7/PLLtXPnTt1zzz0aHx+XJD3++ONau3atnnrqKW3evFkVFRW68cYbp6+566671NnZqXXr1unmm2/W2rVrz7vvvffeqzvuuEM1NTWKRqO67rrrZq1lcHBQjzzyiC655BItWrRITz/9tEpLS9XT06OHHnpIp06d0tmzZ/Xwww9r3bp1+slPfqL7779fZqZYLLZw/1EAYAHY1ES1fBKNRn1gYCDsMgAAReD06dNaunSp3F0tLS2KRCL66le/GnZZALLAzA66ezTsOvIZw6gAAPgIP/7xj/XJT35S69at06lTp/TAAw+EXRIA5A2ebAAAAABp8GQjczzZAAAAABAIJogDCFWsK6b+4X6NnRlTeWm5alfXanfD7rDLAgAAC4CwASA0sa6YDhw7oOYtzapcVanESEKdfZ2KdcUIHAAAFACGUQEITf9wvxrrGhVZE1FJSYkiayJqrGtU/3B/2KUBAIAFQNgAEJqxM2OqXFU541zlqkqNnTl/J2YAAJB/CBsAQlNeWq7ESGLGucRIQuWl5SFVBAAAFhJhA0BoalfXqrOvU0NHhzQxMaGho0Pq7OtU7erasEsDAAALgAniAEKzu2G3Yl0x7XhpB6tRAQBQgAgbAEJFsAAAoHAxjAoAAABAIAgbAAAAAAJB2AAAAAAQCMIGAAAAgEAQNgDMWfdgt6rbq1XyWImq26vVPdgddkkAACAHsRoVgDnpHuzWtr3bVL+xXg+sekCJkYS27d0mSdpaszXk6gAAQC4xdw+7hjmLRqM+MDAQdhlAUapur9am2k2KrIlMnxs6OqQ9/Xv0esvr0+diXTH1D/dr7MyYFpUs0iWXXKLxD8bZSwMAkDfM7KC7R8OuI58xjArAnMRPxFW5qnLGucpVlYqfiE8fx7piOnDsgJq3NOveTfdq6ZKl+uKff1E/aPmBmrc068CxA4p1xbJdOgAAyDLCBoA5qVpRpcRIYsa5xEhCVSuqpo/7h/vVWNeoyJqI9h7cq623bVVkTUQlJSWKrImosa5R/cP92S4dAABkGWEDwJy0bmhVz74eDR0d0sTEhIaODqlnX49aN7ROtxk7Mzb99OP4yeNpn4SMnRnLat0AACD7mCAOYE6mJoG37W9T/ERcVSuqtP3W7TMmh5eXlisxklBkTUQrl6+cfj0lMZJQeWl51msHAADZxQRxAAtuas5GY12j3j/9vnoP9GrrbVtVuapSiZGEOvs6dcuVtzBJHACQ05ggnrkFCRtmtlnS30kqkfSsuz95zvtlkjol3SDphKTPu/v/Jd/7mqRmSROSHnL3vtl+P8IGkPtYjQoAkO8IG5nLeBiVmZVIape0SdKwpFfMbJe7v5bSrFnSSXevMrO7JX1H0ufN7HpJd0taJ2mVpJ+b2Vp3n8i0LgDhIkwAAICFmCB+k6S4uyfc/Yyk5yXdeU6bOyV1JF/3SLrVzCx5/nl3H3f3NyTFk/cDkKNiXTEtfmKxrM20+InFLGELAAAuaCHCxickHU05Hk6eS9vG3c9KOiVpxUVeK0kysy+Z2YCZDbzzzjsLUDaAuUrdP4M9MwAAwGzyZulbd3/G3aPuHr388svDLgcoSDVP18x4alHzdM2M91P3z2DPDAAAMJuFWPr2LUlrUo5XJ8+lazNsZpdK+rgmJ4pfzLUAsqDm6Rq9efpNNW9pnrFqVM3TNRr88qCkmftnTGHPDAAAcCEL8WTjFUkRM7vWzEo1OeF71zltdklqSr6ul7TPJ5fB2iXpbjMrM7NrJUUk/XoBagIwR/H342mfWsTfj0+3mdo/IxV7ZgAAgAvJOGwk52B8RVKfpN9IesHdj5jZY2b22WSzHZJWmFlc0jZJjyavPSLpBUmvSfqZpBZWogLCcTFPLWpX16qzr3PG7uGdfZ2qXV2b7XIBAEAeWJAdxN29V1LvOee+kfJ6TNJfXODab0v69kLUAWD+Unf9nnLuU4vdDbsV64ppx0s7NHZmjD0zAADAR1qQsAEg/1Utq5rc2fv6WzSYGNTxk8dVtqhMS0qWzGhHsAAAABcrb1ajAhCswS8PapEv0oHXDuiuT92l7z/4fTVvadZZO6uW3pawywMAAHmIsAFg2viH4+dNEm+oa1DH4Y7ZLwYAADgHYQPAtNHx0bSTxEfHR0OqCAAA5DPCBoBpFWUVaZe2rSirCKkiAACQzwgbAKY1rW9SV1/XjKVtu/q61LS+afaLAQAAzsFqVACmtd/eLkna2btTo+OjqiirUNP6JrXf3q7uwW617W9T/ERcVSuq1LqhVVtrtoZcMQAAyGU2uZF3folGoz4wMBB2GUDR6B7s1ra921S/sV6VqyqVGEmoZ1+Ptt+6ncABAChYZnbQ3aNh15HPGEYFYFZt+9tUv7F+xipV9Rvr1ba/LezSAABADiNsAJhV/EQ87SpV8RPxkCoCAAD5gLABYFZVK6rSrlJVtaIqpIoAAEA+YII4UMRaelvUcbjjvMng52rd0Do9Z+PVoVd1aOiQfj/+ey0pW6KW3pa01wAAABA2gCLV0tui5448p/tuv2960ndXX5cknRcepiaBP9j7oD685EPdf/v9s14zJdYVU/9wv8bOjKm8tFy1q2u1u2F3gJ8MAADkCoZRoeB1D3arur1aJY+VqLq9Wt2D3WGXlBM6Dneooa5hxqTvhroGdRzuSNt+a81WnfWzaqxrvOhrYl0xHTh2QM1bmvWDlh+oeUuzDhw7oFhXLMiPBgAAcgRhAwVtasnWTbWb9L0Hv6dNtZu0be+2og4cLb0tWvrkUo2Oj6ad9D06PnrBa+d6Tf9w/3nhpLGuUf3D/Zl/EAAAkPMIGyhoLNk6U+rQqSsvuzLtpO+KsooLXl9RVjGna8bOjKUNJ2Nnxub5CQAAQD4hbKCgsWTrTKlDpzbduEnP731eQ0eHNDExoaGjQ+rq61LT+qYLXr+8bLk6+zpnXNPZ16nlZcvTti8vLU8bTspLyxf0cwEAgNzEBHEUtKklWyNrItPninnJ1tRhUDdU3yBJevGXL+rYe8c+cjWqKe+Ovava9bV68Zcv6vjJ41q5fKVuuf4W9R9OPyyqdnWtOvs61VjXOD2hvLOvU7Wraxf+wwEAgJxD2EBBS12ydeovuz37erT91u1hlxaKqWFQU+Hrhuob9LElH9PO3p06/ejpGW3TrSI1dmZMm2/erC1/smW63cTEhPYM7En7++1u2K1YV0w7XtrBalQAABQhwgYK2tSSrW3729M8/bMAAAqbSURBVBQ/EVfViiptv3X79Pli07S+SV19XWqoa5ixdO25Q6dSV5FKfSKxqGRR2idFHzUsimABAEDxMncPu4Y5i0ajPjAwEHYZQF66mI38Fj+xWM1bmmeEiqGjQ3r2X59V6aLS84ZF3XLlLYQKAEDBMbOD7h4Nu458xpMNoMi0394+647fF1pFavyDcW1Ys4FhUQAA4KIQNgCcZ2oVqXTDpQgWAADgYrH0LYDzTK0ide4St6wiBQAA5oInGwDOwypSAABgIRA2AKRFsAAAAJliGBWQB1p6W7T0yaWyNtPSJ5eqpbdlwe4d64pp8ROLZW2mxU8sVqwrtmD3BgAAxY0nG0COa+lt0XNHntN9t983Y28MSbOuKjWbC+2nEeuK8WQDAABkjCcbQI7rONyhhroGRdZEVFJSosiaiBrqGtRxuCPje/cP96uxrnHGvRvrGtU/3L8AlQMAgGJH2ABy3Oj4aNo9L0bHRzO+94X20xg7M5bxvQEAAAgbQI6rKKtQYiQx41xiJKGKsoqM7z21n8a59y4vLc/43gAAAIQNIMc1rW9SV1/XjD0vuvq61LS+KeN7s58GAAAIEhPEgRw3NQl8Z+9OjY6PqqKsQk3rmzKeHC6xnwYAAAiWuXvYNcxZNBr1gYGBsMsAAABAATOzg+4eDbuOfMYwKgAAAACBIGwAAAAACARhAwAAAEAgCBsAUGC6B7tV3V6tksdKVN1ere7B7rBLAgAUKVajAoAC0j3YrW17t6l+Y70eWPWAEiMJbdu7TZK0tWZryNUBAIoNTzYAoIC07W9T/cZ6RdZEVFJSosiaiOo31qttf1vYpQEAihBhAwAKSPxEXJWrKmecq1xVqfiJeEgVAQCKGWEDAApI1YoqJUYSM84lRhKqWlEVUkUAgGJG2ACAAtK6oVU9+3o0dHRIExMTGjo6pJ59PWrd0Bp2aQCAIsQEcQAoIFOTwNv2tyl+Iq6qFVXafut2JocDAEJh7h52DXMWjUZ9YGAg7DIAAABQwMzsoLtHw64jnzGMCgAAAEAgCBsAAAAAAkHYAAAAABAIwgYAAACAQBA2AAAAAASCsAEAAAAgEIQNAAAAAIEgbAAAAAAIBGEDAAAAQCAIGwAAAAACQdgAAAAAEAjCBgAAAIBAEDYAAAAABIKwAQAAACAQhA0AAAAAgSBsAAAAAAgEYQMAAABAIAgbAAAAAAJB2AAAAAAQiIzChpldZmZ7zGwo+XX5Bdo1JdsMmVlTyvlvm9lRMzudSR0AAAAAck+mTzYelbTX3SOS9iaPZzCzyyS1SrpZ0k2SWlNCyb8kzwEAAAAoMJmGjTsldSRfd0j6XJo2dZL2uPt77n5S0h5JmyXJ3Q+4+9sZ1gAAAAAgB2UaNlamhIVjklamafMJSUdTjoeT5wAAAAAUsEtna2BmP5d0ZZq3vp564O5uZr5QhaWp40uSviRJV199dVC/DQAAAIAFMmvYcPfbLvSemR03s6vc/W0zu0rSb9M0e0vSp1OOV0v6xRzrlLs/I+kZSYpGo4GFGgAAAAALI9NhVLskTa0u1STpp2na9EmKmdny5MTwWPIcAAAAgAKWadh4UtImMxuSdFvyWGYWNbNnJcnd35P0LUmvJH89ljwnM/uumQ1LWmJmw2b2zQzrAQAAAJAjzD3/RiRFo1EfGBgIuwwAAAAUMDM76O7RsOvIZ+wgDgAAACAQhA0AAAAAgSBsAAAAAAgEYQMAAABAIAgbAAAAAAJB2AAAAAAQCMIGAAAAgEAQNgAAAAAEgrABAAAAIBCEDQAAAACBIGwAAAAACARhAwAAAEAgCBsAAAAAAkHYAAAAABAIwgYAAACAQBA2AAAAAASCsAEAAAAgEIQNAAAAAIEgbAAAAAAIBGEDAAAAQCAIGwAAAAACQdgAAAAAEAjCBgAAAIBAEDYAAAAABIKwAQAAACAQhA0AAAAAgSBsAAAAAAgEYQMAAABAIAgbAAAAAAJB2AAAAAAQCMIGAAAAgEAQNgAAAAAEwtw97BrmzMzekfRm2HUUqD+Q9G7YRSAv0XcwX/QdzBd9B/N1sX3nD9398qCLKWR5GTYQHDMbcPdo2HUg/9B3MF/0HcwXfQfzRd/JHoZRAQAAAAgEYQMAAABAIAgbONczYReAvEXfwXzRdzBf9B3MF30nS5izAQAAACAQPNkAAAAAEAjCRhEys8vMbI+ZDSW/Lr9Au6ZkmyEza0qeW2JmL5nZ/5jZETN7MrvVI9vMbLOZvW5mcTN7NM37ZWb2T8n3/9PMrkl572vJ86+bWV0260b45tt3zGyTmR00s8Hk143Zrh3hyuT7TvL9q83stJn9VbZqRm7I8GfWejP7j+TfbwbNrDybtRcqwkZxelTSXnePSNqbPJ7BzC6T1CrpZkk3SWpNCSXfd/frJP2RpD81s89kp2xkm5mVSGqX9BlJ10u6x8yuP6dZs6ST7l4l6YeSvpO89npJd0taJ2mzpKeS90MRyKTvaHLt+zvcvUZSk6Su7FSNXJBh35myXdK/BV0rckuGP7MulfScpL9093WSPi3pgyyVXtAIG8XpTkkdydcdkj6Xpk2dpD3u/p67n5S0R9Jmd/+du78sSe5+RtIhSauzUDPCcZOkuLsnkv+/n9dk/0mV2p96JN1qZpY8/7y7j7v7G5LiyfuhOMy777j7f7n7SPL8EUmLzawsK1UjF2TyfUdm9jlJb2iy76C4ZNJ3YpIOu/t/S5K7n3D3iSzVXdAIG8Vppbu/nXx9TNLKNG0+IeloyvFw8tw0M1sm6Q5NPh1BYZq1H6S2cfezkk5JWnGR16JwZdJ3Ut0l6ZC7jwdUJ3LPvPuOmS2V9DeS2rJQJ3JPJt931kpyM+szs0Nm9tdZqLcoXBp2AQiGmf1c0pVp3vp66oG7u5nNeUmy5OPGf5T09+6emF+VAHBhZrZOk0McYmHXgrzxTUk/dPfTyQcdwMW6VNKfSbpR0u8k7TWzg+7OP6hmiLBRoNz9tgu9Z2bHzewqd3/bzK6S9Ns0zd7S5HjFKasl/SLl+BlJQ+7+owUoF7nrLUlrUo5XJ8+lazOcDKEfl3TiIq9F4cqk78jMVkv6Z0mN7v6/wZeLHJJJ37lZUr2ZfVfSMkkfmtmYu/9D8GUjB2TSd4Yl7Xf3dyXJzHol/bEYvZExhlEVp12anHSp5NefpmnTJylmZsuTE8NjyXMys8c1+Yfz4SzUinC9IiliZteaWakmJ3zvOqdNan+ql7TPJzfw2SXp7uTKH9dKikj6dZbqRvjm3XeSQzRfkvSou/971ipGrph333H3Wne/xt2vkfQjSX9L0CgqmfzM6pNUk1x181JJn5L0WpbqLmiEjeL0pKRNZjYk6bbkscwsambPSpK7vyfpW5r8g/uKpMfc/b3kvzZ+XZOrPBwys1fN7IthfAgELzme9Sua/Cb8G0kvuPsRM3vMzD6bbLZDk2Ol45K2Kbm6mbsfkfSCJr9Z/0xSC5PtikcmfSd5XZWkbyS/x7xqZldk+SMgJBn2HRSxDH9mndTkKmavSHpVk3PFXsr2ZyhE7CAOAAAAIBA82QAAAAAQCMIGAAAAgEAQNgAAAAAEgrABAAAAIBCEDQAAAACBIGwAAAAACARhAwAAAEAgCBsAAAAAAvH/LfeRkJAGQuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = ft_model.wv[words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "P = pca.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(P[:, 0], P[:, 1], c='lightgreen', edgecolors='g')\n",
    "for label, x, y in zip(labels, P[:, 0], P[:, 1]):\n",
    "    plt.annotate(label, xy=(x+0.01, y+0.01), xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 3: Generate document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the tokenized keywords in cluster for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    #tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    #for token in tokens:\n",
    "    #    if re.search('[a-zA-Z]', token):\n",
    "    #        filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "8    None\n",
       "9    None\n",
       "Name: PaperTextClean, dtype: object"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalvocab_stemmed=[]\n",
    "df.PaperTextClean.apply(lambda x: totalvocab_stemmed.extend(tokenize_and_stem(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31376"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"welcome to stackoverflow my friend\", \n",
    "          \"my friend, don't worry, you can get help from stackoverflow\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "print(matrix)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "#https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python\n",
    "\"\"\" DEMO \"\"\"\n",
    "documents1 = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "documents =[(i,documents1[i]) for i in range(len(documents1))]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document[1].lower().split() if word not in stoplist] for document in documents]\n",
    "\n",
    "all_tokens = sum(texts, [])#tokens\n",
    "\n",
    "# remove words that appear only once\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "texts1 = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "\n",
    "# Create Dictionary.\n",
    "id2word = corpora.Dictionary(texts)\n",
    "# Creates the Bag of Word corpus.\n",
    "mm = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Trains the LDA models.\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=3, \\\n",
    "                               update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "# Prints the topics.\n",
    "for top in lda.print_topics():\n",
    "    print(top)\n",
    "\n",
    "\n",
    "# Assigns the topics to the documents in corpus\n",
    "lda_corpus = lda[mm]\n",
    "\n",
    "# Find the threshold, let's set the threshold to be 1/#clusters,\n",
    "# To prove that the threshold is sane, we average the sum of all probabilities:\n",
    "scores = list(chain(*[[score for topic_id,score in topic] for topic in [doc for doc in lda_corpus]]))\n",
    "\n",
    "[lda_corpus[i] for i in range(3)]\n",
    "threshold = sum(scores)/len(scores)\n",
    "\n",
    "print (threshold)\n",
    "print\n",
    "\n",
    "cluster1 = [j[0] for i,j in zip(lda_corpus,documents) if i[0][1] > threshold]\n",
    "cluster2 = [j[0] for i,j in zip(lda_corpus,documents) if i[1][1] > threshold]\n",
    "cluster3 = [j[0] for i,j in zip(lda_corpus,documents) if i[2][1] > threshold]\n",
    "\n",
    "print (cluster1)\n",
    "print (cluster2)\n",
    "print (cluster3)\n",
    "\n",
    "lda.show_topic(1, topn = 5)\n",
    "\n",
    "lda.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"./data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Id', 'Title', 'EventType', 'PdfName', 'Abstract',\n",
       "       'PaperText', 'AbstractClean', 'PaperTextClean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =[(i,j) for i,j in zip(df.Id, df.PaperTextClean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document[1].lower().split() if len(word)>3] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sum(texts, [])#tokens\n",
    "\n",
    "# remove words that appear only once\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1 and len(word)>3)\n",
    "texts1 = [[word for word in text if word not in tokens_once] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(texts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.006*\"distribution\" + 0.005*\"image\" + 0.005*\"matrix\" + 0.005*\"data\" + 0.005*\"learning\" + 0.004*\"loss\" + 0.004*\"using\" + 0.004*\"approach\" + 0.004*\"sequence\" + 0.004*\"algorithm\"')\n",
      "(1, '0.008*\"data\" + 0.006*\"algorithm\" + 0.006*\"number\" + 0.006*\"distribution\" + 0.006*\"learning\" + 0.005*\"matrix\" + 0.004*\"time\" + 0.004*\"mechanism\" + 0.004*\"clustering\" + 0.004*\"using\"')\n",
      "(2, '0.008*\"image\" + 0.006*\"algorithm\" + 0.005*\"number\" + 0.005*\"data\" + 0.005*\"clustering\" + 0.005*\"text\" + 0.004*\"vertices\" + 0.004*\"time\" + 0.004*\"learning\" + 0.004*\"algorithms\"')\n",
      "(3, '0.007*\"number\" + 0.006*\"learning\" + 0.005*\"loss\" + 0.005*\"distribution\" + 0.005*\"algorithm\" + 0.005*\"clustering\" + 0.004*\"data\" + 0.004*\"matrix\" + 0.004*\"covariance\" + 0.004*\"time\"')\n",
      "(4, '0.008*\"loss\" + 0.008*\"distribution\" + 0.008*\"learning\" + 0.005*\"training\" + 0.005*\"noise\" + 0.005*\"stability\" + 0.005*\"unhinged\" + 0.004*\"algorithm\" + 0.004*\"risk\" + 0.004*\"approach\"')\n",
      "(5, '0.009*\"algorithm\" + 0.007*\"learning\" + 0.007*\"mechanism\" + 0.006*\"stability\" + 0.005*\"distribution\" + 0.005*\"data\" + 0.005*\"number\" + 0.004*\"algorithmic\" + 0.004*\"loss\" + 0.004*\"given\"')\n",
      "(6, '0.009*\"learning\" + 0.008*\"algorithm\" + 0.007*\"data\" + 0.006*\"distribution\" + 0.006*\"stability\" + 0.006*\"number\" + 0.006*\"training\" + 0.005*\"algorithmic\" + 0.005*\"risk\" + 0.005*\"generalization\"')\n",
      "(7, '0.010*\"number\" + 0.008*\"distribution\" + 0.007*\"data\" + 0.007*\"learning\" + 0.006*\"covariance\" + 0.006*\"matrix\" + 0.005*\"algorithm\" + 0.005*\"loss\" + 0.005*\"ccadl\" + 0.004*\"sghmc\"')\n",
      "(8, '0.005*\"image\" + 0.005*\"model\" + 0.005*\"payment\" + 0.005*\"algorithm\" + 0.005*\"learning\" + 0.005*\"data\" + 0.004*\"sequence\" + 0.004*\"questions\" + 0.004*\"training\" + 0.004*\"using\"')\n",
      "(9, '0.008*\"learning\" + 0.008*\"loss\" + 0.006*\"data\" + 0.006*\"number\" + 0.006*\"distribution\" + 0.005*\"algorithm\" + 0.004*\"class\" + 0.004*\"given\" + 0.004*\"clustering\" + 0.003*\"section\"')\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary.\n",
    "id2word = corpora.Dictionary(texts1)\n",
    "# Creates the Bag of Word corpus.\n",
    "mm = [id2word.doc2bow(text) for text in texts1]\n",
    "\n",
    "# Trains the LDA models.\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10, \\\n",
    "                               update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "# Prints the topics.\n",
    "for top in lda.print_topics():\n",
    "    print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5677,\n",
       "  'double nothing multiplicative incentive mechanisms crowdsourcing nihar b shah university california berkeley nihareecs berkeley edu dengyong zhou microsoft research dengyong zhoumicrosoft com abstract crowdsourcing gained immense popularity machine learning applications obtaining large amounts labeled data crowdsourcing cheap fast suffers problem lowquality data address fundamental challenge crowdsourcing propose simple payment mechanism incentivize workers answer questions sure skip rest show surprisingly mild natural nofreelunch requirement mechanism one incentivecompatible payment mechanism possible also show among possible incentivecompatible mechanisms may may satisfy nofreelunch mechanism makes smallest possible payment spammers interestingly unique mechanism takes multiplicative form simplicity mechanism added benefit preliminary experiments involving several hundred workers observe significant reduction error rates unique mechanism lower monetary expenditure introduction complex machine learning tools deep learning gaining increasing popularity applied wide variety problems tools however require large amounts labeled data [ hdy ryz dds cbw ] large labeling tasks performed coordinating crowds semiskilled workers internet known crowdsourcing crowdsourcing means collecting labeled training data become indispensable engineering intelligent systems workers crowdsourcing experts consequence labels obtained crowdsourcing typically significant amount error [ kkkmf vdve wlc ] recent efforts focused developing statistical techniques postprocess noisy labels order improve quality e g [ ryz zlp kos ipsw ] however inputs algorithms erroneous difficult guarantee processed labels reliable enough subsequent use machine learning applications order avoid garbage garbage take complementary approach problem cleaning data time collection consider crowdsourcing settings workers paid services popular crowdsourcing platforms amazon mechanical turk others commercial platforms gained substantial popularity due support diverse range tasks machine learning labeling varying image annotation text recognition speech captioning machine translation consider problems objective nature definite answer figure depicts example question worker shown set images image worker required identify image depicts golden gate bridge golden gate bridge golden gate bridge yes yes im sure b figure different interfaces crowdsourcing setup conventional interface b option skip approach builds simple insight typical crowdsourcing setups workers simply paid proportion amount tasks complete result workers attempt answer questions sure thereby increasing error rate labels questions worker sure answers could unreliable [ wlc kkkmf vdve jsv ] ensure acquisition highquality labels wish encourage worker skip questions unsure instance providing explicit im sure option every question see figure b goal develop payment mechanisms encourage worker select option unsure term payment mechanism incentivizes worker incentive compatible addition incentive compatibility preventing spammers another desirable requirement incentive mechanisms crowdsourcing spammers workers answer randomly without regard question asked hope earning free money known exist large numbers crowdsourcing platforms [ wlc boh kkkmf vdve ] thus interest deter spammers paying low possible intuitive objective end ensure zero expenditure spammers answer randomly paper however impose strictly significantly weaker condition show one one incentivecompatible mechanism satisfy weak condition requirement referred nofreelunch axiom says questions attempted worker answered incorrectly payment must zero propose payment mechanism aforementioned setting incentive compatibility plus nofreelunch show surprisingly possible mechanism also show additionally mechanism makes smallest possible payment spammers among possible incentive compatible mechanisms may may satisfy nofreelunch axiom payment mechanism takes multiplicative form evaluation workers response question certain score final payment product scores mechanism additional appealing features simple compute also simple explain workers mechanism applicable type objective questions including multiple choice annotation questions transcription tasks etc order test whether mechanism practical assess quality final labels obtained conducted experiments amazon mechanical turk crowdsourcing platform preliminary experiments involved several hundred workers found quality data improved twofold unique mechanism total monetary expenditure lower compared conventional baseline problem setting crowdsourcing setting consider one workers perform task task consists multiple questions questions objective mean question precisely one correct answer examples objective questions include multiplechoice classification questions figure questions transcribing text audio images etc possible answer question define workers confidence answer probability according belief answer correct words one assume worker mind probability distribution possible answers question confidence answer probability answer correct shorthand also define confidence question confidence answer worker confident question assume workers confidences different questions independent goal every question worker incentivized skip confidence certain predefined threshold otherwise select answer thinks confident formally let predefined value goal design payment mechanisms incentivize worker skip questions confidence lower attempt confidence higher moreover questions attempts answer must incentivized select answer believes likely correct threshold may chosen based various factors problem hand example downstream machine learning algorithms using crowdsourced data knowledge statistics worker abilities etc paper assume threshold given us let n denote total number questions task among assume existence gold standard questions set questions whose answers known requester let g g n denote number gold standard questions g gold standard questions assumed distributed uniformly random pool n questions course worker know g n questions form gold standard payment worker task computed receiving responses questions task payment based workers performance gold standard questions since payment based known answers payments different workers depend thereby allowing us consider presence one worker without loss generality employ following standard notation positive integer k set k denoted [ k ] indicator function denoted e z z true otherwise notation r denotes set nonnegative real numbers let x xg denote evaluations answers worker gives g gold standard questions denotes worker skipped question denotes worker attempted answer question answer incorrect denotes worker attempted answer question answer correct let f g r denote payment function namely function determines payment worker based evaluations x xg note crowdsourcing platforms today mandate payments nonnegative let denote budget e maximum amount paid individual worker task max f x xg x xg amount thus amount compensation paid perfect agent work assume budget condition throughout rest paper assume worker attempts maximize overall expected payment follows expression workers expected payment refer expected payment workers point view expectation taken respect workers confidences answers uniformly random choice g gold standard questions among n questions task question [ n ] let yi worker attempts question set yi otherwise every question [ n ] yi let pi confidence worker answer selected question every question [ n ] yi let pi arbitrary value let e g g workers perspective expected payment selected answers confidencelevels g x x f yj g yjg pji pji n g j jg e g n expression outermost summation corresponds expectation respect randomness arising unknown choice gold standard questions inner summation corresponds expectation respect workers beliefs correctness responses event confidence question exactly equal worker may equally incentivized answer skip call payment function f incentivecompatible mechanism expected payment worker payment function strictly maximized worker responds manner desired main results incentivecompatible mechanism guarantees section present main results paper namely design incentivecompatible mechanisms practically useful properties end impose following natural requirement payment function f motivated practical considerations budget constraints discouraging spammers miscreants [ boh kkkmf vdve wlc ] term requirement nofreelunch axiom axiom nofreelunch axiom answers attempted worker gold standard wrong payment zero formally every set evaluations x xg satisfy pg pg xi xi require payment satisfy f x xg observe nofreelunch extremely mild requirement fact significantly weaker imposing zero payment workers answer randomly instance questions binarychoice format randomly choosing among two options question would result answers correct expectation nofreelunch axiom applicable none turns correct proposed multiplicative mechanism present proposed payment mechanism algorithm algorithm multiplicative incentivecompatible mechanism inputs threshold budget evaluations x xg g workers answers g gold standard questions pg pg let c xi w xi payment f x xg g c w proposed mechanism multiplicative form answer gold standard given score based whether correct score incorrect score skipped score final payment simply product scores scaled mechanism easy describe workers instance g cents description reads reward starts cents every correct answer gold standard questions reward double however questions answered incorrectly reward become zero please use im sure option wisely observe payment rule similar popular double nothing paradigm [ dou ] algorithm makes zero payment one attempted answers gold standard wrong note property significantly stronger property nofreelunch originally required wanted zero payment attempted answers wrong surprisingly prove shortly algorithm incentivecompatible mechanism satisfies nofreelunch following theorem shows proposed payment mechanism indeed incentivizes worker skip questions confidence answering confidence greater latter case worker incentivized select answer thinks likely correct theorem payment mechanism algorithm incentivecompatible satisfies nofreelunch condition payment function based gold standard questions also called strictly proper scoring rule [ gr ] proof theorem presented appendix easy see mechanism satisfies nofreelunch proof incentive compatibility also hard consider arbitrary worker arbitrary belief distributions compute expected payment worker case choices task follow requirements show choice leads strictly smaller expected payment started weak condition nofreelunch making zero payment attempted answers wrong mechanism proposed algorithm significantly strict makes zero payment attempted answers wrong natural question arises design alternative mechanism satisfying incentive compatibility nofreelunch operates somewhere uniqueness mechanism previous section showed proposed multiplicative mechanism incentive compatible satisfies intuitive requirement nofreelunch turns perhaps surprisingly mechanism unique respect theorem payment mechanism algorithm incentivecompatible mechanism satisfies nofreelunch condition theorem gives strong result despite imposing weak requirements see recall earlier discussion deterring spammers incurring low expenditure workers answer randomly instance task comprises binarychoice questions one may wish design mechanisms make zero payment responses questions gold standard incorrect nofreelunch axiom much weaker requirement mechanism satisfy requirement mechanism algorithm proof theorem available appendix b proof relies following key lemma establishes condition incentivecompatible mechanism must necessarily satisfy lemma applies incentivecompatible mechanism satisfying nofreelunch lemma incentivecompatible payment mechanism f must satisfy every g every yi yi yg g f yi yi yg f yi yi yg f yi yi yg proof lemma provided appendix c given lemma proof theorem completed via induction number skipped questions optimality spamming behavior discussed earlier crowdsouring tasks especially multiple choice questions often encounter spammers answer randomly without heed question asked instance binarychoice setup spammer choose one two options uniformly random every question highly desirable objective crowdsourcing settings deter spammers end one may wish impose condition zero payment responses attempted questions gold standard incorrect second desirable metric could minimize expenditure worker simply skips questions aforementioned requirements deterministic functions workers responses one may alternatively wish impose requirements depend distribution workers answering process instance third desirable feature would minimize expected payment worker answers questions uniformly random show interestingly unique multiplicative payment mechanism simultaneously satisfies requirements result stated assuming multiplechoice setup extends trivially nonmultiplechoice settings theorem distributional consider value g among incentivecompatible mechanisms may may satisfy nofreelunch algorithm strictly minimizes expenditure worker skips questions gold standard chooses answers remaining g questions uniformly random theorem b deterministic consider value b ] among incentivecompatible mechanisms may may satisfy nofreelunch algorithm strictly minimizes expenditure worker gives incorrect answers fraction b questions attempted gold standard proof theorem presented appendix see result multiplicative payment mechanism algorithm thus possesses useful properties geared deter spammers ensuring good worker paid high enough amount illustrate point let us compare mechanism algorithm popular additive class payment mechanisms example consider popular class additive mechanisms payments worker added across gold standard questions additive payment mechanism offers reward g every correct answer gold standard g every question skipped every incorrect answer importantly final payment worker sum rewards across g gold standard questions one verify additive mechanism incentive compatible one also see guaranteed theory additive payment mechanism satisfy nofreelunch axiom suppose question involves choosing two options let us compute expenditure two mechanisms make spamming behavior choosing answer randomly question given likelihood question correct compute additive mechanism makes payment expectation hand mechanism pays expected amount g payment spammers thus reduces exponentially number gold standard questions mechanism whereas reduce additive mechanism consider different means exploiting mechanism worker simply skips questions end observe worker skips questions additive payment mechanism incur expenditure hand proposed payment mechanism algorithm pays exponentially smaller amount g recall simulations experiments section present synthetic simulations realworld experiments evaluate effects setting mechanism final label quality synthetic simulations employ synthetic simulations understand effects various kinds labeling errors crowdsourcing consider binarychoice questions set simulations whenever worker answers question confidence correct answer drawn distribution p independent else investigate effects following five choices distribution p uniform distribution support [ ] triangular distribution lower endpoint upper endpoint mode beta distribution parameter values hammerspammer distribution [ kos ] uniform discrete set truncated gaussian distribution truncation n interval [ ] worker confidence p drawn distribution p attempts question probability making error equals p compare setting workers attempt every question b setting workers skip questions confidence certain threshold set simulations set either setting aggregate labels obtained workers question via majority vote two classes ties broken choosing one two options uniformly random figure error different interfaces synthetic simulations five distributions workers error probabilities figure depicts results simulations bar represents fraction questions labeled incorrectly average across trials standard error mean small visible see skipbased setting consistently outperforms conventional setting gains obtained moderate high depending underlying distribution workers errors particular gains quite striking hammerspammer model result surprising since mechanism ideally screens spammers leaves hammers answer perfectly experiments amazon mechanical turk conducted preliminary experiments amazon mechanical turk commercial crowdsourcing platform mturk com evaluate proposed scheme realworld scenarios complete data including interface presented workers tasks results obtained workers ground truth solutions available website first author goal delving details first note certain caveats relating study mechanism design crowdsourcing platforms worker encounters mechanism small amount time handful tasks typical research experiments small amount money dollars typical crowdsourcing tasks cannot expect worker completely understand mechanism act precisely required instance wouldnt expect experimental results change significantly even upon moderate modifications promised amounts furthermore expect outcomes noisy incentive compatibility kicks worker encounters mechanism across longer term example proposed mechanism adopted standard platform higher amounts involved would expect workers others e g bloggers researchers design strategies game mechanism theoretical guarantee incentive compatibility strict properness prevents gaming long run thus regard experiments preliminary intentions towards experimental exercise evaluate potential algorithms work practice b investigate effect proposed algorithms net error collected labelled data experimental setup conducted five following experiments tasks amazon mechanical turk identifying golden gate bridge pictures b identifying breeds dogs pictures c identifying heads countries identifying continents flags belong e identifying textures displayed images tasks comprised multi figure error different interfaces mechanisms five experiments conducted mechanical turk ple choice questions experiment compared baseline setting figure additive payment mechanism pays fixed amount per correct answer ii skipbased setting figure b multiplicative mechanism algorithm experiment two settings workers independently perform task upon completion tasks amazon mechanical turk aggregated data following manner mechanism experiment subsampled workers took majority vote responses averaged accuracy across questions across iterations subsampleandaggregate procedure results figure reports error aggregate data five experiments see cases skipbased setting results higher quality data many instances reduction twofold higher experiments observed substantial reduction amount error labelled data expending lower amounts receiving negative comments workers observations suggest proposed skipbased setting coupled multiplicative payment mechanisms potential work practice underlying fundamental theory ensures system cannot gamed long run discussion conclusions extended version paper [ sz ] generalize skipbased setting considered one also elicit workers confidence answers moreover companion paper [ szp ] construct mechanisms elicit support workers beliefs mechanism offers additional benefits pattern skips workers provide reasonable estimate difficulty question practice questions estimated difficult may delegated expert additional nonexpert workers secondly theoretical guarantees mechanism may allow better postprocessing data incorporating confidence information improving overall accuracy developing statistical aggregation algorithms augmenting existing ones e g [ ryz kos lpi zlp ] purpose useful direction research thirdly simplicity mechanisms may facilitate easier adoption among workers conclusion given uniqueness optimality theory simplicity good performance observed practice envisage multiplicative payment mechanisms interest practitioners well researchers employ crowdsourcing see extended version paper [ sz ] additional experiments involving freeform responses text transcription references [ boh ] john bohannon social science pennies science [ cbw ] andrew carlson justin betteridge richard c wang estevam r hruschka jr tom mitchell coupled semisupervised learning information extraction acm wsdm pages [ dds ] jia deng wei dong richard socher lijia li kai li li feifei imagenet largescale hierarchical image database ieee conference computer vision pattern recognition pages [ dou ] double nothing httpwikipedia orgwikidouble_or_nothing last accessed july [ gr ] tilmann gneiting adrian e raftery strictly proper scoring rules prediction estimation journal american statistical association [ hdy ] geoffrey hinton li deng dong yu george e dahl abdelrahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara n sainath et al deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine panagiotis g ipeirotis foster provost victor sheng jing wang repeated labeling using multiple noisy labelers data mining knowledge discovery [ ipsw ] [ jsv ] srikanth jagabathula lakshminarayanan subramanian ashwin venkataraman reputationbased worker filtering crowdsourcing advances neural information processing systems pages [ kkkmf ] gabriella kazai jaap kamps marijn koolen natasa milicfrayling crowdsourcing book search evaluation impact hit design comparative system ranking acm sigir pages [ kos ] david r karger sewoong oh devavrat shah iterative learning reliable crowdsourcing systems advances neural information processing systems pages [ lpi ] qiang liu jian peng alexander ihler variational inference crowdsourcing nips pages [ ryz ] vikas c raykar shipeng yu linda h zhao gerardo hermosillo valadez charles florin luca bogoni linda moy learning crowds journal machine learning research [ sz ] nihar b shah dengyong zhou double nothing multiplicative incentive mechanisms crowdsourcing arxiv [ szp ] nihar b shah dengyong zhou yuval peres approval voting incentives crowdsourcing international conference machine learning icml [ vdve ] jeroen vuurens arjen p de vries carsten eickhoff much spam take analysis crowdsourcing results increase accuracy acm sigir workshop crowdsourcing information retrieval pages [ wlc ] paul wais shivaram lingamneni duncan cook jason fennell benjamin goldenberg daniel lubarov david marin hari simons towards building highquality workforce mechanical turk nips workshop computational social science wisdom crowds [ zlp ] dengyong zhou qiang liu john c platt christopher meek nihar b shah regularized minimax conditional entropy crowdsourcing arxiv preprint arxiv'),\n",
       " (5941,\n",
       "  'learning symmetric label noise importance unhinged brendan van rooyen aditya krishna menon australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation de facto approach binary classification however long servedio [ ] proved symmetric label noise sln minimisation convex potential linear function class result classification performance equivalent random guessing ostensibly shows convex losses slnrobust paper propose convex classificationcalibrated loss prove slnrobust loss avoids long servedio [ ] result virtue negatively unbounded loss modification hinge loss one clamp zero hence call unhinged loss show optimal unhinged solution equivalent strongly regularised svm limiting solution convex potential implies strong ` regularisation makes standard learners slnrobust experiments confirm unhinged loss slnrobustness borne practice apologies wilde [ ] truth rarely pure simple learning symmetric label noise binary classification canonical supervised learning problem given instance space x samples distribution x goal learn scorer x r low misclassification error future samples drawn interest realistic scenario learner observes samples corruption labels constant probability flipped goal still perform well respect problem known learning symmetric label noise sln learning [ angluin laird ] long servedio [ ] showed exist linearly separable learner observes corruption symmetric label noise nonzero rate minimisation convex potential linear function class results classification performance equivalent random guessing ostensibly establishes convex losses slnrobust motivates use nonconvex losses [ stempfel ralaivola masnadishirazi et al ding vishwanathan denchev et al manwani sastry ] paper propose convex loss prove slnrobust loss avoids result long servedio [ ] virtue negatively unbounded loss modification hinge loss one clamp zero thus call unhinged loss loss several appealing properties unique convex loss satisfying notion strong slnrobustness proposition classificationcalibrated proposition consistent minimised proposition simple optimal solution difference two kernel means equation finally show optimal solution equivalent strongly regularised svm proposition twicedifferentiable convex potential proposition implying strong ` regularisation endows standard learners slnrobustness classifier resulting minimising unhinged loss new [ devroye et al chapter ] [ scholkopf smola section ] [ shawetaylor cristianini section ] however establishing classifiers strong slnrobustness uniqueness thereof equivalence highly regularised svm solution knowledge novel background problem setup fix instance space x denote distribution x random variables x may expressed via classconditionals p q p x p x base rate p via marginal p x classprobability function x p x x interchangeably write dpq dm classifiers scorers risks scorer function x r loss function ` r r use ` ` refer ` ` ` conditional risk l ` [ ] r r defined l ` v ` v ` v given distribution ` risk scorer defined ld ` [ ` x ] e xy ld ` e [ l ` x x ] set l ` set ` risks scorers xm function class f rx given f set restricted bayesoptimal scorers loss ` scorers f minimise ` risk sdf argmin ld ` ` sf set unrestricted bayesoptimal scorers sd sdf f rx restricted ` ` ` regret scorer excess risk restricted bayesoptimal scorer regretdf ld ` inf l ` ` tf binary classification concerned zeroone loss ` v jyv k jv k loss ` classificationcalibrated bayesoptimal scorers also optimal zeroone loss sd sd convex potential loss ` v yv r r ` convex nonincreasing differentiable [ long servedio definition ] convex potentials classificationcalibrated [ bartlett et al theorem ] learning symmetric label noise sln learning problem learning symmetric label noise sln learning following [ angluin laird kearns blum mitchell natarajan et al ] notional clean distribution would like observe instead observe samples corrupted distribution sln [ distribution sln marginal distribution instances unchanged label independently flipped probability goal learn scorer corrupted samples ld small quantity denote corrupted counterparts sln bar e g corrupted marginal distribution corrupted classprobability function additionally clear context occasionally refer sln easy check corrupted marginal distribution [ natarajan et al lemma ] x x x x slnrobustness formalisation consider learners ` f loss ` function class f learning search f minimises ` risk informally ` f robust symmetric label noise slnrobust minimising ` f gives classifier clean distribution learner would like observe sln [ learner actually observes formalise notion review known slnrobust learners slnrobust learners formal definition fixed instance space x let denote set distributions x given notional clean distribution nsln returns set possible corrupted versions learner may observe labels flipped unknown probability nsln sln equipped define notion slnrobustness definition slnrobustness say learner ` f slnrobust df df nsln ld ld ` ` slnrobustness requires level label noise observed distribution classification performance wrt learner learner directly observes unfortunately widely adopted class learners slnrobust see convex potentials linear function classes slnrobust fix x rd consider learners convex potential ` function class linear scorers flin x hw xi w rd captures e g linear svm logistic regression widely studied theory applied practice disappointingly learners slnrobust long servedio [ theorem ] give example learning symmetric label noise convex potential ` corrupted ` risk minimiser flin classification performance equivalent random guessing implies ` flin slnrobust per definition proposition long servedio [ theorem ] let x rd pick convex potential ` ` flin slnrobust fallout learners slnrobust light proposition two ways proceed order obtain slnrobust learners either change class losses ` change function class f first approach pursued large body work embraces nonconvex losses [ stempfel ralaivola masnadishirazi et al ding vishwanathan denchev et al manwani sastry ] losses avoid conditions proposition automatically imply slnrobust used flin appendix b present evidence losses fact slnrobust used flin second approach consider suitably rich f contains bayesoptimal scorer e g employing universal kernel choice one still use convex potential loss fact owing equation classificationcalibrated loss proposition pick classificationcalibrated ` ` rx slnrobust approaches drawbacks first approach computational penalty requires optimising nonconvex loss second approach statistical penalty estimation rates rich f require larger sample size thus appears slnrobustness involves computationalstatistical tradeoff however variant first option pick loss convex convex potential loss would afford computational statistical advantages minimising convex risks linear scorers manwani sastry [ ] demonstrated square loss ` v yv one loss show simpler loss convex slnrobust class convex potentials virtue negatively unbounded derive loss first reinterpret robustness via noisecorrection procedure even content difference [ ] clean corrupted minimisers performance long servedio [ theorem ] implies worst case noisecorrected loss perspective slnrobustness reexpress slnrobustness reason optimal scorers distribution two different losses help characterise set strongly slnrobust losses reformulating slnrobustness via noisecorrected losses given [ natarajan et al [ lemma ] showed associate loss ` noisecorrected counterpart ` ld ` l ` loss ` defined follows definition noisecorrected loss given loss ` [ noisecorrected loss ` v r ` v ` v ` v since ` depends unknown parameter directly usable design slnrobust learner nonetheless useful theoretical device since construction f sdf ` sdf sdf means sufficient condition ` f slnrobust sdf ` ` ` ghosh et al [ theorem ] proved sufficient condition ` holds namely c r v r ` v ` v c interestingly equation necessary stronger notion robustness explore characterising stronger notion slnrobustness first step towards stronger notion robustness rewrite slight abuse notation ld ` e xy [ ` x ] e ys r ds [ ` ] l ` r r distribution labels scores standard slnrobustness requires label noise change ` risk minimisers e l ` r l ` r relation holds place strong slnrobustness strengthens notion requiring label noise affect ordering pairs joint distributions labels scores course trivially implies slnrobustness definition given distribution r labels scores let r corresponding distribution labels flipped probability strong slnrobustness made precise follows definition strong slnrobustness call loss ` strongly slnrobust every [ r r l ` r l ` r l ` r l ` r reexpress strong slnrobustness using notion order equivalence loss pairs simply requires two losses order distributions labels scores identically order equivalent definition order equivalent loss pairs call pair losses ` ` r r l ` r l ` r l ` r l ` r clearly order equivalence ` ` implies sdf sdf turn implies slnrobustness ` ` thus surprising relate order equivalence strong slnrobustness ` proposition loss ` strongly slnrobust iff every [ ` ` order equivalent connection lets us exploit classical result decision theory order equivalent losses affine transformations combined definition ` lets us conclude sufficient condition equation also necessary strong slnrobustness ` proposition loss ` strongly slnrobust satisfies equation return original goal find convex ` slnrobust flin ideally general function classes suggests reasonable consider losses satisfy equation unfortunately evident ` convex nonconstant bounded zero cannot possibly admissible sense show removing boundedness restriction allows existence convex admissible loss unhinged loss convex strongly slnrobust loss consider following simple nonstandard convex loss unh ` unh v v ` v v compared hinge loss loss clamp zero e hinge thus peculiarly negatively unbounded issue discuss thus call unhinged loss loss number attractive properties immediate slnrobustness unhinged loss strongly slnrobust unh unh since ` unh strongly slnrobust thus v ` v proposition implies ` unh ` f slnrobust f following uniqueness property hard show proposition pick convex loss ` c r ` v ` v c b r ` v v b ` v v scaling translation ` unh convex loss strongly slnrobust returning case linear scorers implies ` unh flin slnrobust contradict proposition since ` unh convex potential negatively unbounded intuitively property allows loss offset penalty incurred instances misclassified high margin awarding gain instances correctly classified high margin unhinged loss classification calibrated slnrobustness insufficient learner useful example loss uniformly zero strongly slnrobust useless classificationcalibrated fortunately unhinged loss classificationcalibrated establish technical reasons see operate fb [ b b ] x set scorers range bounded b [ proposition fix ` ` unh dm b [ ` dfb x b sign x thus every b [ restricted bayesoptimal scorer fb sign bayesoptimal classifier loss limiting case f rx optimal scorer attainable operate extended reals r ` unh classificationcalibrated enforcing boundedness loss classificationcalibration ` unh encouraging proposition implies unrestricted bayesrisk thus regret every nonoptimal scorer identically hampers analysis consistency orthodox decision theory analogous theoretical issues arise attempting establish basic theorems unbounded losses [ ferguson pg ] sidestep issue restricting attention bounded scorers ` unh effectively bounded proposition affect classificationcalibration loss context linear scorers boundedness scorers achieved regularisation instead work ing flin one instead use flin x hw xi w flin fr r supxx x observe ` unh f slnrobust f ` unh flin slnrobust shall see working flin also lets us establish slnrobustness hinge loss large unhinged loss minimisation corrupted distribution consistent using bounded scorers makes possible establish surrogate regret bound unhinged loss shows classification consistency unhinged loss minimisation corrupted distribution loss considered sriperumbudur et al [ ] reid williamson [ ] context maximum mean discrepancy see appendix analysis slnrobustness knowledge novel proposition fix ` ` unh [ b [ scorer fb dfb regretd regret ` dfb regret ` standard rates convergence via generalisation bounds also trivial derive see appendix learning unhinged loss kernels show optimal solution unhinged loss employing regularisation kernelised scorers simple form sheds light slnrobustness regularisation centroid classifier optimises unhinged loss consider minimising unhinged risk class kernelised scorers fh x hw x ih w h x h feature mapping reproducing kernel hilbert space h kernel k equivalently given distribution want wunh argmin e [ hw x ] hw wih xy wh firstorder optimality condition implies wunh e [ x ] xy kernel mean map [ smola et al ] thus optimal unhinged scorer sunh x e [ k x x ] x e [ k x x ] e [ k x x ] xp xq xy equation unhinged solution equivalent nearest centroid classifier [ manning et al pg ] [ tibshirani et al ] [ shawetaylor cristianini section ] equation gives simple way understand slnrobustness ` unh fh optimal scorers clean corrupted distributions differ scaling see appendix e k x x x x e [ k x x ] xy xy interestingly servedio [ theorem ] established nearest centroid classifier termed average robust general class label noise required assumption uniform unit sphere result establishes sln robustness classifier holds without assumptions fact ghosh et al [ theorem ] lets one quantify unhinged loss performance general noise model see appendix discussion practical considerations note several points relating practical usage unhinged loss kernelised scorers first crossvalidation required select since changing changes magnitude scores sign thus purposes classification one simply use second easily extend scorers use bias regularised strength b tuning b equivalent computing sunh per equation tuning threshold holdout set third h rd small store wunh explicitly use make predictions high infinite dimensional h either make predictions directly via equation use random fourier features [ rahimi recht ] approximately embed h low dimensional rd store wunh usual latter requires translationinvariant kernel show assumptions wunh coincides solution two established methods appendix discusses relationships e g maximum mean discrepancy given training sample dn use plugin estimates appropriate equivalence highly regularised svm convex potentials interesting equivalence unhinged solution highly regularised svm noted e g hastie et al [ section ] showed svms approach nearest centroid classifier course optimal unhinged solution proposition pick x h r supxx x h let whinge argmin wh e xy [ max hw x ih ] hw wih softmargin svm solution r whinge wunh since ` unh fh slnrobust follows ` hinge v max yv ` hinge fh similarly slnrobust provided sufficiently large strong ` regularisation bounded feature map endows hinge loss slnrobustness proposition generalised show wunh limiting solution twice differentiable convex potential shows strong ` regularisation endows learners slnrobustness intuitively strong regularisation one considers behaviour loss near zero since convex potential behave similarly linear approximation around zero viz unhinged loss proposition pick bounded feature mapping x h twice differentiable convex potential [ ] bounded let w minimiser regularised risk w wunh lim w h wunh h h equivalence fisher linear discriminant whitened data binary classification dm fisher linear discriminant fld finds weight vector proportional minimiser square loss ` sq v yv [ bishop section ] wsq exm [ xxt ] e xy [ x ] wsq changed scaling equation fact corrupted marginal factor label noise provides alternate proof fact ` sq flin slnrobust [ manwani sastry theorem ] clearly unhinged loss solution wunh equivalent fld square loss solution wsq input data whitened e e xx xm wellspecified f e g universal kernel unhinged square loss asymptotically recover optimal classifier unhinged loss require matrix inversion misspecified f one cannot general argue superiority unhinged loss square loss viceversa universally good surrogate loss [ reid williamson appendix ] appendix illustrate examples losses may underperform slnrobustness unhinged loss empirical illustration illustrate unhinged loss slnrobustness empirically manifest reiterate high regularisation unhinged solution equivalent svm limit classificationcalibrated loss solution thus aim assert unhinged loss better losses rather demonstrate slnrobustness purely theoretical first show unhinged risk minimiser performs well example long servedio [ ] henceforth ls figure shows distribution x r marginal distribution three instances deterministically positive pick unhinged minimiser perfectly classifies three points regardless level label noise figure hinge minimiser perfect noise even small amount noise achieves error rate long servedio [ section ] show ` regularisation endow slnrobustness square loss escapes result long servedio [ ] since monotone decreasing unhinged hinge noise hinge noise hinge tlogistic unhinged table mean standard deviation error trials ls grayed cells denote best performer noise rate figure ls dataset next consider empirical risk minimisers random training sample construct training set instances injected varying levels label noise evaluate classification performance test set instances compare hinge tlogistic [ ding vishwanathan ] unhinged minimisers using linear scorer without bias term regularisation strength table even label noise unhinged classifier able find perfect solution contrast losses suffer even moderate noise rates next report results uci datasets additionally tune threshold ensure best training set accuracy table summarises results sample four datasets appendix contains results datasets performance metrics losses even noise close unhinged loss often able learn classifier discriminative power hinge tlogistic unhinged hinge tlogistic unhinged iris b housing hinge tlogistic unhinged c uspsv hinge tlogistic unhinged splice table mean standard deviation error trials uci datasets conclusion future work proposed convex classificationcalibrated loss proved robust symmetric label noise slnrobust showed unique loss satisfies notion strong slnrobustness established optimised nearest centroid classifier showed convex potentials svm also slnrobust highly regularised apologies wilde [ ] truth rarely pure simple acknowledgments nicta funded australian government department communications australian research council ict centre excellence program authors thank cheng soon ong valuable comments draft paper references dana angluin philip laird learning noisy examples machine learning peter l bartlett michael jordan jon mcauliffe convexity classification risk bounds journal american statistical association christopher bishop pattern recognition machine learning springerverlag new york inc avrim blum tom mitchell combining labeled unlabeled data cotraining conference computational learning theory colt pages vasil denchev nan ding hartmut neven v n vishwanathan robust classification adiabatic quantum optimization international conference machine learning icml pages luc devroye laszlo gyorfi gabor lugosi probabilistic theory pattern recognition springer nan ding v n vishwanathan tlogistic regression advances neural information processing systems nips pages curran associates inc thomas ferguson mathematical statistics decision theoretic approach academic press aritra ghosh naresh manwani p sastry making risk minimization tolerant label noise neurocomputing trevor hastie saharon rosset robert tibshirani ji zhu entire regularization path support vector machine journal machine learning research december issn michael kearns efficient noisetolerant learning statistical queries journal acm november philip long rocco servedio random classification noise defeats convex potential boosters machine learning issn christopher manning prabhakar raghavan hinrich schutze introduction information retrieval cambridge university press new york ny usa isbn naresh manwani p sastry noise tolerance risk minimization ieee transactions cybernetics june hamed masnadishirazi vijay mahadevan nuno vasconcelos design robust classifiers computer vision ieee conference computer vision pattern recognition cvpr nagarajan natarajan inderjit dhillon pradeep ravikumar ambuj tewari learning noisy labels advances neural information processing systems nips pages ali rahimi benjamin recht random features largescale kernel machines advances neural information processing systems nips pages mark reid robert c williamson composite binary losses journal machine learning research december mark reid robert c williamson information divergence risk binary experiments journal machine learning research mar bernhard scholkopf alexander j smola learning kernels volume mit press rocco servedio pac learning using winnow perceptron perceptronlike algorithm conference computational learning theory colt john shawetaylor nello cristianini kernel methods pattern analysis cambridge uni press alex smola arthur gretton le song bernhard scholkopf hilbert space embedding distributions algorithmic learning theory alt bharath k sriperumbudur kenji fukumizu arthur gretton gert r g lanckriet bernhard scholkopf kernel choice classifiability rkhs embeddings probability distributions advances neural information processing systems nips guillaume stempfel liva ralaivola learning svms sloppily labeled data artificial neural networks icann volume pages springer berlin heidelberg robert tibshirani trevor hastie balasubramanian narasimhan gilbert chu diagnosis multiple cancer types shrunken centroids gene expression proceedings national academy sciences oscar wilde importance earnest'),\n",
       " (6019,\n",
       "  'algorithmic stability uniform generalization ibrahim alabdulmohsin king abdullah university science technology thuwal saudi arabia ibrahim alabdulmohsinkaust edu sa abstract one central questions statistical learning theory determine conditions agents learn experience includes necessary sufficient conditions generalization given finite training set new observations paper prove algorithmic stability inference process equivalent uniform generalization across parametric loss functions provide various interpretations result instance relationship proved stability data processing reveals algorithmic stability improved postprocessing inferred hypothesis augmenting training examples artificial noise prior learning addition establish relationship algorithmic stability size observation space provides formal justification dimensionality reduction methods finally connect algorithmic stability size hypothesis space recovers classical pac result size complexity hypothesis space controlled order improve algorithmic stability improve generalization introduction one fundamental goal learning algorithm strike right balance underfitting overfitting mathematical terms often translated two separate objectives first would like learning algorithm produce hypothesis reasonably consistent empirical evidence e small empirical risk second would like guarantee empirical risk training error valid estimate true unknown risk test error former condition protects underfitting latter condition protects overfitting rationale behind two objectives understood define generalization risk rgen absolute difference empirical true risks rgen remp rtrue elementary observe true risk rtrue bounded sum remp rgen hence minimizing empirical risk underfitting generalization risk overfitting one obtains inference procedure whose true risk minimal minimizing empirical risk alone carried using empirical risk minimization erm procedure [ ] approximations however generalization risk often impossible deal directly instead common practice bound analyticaly establish conditions guaranteed small establishing conditions generalization one hopes design better learning algorithms perform well empirically generalize well novel observations future prominent example approach support vector machines svm algorithm binary classification [ ] however bounding generalization risk quite intricate approached various angles fact several methods proposed past prove generalization bounds including uniform convergence algorithmic stability rademacher gaussian complexities generic chaining bounds pacbayesian framework robustnessbased analysis [ ] concentration measure inequalities form building blocks rich theories proliferation generalization bounds understood look general setting learning introduced vapnik [ ] setting observation space z hypothesis space h learning algorithm henceforth denoted l h uses finite set z observations infer hypothesis h h general setting inference process endtoend influenced three key factors nature observation space z nature hypothesis space h details learning algorithm l imposing constraints three components one may able derive new generalization bounds example vapnikchervonenkis vc theory derives generalization bounds assuming constraints h stability bounds e g [ ] derived assuming constraints l given different generalization bounds established imposing constraints z h l intriguing ask exists single view generalization ties different components together paper answer question affirmative establishing algorithmic stability alone equivalent uniform generalization informally speaking inference process said generalize uniformly generalization risk vanishes uniformly across bounded parametric loss functions limit large training sets precise definition presented sequel show constraints imposed either h z l improve uniform generalization interpreted methods improving stability learning algorithm l similar spirit result kearns ron showed finite vc dimension hypothesis space h implies certain notion algorithmic stability inference process [ ] statement however general applies learning algorithms fall vapniks general setting learning well beyond uniform convergence rest paper follows first review current literature algorithmic stability generalization learnability introduce key definitions repeatedly used throughout paper next prove central theorem reveals algorithmic stability equivalent uniform generalization provide various interpretations result afterward related work perhaps two fundamental concepts statistical learning theory learnability generalization [ ] two concepts distinct discussed details next whereas learnability concerned measuring excess risk within hypothesis space generalization concerned estimating true risk order define learnability generalization suppose observation space z probability distribution observations p z bounded stochastic loss function l h z [ ] h h inferred hypothesis note l implicitly function parameterized h well define true risk hypothesis h h risk functional rtrue h ezp z l z h learning algorithm called consistent true risk inferred hypothesis h converges optimal true risk within hypothesis space h limit large training sets problem called learnable admits consistent learning algorithm [ ] known learnability supervised classification regression problems equivalent uniform convergence [ ] however shalevshwartz et al recently showed uniform convergence necessary vapniks general setting learning proposed algorithmic stability alternative key condition learnability [ ] unlike learnability question generalization concerned primarily representative empirical risk remp true risk rtrue elaborate suppose finite training set sm zi comprises observations zi p z define empirical risk hypothesis h respect sm x remp h sm l zi h zi sm also let rtrue h true risk defined eq learning algorithm l said generalize empirical risk inferred hypothesis converges true risk similar learnability uniform convergence definition sufficient generalization [ ] necessary learning algorithm always restrict search space smaller subset h artificially speak contrast known whether algorithmic stability necessary generalization shown various notions algorithmic stability defined sufficient generalization [ ] however known whether appropriate notion algorithmic stability defined necessary sufficient generalization vapniks general setting learning paper answer question showing stability inference process sufficient generalization fact equivalent uniform generalization notion generalization stronger one traditionally considered literature preliminaries simplify discussion always assume sets countable including observation space z hypothesis space h similar assumptions used previous works [ ] however main results presented section readily generalized addition assume learning algorithms invariant permutations training set hence order training examples irrelevant moreover x p x random variable drawn alphabet x f x function p x write exp x f x mean xx p x f x often simply write ex f x mean exp x f x distribution x clear context x takes values finite set uniformly random write x denote distribution x x boolean random variable x x true otherwise x general random variables denoted capital letters instances random variables denoted small letters alphabets denoted calligraphic typeface also given two probability mass functions p q defined alphabet write hp qi denote overlapping p coefficient e intersection p q hp qi aa min p q note hp qi p q p q total variation distance last write b k n nk k nk denote binomial distribution paper consider general setting learning introduced vapnik [ ] reiterate observation space z hypothesis space h learning algorithm l receives set observations sm zi z generated fixed unknown distribution p z picks hypothesis h h probability pl h h sm formally l h z stochastic map paper allow hypothesis h summary statistic training set measure central tendency unsupervised learning mapping input space output space supervised learning fact even allow h subset training set formal terms l stochastic map two random variables h h sm z exact interpretation random variables irrelevant learning task assume nonnegative bounded loss function l z h z [ ] used measure quality inferred hypothesis h h observation z z importantly assume l h z [ ] parametric definition parametric loss functions loss function l h z [ ] called parametric independent training set sm given inferred hypothesis h parametric loss function satisfies markov chain sm h l h fixed hypothesis h h define true risk rtrue h eq define empirical risk training set sm denoted remp h sm eq also define true empirical risks learning algorithm l expected risk inferred hypothesis rtrue l esm eh pl h sm rtrue h esm eh sm rtrue h remp l esm eh pl h sm remp h sm esm eh sm remp h sm simplify notation write rtrue remp instead rtrue l remp l consider following definition generalization definition generalization learning algorithm l h parametric z loss function l h z [ ] generalizes distribution p z z limm remp rtrue rtrue remp given eq eq respectively words learning algorithm l generalizes according definition empirical performance training loss becomes unbiased estimator true risk next define uniform generalization definition uniform generalization learning algorithm l h generalizes z uniformly exists distributions p z z parametric loss functions sample sizes remp l rtrue l uniform generalization stronger original notion generalization definition particular learning algorithm generalizes uniformly generalizes according definition well converse however true even though uniform generalization appears quite strong condition first sight key contribution paper show strong condition equivalent simple condition namely algorithmic stability main results prove algorithmic stability equivalent uniform generalization introduce probabilistic notion mutual stability two random variables order abstract away labeling information random variables might possess e g observation space may may metric space define stability impact observations probability distributions definition mutual stability let x x two random variables mutual stability x defined x hp x p p x ex hp p x ey hp x p x recall hp qi overlapping coefficient two probability distributions p q see x given definition indeed probabilistic measure mutual stability measures stable distribution observing instance x vice versa small value x means probability distribution x heavily perturbed single observation random variable perfect mutual stability achieved two random variables independent probabilistic notion mutual stability mind define stability learning algorithm l mutual stability inferred hypothesis random training example h learning algorithm receives definition algorithmic stability let l z finite set training examples sm zi z drawn fixed distribution p z let h pl h sm hypothesis inferred l let ztrn sm single random training example define stability l l inf p z h ztrn infimum taken possible distributions observations p z learning algorithm called algorithmically stable limm l note definition algorithmic stability rather weak requires contribution single training example overall inference process negligible sample size increases addition welldefined even learning algorithm deterministic hypothesis h deterministic function entire training set observations remains stochastic function individual observation illustrate concept following example example suppose observations zi bernoulli p trials p zi hypothesis produced l empirical average h zi p h km ztrn b k p h km ztrn b k shown using stirlings approximation [ ] algorithmic stability learning algorithm asymptotically given l achieved general statement proved later section next show notion algorithmic stability definition equivalent notion uniform generalization definition first state following lemma lemma data processing inequality let b c three random variables satisfy markov chain b c b c proof proof consists two steps first note markov chain implies p c b p c b b c b direct substitution definition second similar informationcannothurt inequality information theory [ ] shown b c c random variables b c proved using algebraic manipulationand minimum sums always larger &#9; thep p fact p sum minimums e min min combining results yields b b c c desired result ready state main result paper theorem learning algorithm l h algorithmic stability given defm z inition necessary sufficient uniform generalization see definition addition rtrue remp h ztrn l rtrue remp true empirical risks learning algorithm defined eq respectively proof outline proof first parametric loss function l h z [ ] random variable satisfies markov chain sm h l h independent ztrn sm hence empirical risk given remp el h eztrn l h l ztrn h contrast true risk given rtrue el h eztrn p z l ztrn h difference rtrue remp el h eztrn l ztrn h eztrn l h l ztrn h sandwich righthand side upper lower bound note p z p z two distributions defined alphabet z f z [ ] bounded loss function ezp z f z ezp z f z p z p z p q total variation distance proof result immediately deduced considering two regions z z p z p z z z p z p z separately used deduce inequalities rtrue remp l h ztrn h ztrn l second inequality follows data processing inequality lemma whereas last inequality follows definition algorithmic stability see definition proves l algorithmically stable e l rtrue remp converges zero uniformly across parametric loss functions therefore algorithmic stability sufficient uniform generalization converse proved showing bounded exists parametric loss distribution p z l rtrue remp l therefore algorithmic stability also necessary uniform generalization interpreting algorithmic stability uniform generalization section provide several interpretations algorithmic stability uniform generalization addition show theorem recovers classical results learning theory algorithmic stability data processing relationship algorithmic stability data processing presented lemma given random variables b c markov chain b c always b c presents us qualitative insights design machine learning algorithms first suppose two different hypotheses h h say h contains less informative h markov chain sm h h holds example observations zi bernoulli trials h r empirical average given example h label occurs often training set h h hypothesis h contains strictly less information original training set h formally sm h h case h enjoys better uniform generalization bound h dataprocessing intuitively know result hold h less tied original training set h brings us following remark detailed proofs available supplementary file remark improve uniform generalization bound equivalently algorithmic stability learning algorithm postprocessing inferred hypothesis h manner conditionally independent original training set given h example postprocessing hypotheses common technique used machine learning includes sparsifying coefficient vector w rd linear methods wj set zero small absolute magnitude also includes methods proposed reduce number support vectors svm exploiting linear dependence [ ] data processing inequality methods improve algorithmic stability uniform generalization needless mention better generalization immediately translate smaller true risk empirical risk may increase inferred hypothesis postprocessed independently original training set second markov chain b c holds also obtain c b c applying data processing inequality reverse markov chain c b result improve algorithmic stability contaminating training examples artificial noise prior learning sm perturbed version training set sm sm sm h implies ztrn h ztrn h ztrn sm ztrn sm random training examples drawn uniformly random training set respectively brings us following remark remark improve algorithmic stability learning algorithm introducing artificial noise training examples applying learning algorithm perturbed training set example corrupting training examples artificial noise recent dropout method popular techniques neural networks improve generalization [ ] data processing inequality methods indeed improve algorithmic stability uniform generalization algorithmic stability size observation space next look size observation space z influences algorithmic stability first start following definition definition lazy learning learning algorithm l called lazy hypothesis h h mapped onetoone training set sm e mapping h sm injective lazy learner called lazy hypothesis equivalent original training set information content hence learning actually takes place one example instancebased learning h sm despite simple nature lazy learners useful practice useful theoretical tools well particular equivalence h sm data processing inequality algorithmic stability lazy learner provides lower bound stability possible learning algorithm therefore relate algorithmic stability uniform generalization size observation space quantifying algorithmic stability lazy learners size z usually infinite however introduce following definition effective set size definition countable space z endowed probability mass function p z effective p p size z w r p z defined ess [ z p z ] p z p z zz one extreme p z uniform finite alphabet z ess [ z p z ] z extreme p z kronecker delta distribution ess [ z p z ] proved next notion effective set size determines rate convergence empirical probability mass function true distribution distance measured total variation sense result allows us relate algorithmic stability property observation space z theorem let z countable space endowed probability mass function p z let sm set samples zi p z define psm z empirical probability mass function q induced drawing samples uniformly random sm esm p z psm z ess [ z p z ] ess [ z p z ] z effective size z see defm inition q addition learning algorithm l z h h ztrn p z ] ess [ z bound achieved lazy learners see definition special case theorem proved de moivre showed pempirical mean bernoulli trials probability success converges true mean rate proof outline proof first know p sm p p multinomial coefficient using relation p q p q multinomial series de moivres formula mean deviation binomial random variable [ ] shown algebraic manipulations x k esm p z psm z pk pk pmp k pk pk k using stirlings approximation factorial [ ] obtain simple asymptotic expression r r x pk pk ess [ z p z ] esm p z psm z k tight due tightness stirling approximation rest theorem follows markov chain sm sm h data processing inequality definition corollary given conditions theorem q z addition finite e z learning algorithm l l z proof finite observation space z maximum effective set size see definition z attained uniform distribution p z z intuitively speaking theorem corollary state order guarantee good uniform generalization possible learning algorithms number observations must sufficiently large cover entire effective size observation space z needless mention difficult achieve practice algorithmic stability machine learning algorithms must controlled order guarantee good generalization empirical observations similarly uniform generalization bound improved reducing effective size observation space using dimensionality reduction methods algorithmic stability complexity hypothesis space finally look hypothesis space influences algorithmic stability first look role size hypothesis space formalized following theorem theorem denote h h hypothesis inferred learning algorithm l z h following bound algorithmic stability always holds r r h h log h l h shannon entropy measured nats e using natural logarithms proof proof informationtheoretic let x mutual information r v x let sm z z zm random choice training set hx h sm h h sm h sm h h zi h z h h z z h conditioning reduces entropy e h b h r v b sm h x [ h zi h zi h ] [ h ztrn h ztrn h ] therefore ztrn h sm h average believed first appearance squareroot law statistical inference literature [ ] effective set size bernoulli distribution according definition given theorem agrees fact generalizes de moivres result next use pinskers q inequality [ ] states probability distributions p p q q p q p q total variation distance p q kullbackleibler divergence measured nats e using natural logarithms recall sm h p sm p h p sm h mutual information sm h p sm h p sm p h deduce pinskers inequality eq ztrn h p ztrn p h p ztrn h r r r r ztrn h sm h h h log h last line used fact x h x random variables x theorem reestablishes classical pac result finite hypothesis space [ ] terms algorithmic stability learning algorithm enjoy high stability size hypothesis space small terms uniform generalization states generalizationp risk learning algorithm bounded uniformly across parametric loss functions h h p log h h h shannon entropy h next relate algorithmic stability vapnikchervonenkis vc dimension despite fact vc dimension defined binaryvalued functions whereas algorithmic stability functional probability distributions exists connection two concepts show first introduce notion induced concept class exists learning algorithm l definition concept class c induced learning algorithm l h defined z set total boolean functions c z p ztrn z h p ztrn z h h intuitively every hypothesis h h induces total partition observation space z given boolean function definition h splits z two disjoint sets set values z posteriori less likely present training set given inferred hypothesis h set values complexity richness induced concept class c related algorithmic stability via vc dimension theorem let l h learning algorithm induced concept class c let z dv c c vc dimension c following bound holds dv c c p dv c c log l particular l algorithmically stable induced concept class c finite vc dimension proof bounded n proof relies fact algorithmic stability l supp z esm suphh ezp z ch z ezsm ch z ch z p ztrn z h p ztrn z final bound follows applying uniform convergence results [ ] conclusions paper showed probabilistic notion algorithmic stability equivalent uniform generalization informal terms learning algorithm called algorithmically stable impact single training example probability distribution final hypothesis always vanishes limit large training sets words inference process never depends heavily single training example algorithmic stability holds learning algorithm generalizes well regardless choice parametric loss function also provided several interpretations result instance relationship algorithmic stability data processing reveals algorithmic stability improved either postprocessing inferred hypothesis augmenting training examples artificial noise prior learning addition established relationship algorithmic stability effective size observation space provided formal justification dimensionality reduction methods finally connected algorithmic stability complexity richness hypothesis space reestablished classical pac result complexity hypothesis space controlled order improve stability hence improve generalization references [ ] v n vapnik overview statistical learning theory neural networks ieee transactions vol september [ ] c cortes v vapnik supportvector networks machine learning vol pp [ ] blumer ehrenfeucht haussler k warmuth learnability vapnikchervonenkis dimension journal acm jacm vol pp [ ] talagrand majorizing measures generic chaining annals probability vol pp [ ] mcallester pacbayesian stochastic model selection machine learning vol pp [ ] bousquet elisseeff stability generalization journal machine learning research jmlr vol pp [ ] p l bartlett mendelson rademacher gaussian complexities risk bounds structural results journal machine learning research jmlr vol pp [ ] j audibert bousquet combining pacbayesian generic chaining bounds journal machine learning research jmlr vol pp [ ] h xu mannor robustness generalization machine learning vol pp [ ] elisseeff pontil et al leaveoneout error stability learning algorithms applications natoasi series learning theory practice science series sub series iii computer systems sciences [ ] kutin p niyogi almosteverywhere algorithmic stability generalization error proceedings eighteenth conference uncertainty artificial intelligence uai [ ] poggio r rifkin mukherjee p niyogi general conditions predictivity learning theory nature vol pp [ ] kearns ron algorithmic stability sanitycheck bounds leaveoneout crossvalidation neural computation vol pp [ ] shalevshwartz shamir n srebro k sridharan learnability stability uniform convergence journal machine learning research jmlr vol pp [ ] l devroye l gyorfi g lugosi probabilistic theory pattern recognition springer [ ] v vapnik chapelle bounds error expectation support vector machines neural computation vol pp [ ] h robbins remark stirlings formula american mathematical monthly pp [ ] cover j thomas elements information theory wiley sons [ ] downs k e gates masters exact simplification support vector solutions jmlr vol pp [ ] wager wang p liang dropout training adaptive regularization nips pp [ ] stigler history statistics measurement uncertainty harvard university press [ ] p diaconis zabell closed form summation classical distributions variations theme de moivre statlstlcal science vol pp [ ] shalevshwartz bendavid understanding machine learning theory algorithms cambridge university press'),\n",
       " (6035,\n",
       "  'adaptive lowcomplexity sequential inference dirichlet process mixture models theodoros tsiligkaridis keith w forsythe massachusetts institute technology lincoln laboratory lexington usa ttsilill mit edu forsythell mit edu abstract develop sequential lowcomplexity inference procedure dirichlet process mixtures gaussians online clustering parameter estimation number clusters unknown apriori present easily computable closed form parametric expression conditional likelihood hyperparameters recursively updated function streaming data assuming conjugate priors motivated largesample asymptotics propose novel adaptive lowcomplexity design dirichlet process concentration parameter show number classes grow logarithmic rate prove largesample limit conditional likelihood data predictive distribution become asymptotically gaussian demonstrate experiments synthetic real data sets approach superior online stateoftheart methods introduction dirichlet process mixture models dpmm widely used clustering data neal rasmussen traditional finite mixture models often suffer overfitting underfitting data due possible mismatch model complexity amount data thus model selection model averaging required find correct number clusters model appropriate complexity requires significant computation highdimensional data sets large samples bayesian nonparametric modeling alternative approaches parametric modeling example dpmms automatically infer number clusters data via bayesian inference techniques use markov chain monte carlo mcmc methods dirichlet process mixtures made inference tractable neal however methods exhibit slow convergence convergence tough detect alternatives include variational methods blei jordan deterministic algorithms convert inference optimization approaches take significant computational effort even moderate sized data sets largescale data sets lowlatency applications streaming data need inference algorithms much faster require multiple passes data work focus lowcomplexity algorithms adapt sample arrive making highly scalable online algorithm learning dpmms based sequential variational approximation sva proposed lin authors wang dunson recently proposed sequential maximum aposterior map estimator class labels given streaming data algorithm called sequential updating greedy search sugs iteration composed greedy selection step posterior update step choice concentration parameter critical dpmms controls number clusters antoniak fast dpmm algorithms use fixed fearnhead daume kurihara et al imposing prior distribution sampling provides flexibility approach still heavily relies experimentation prior knowledge thus many fast inference methods dirichlet process mixture models proposed adapt data including works escobar west learning incorporated gibbs sampling analysis blei jordan gamma prior used conjugate manner directly variational inference algorithm wang dunson also account model uncertainty concentration parameter bayesian manner directly sequential inference procedure approach computationally expensive discretization domain needed stability highly depends initial distribution range values best knowledge first analytically study evolution stability adapted sequence online learning setting paper propose adaptive nonbayesian approach adapting motivated largesample asymptotics call resulting algorithm asugs adaptive sugs basic idea behind asugs directly related greedy approach sugs main contribution novel lowcomplexity stable method choosing concentration parameter adaptively new data arrive greatly improves clustering performance derive upper bound number classes logarithmic number samples prove sequence concentration parameters results adaptive design almost bounded finally prove conditional likelihood primary tool used bayesianbased online clustering asymptotically gaussian largesample limit implying clustering part asugs asymptotically behaves gaussian classifier experiments show method outperforms stateoftheart methods online learning dpmms paper organized follows section review sequential inference framework dpmms build upon introduce notation propose adaptive modification section probabilistic data model given sequential inference steps shown section contains growth rate analysis number classes adaptivelydesigned concentration parameters section contains gaussian largesample approximation conditional likelihood experimental results shown section conclude section sequential inference framework dpmm review sugs framework wang dunson online clustering nonparametric nature dirichlet process manifests modeling mixture models countably infinite components let observations given yi rd denote class label ith observation latent variable define available information time yi online sequential updating greedy search sugs algorithm summarized next completeness set calculate choose best class label yi arg maxhki p h update posterior distribution f yi using yi h parameters class h f yi h observation density conditioned class h ki number classes created time algorithm sequentially allocates observations yi classes based maximizing conditional posterior probability calculate posterior probability p h define variables def def lih yi p yi h ih p h bayes rule p h lih yi ih h ki considered fixed iteration updated fully bayesian manner according dirichlet process prediction predictive probability assigning observation yi class h mi h h ki ih h ki algorithm adaptive sequential updating greedy search asugs input streaming data yi rate parameter set k calculate ki update concentration parameter log n l yi ih b choose best label yi qh p ih l yi h c update posterior distribution end ih ih f yi pi mi h l l h counts number observations labeled class h time concentration parameter adaptation concentration parameter well known concentration parameter strong influence growth number classes antoniak experiments show sequential framework choice even critical choosing fixed online sva algorithm lin requires crossvalidation computationally prohibitive largescale data sets furthermore streaming data setting estimate data complexity exists impractical perform crossvalidation although parameter handled fully bayesian treatment wang dunson prespecified grid possible values take say l l l along prior distribution needs chosen advance storage updating matrix size ki l marginalization needed compute p h iteration thus propose alternative datadriven method choosing works well practice simple compute theoretical guarantees idea start prior distribution favors small shape posterior distribution using data define pi p posterior distribution formed time used asugs time let p p denote prior e g exponential distribution p e dependence trivial first step bayes rule pi p yi p pi ii ii given update made selection used next selection step mean distribution pi e e[ ] shown section distribution pi approximated gamma distribution shape parameter ki rate parameter log approximation ki log requiring storage update one scalar parameter ki iteration asugs algorithm summarized algorithm selection step may implemented sampling probability mass function qh posterior update step efficiently performed updating hyperparameters function streaming data case conjugate distributions section derives updates case multivariate gaussian observations conjugate priors parameters sequential inference unknown mean unknown covariance consider general case unknown mean covariance class probabilistic model parameters class given yi n n co w v n denotes multivariate normal distribution mean precision matrix w v wishart distribution degrees freedom scale matrix v follow normalwishart joint distribution model leads parameters rd closedform expressions lih yi due conjugacy tzikas et al calculate class posteriors conditional likelihoods yi given assignment class h previous class assignments need calculated first conditional likelihood yi given assignment class h history given z lih yi f yi h h dh due conjugacy distributions posterior h always form h n h h ch th w th h vh h ch h vh hyperparameters recursively computed new samples come form recursive computation hyperparameters derived appendix ease interpretation numerical stability define h w h vh vh h h inverse mean wishart distribution matrix natural interpretation covariance matrix class h iteration th component chosen parameter updates th class become ci ci ci c ci ci yi ci yi starting matrix h positive definite matrices h remain positive definite let us return calculation conditional likelihood iterated integration follows rh h det h lih yi h h rh h yi h yi h h def def rh ch ch detailed mathematical derivation conditional likelihood included appendix b remark new class h ki liki form initial choice hyperparameters r growth rate analysis number classes stability section derive model posterior distribution pn using largesample approximations allow us derive growth rates number classes sequence concentration parameters showing number classes grows e[ kn ] log n arbitarily small certain mild conditions probability density parameter updated jth step following fashion innovation class chosen j pj pj otherwise j dependent factors update shown independent factors absorbed normalization probability density choosing innovation class pushes mass toward infinity choosing class pushes mass toward zero thus possibility innovation probability grows undesired manner assess growth number def innovations rn kn simple assumptions likelihood functions appear naturally asugs algorithm assuming initial distribution p e distribution used step n qn proportional rn j j e make use limiting relation theorem following asymptotic behavior holds limn log qn j j log n proof see appendix c using theorem largesample model pn rn e log n suitably normalized recognizing gamma distribution shape parameter rn rate parameter log n rn mean given n log n use mean form choose class membership alg asymptotic approximation leads simple scalar update concentration parameter need discretization tracking evolution continuous probability distributions experiments approximation accurate recall innovation class labeled k kn nth step modeled updates randomly select previous class innovation new class sampling probability distrip k n bution qk p n k n n k note n kk mn k mn k represents number members class k time n assume data follows gaussian mixture distribution def pt k x h n h h h h prior probabilities h h parameters gaussian clusters define mixturemodel probability density function plays role predictive distribution x mn k def lnk lnk n kk probabilities choosing previous class innovation using equ proporp mn k n n tional kk n lnk yn n lnk yn n lnk yn respecn n n tively n denotes innovation probability step n n lnk yn n lnk yn n n n n n n n n positive proportionality factor n define likelihood ratio lr beginning stage n def ln lnk lnk conceptually mixture represents modeled distribution fitting currently observed data modes data observed reasonable expect lnk good model future observations lr ln yn large future observations wellmodeled fact expect lnk pt n discussed section ln yn n ln yn n lemma following bound holds n nl min n n yn n proof result follows directly simple calculation innovation random variable rn described random process associated probabilities transition n k rn p rn k rn n k rn def l lnk independent n depends initial choice hyperparameters discussed sec expectation rn majorized expectation similar random process rn based def transition probability n min rna instead n appendix shows random n sequence given ln yn n log n latter described modification polya urn process selection probability n asymptotic behavior rn related variables described following theorem theorem let n sequence realvalued random variables n satisfying n rna n n n ln yn n log n nonnegative integervalued random variables rn evolve according assume following n n ln yn pt k lnk p k q kullbackleibler divergence distributions p q n rn op log n n op log n proof see appendix e theorem bounds growth rate mean number class innovations concentration parameter n terms sample size n parameter bounded lr bounded kl divergence conditions thm manifest rate exponents experiments section shows conditions thm hold iterations n n n n fact assuming correct clustering mixture distribution lnkn converges true mixture distribution pt implying number class innovations grows log n sequence concentration parameters log n arbitrarily small asymptotic normality conditional likelihood section derive asymptotic expression conditional likelihood order gain insight steadystate algorithm let h denote true prior probability class h using bounds gamma function theorem batir follows lima ed normal convergence conditions algorithm pruning merging steps included classes h k correctly identified populated approximately ni h h observations time thus conditional class prior class h converges h ni h h virtue ih h according rh ch op log expect since h also expect h h according also h ed h ed h parameter updates imply h h h h follows strong law large numbers updates recursive implementations sample mean sample covariance matrix thus largesample approximation conditional likelihood becomes h h lim h h h lih yi limi det h e yi h h yi h det h used limu uc u ec conditional likelihood corresponds multivariate gaussian distribution mean h covariance matrix h similar asymptotic normality result recently obtained tsiligkaridis forsythe gaussian observations von n n h mises prior asymptotics mn h h h h h lnh n h h n n imply mixture distribution lnk converges true gaussian mixture distribution pt thus small expect pt k lnk n n validating assumption theorem experiments apply asugs learning algorithm synthetic class example real data set verify stability accuracy method experiments show value adaptation dirichlet concentration parameter online clustering parameter estimation since possible multiple clusters similar classes might created due outliers due particular ordering streaming data sequence add pruning merging step asugs algorithm done lin compare asugs asugspm sugs sugspm sva svapm proposed lin since shown lin sva svapm outperform blockbased methods perform iterative updates entire data set including collapsed gibbs sampling mcmc splitmerge truncationfree variational inference synthetic data set consider learning parameters class gaussian mixture equal variance training set made iid samples test set made iid samples clustering results shown fig showing asugsbased approaches stable svabased algorithms asugspm performs best identifies correct number clusters parameters fig b shows data loglikelihood test set averaged monte carlo trials mean variance number classes iteration asugsbased approaches achieve higher loglikelihood svabased approaches asymptotically fig provides numerical verification assumptions theorem expected predictive likelihood lik converges true mixture distribution pt likelihood ratio li yi bounded enough samples processed svapm asugs asugspm mean number classes avg joint loglikelihood asugs asugspm sugs sugspm sva svapm iteration iteration variance number classes sva iteration b figure clustering performance sva svapm asugs asugspm synthetic data set asugspm identifies clusters correctly b joint loglikelihood synthetic data mean variance number classes function iteration likelihood values evaluated heldout set samples asugspm achieves highest loglikelihood lowest asymptotic variance number classes real data set applied online nonparametric bayesian methods clustering image data used mnist data set consists training samples test samples sample ik pt k kl li yi sample l sample figure likelihood ratio li yi lik yi left l distance lik true ik mixture distribution pt right synthetic example see image handwritten digit total dimensions perform pca preprocessing reduce dimensionality dimensions kurihara et al use random subset consisting random samples training training set contains data digits approximately uniform proportion fig shows predictive loglikelihood test set mean images clusters obtained using asugspm svapm respectively note asugspm achieves higher loglikelihood values finds digits correctly using clusters svapm finds digits using clusters predictive loglikelihood asugspm sugspm svapm iteration b c figure predictive loglikelihood test set mean images clusters found using asugspm b svapm c mnist data set discussion although sva asugs methods similar computational complexity use decisions information obtained processing previous samples order decide class innovations mechanics methods quite different asugs uses adaptive motivated asymptotic theory sva uses fixed furthermore sva updates parameters components iteration weighted fashion asugs updates parameters mostlikely cluster thus minimizing leakage unrelated components parameter asugs affect performance much threshold parameter sva often leads instability requiring lots pruning merging steps increasing latency critical large data sets streaming applications crossvalidation would required set appropriately observe higher loglikelihoods better numerical stability asugsbased methods comparison sva mathematical formulation asugs allows theoretical guarantees theorem asymptotically normal predictive distribution conclusion developed fast online clustering parameter estimation algorithm dirichlet process mixtures gaussians capable learning single data pass motivated largesample asymptotics proposed novel lowcomplexity datadriven adaptive design concentration parameter showed leads logarithmic growth rates number classes experiments synthetic real data sets show method achieves better performance fast stateoftheart online learning dpmm methods references antoniak c e mixtures dirichlet processes applications bayesian nonparametric problems annals statistics batir n inequalities gamma function archiv der mathematik blei jordan variational inference dirichlet process mixtures bayesian analysis daume h fast search dirichlet process mixture models conference artificial intelligence statistics escobar west bayesian density estimation inference using mixtures journal american statistical association june fearnhead p particle filters mixture models uknown number components statistics computing kurihara k welling vlassis n accelerated variational dirichlet mixture models advances neural information processing systems nips lin dahua online learning nonparametric mixture models via sequential variational approximation burges c j c bottou l welling ghahramani z weinberger k q eds advances neural information processing systems pp curran associates inc neal r bayesian mixture modeling proceedings workshop maximum entropy bayesian methods statistical analysis volume pp neal r markov chain sampling methods dirichlet process mixture models journal computational graphical statistics june rasmussen c e infinite gaussian mixture model advances neural information processing systems pp mit press tsiligkaridis forsythe k w sequential bayesian inference framework blind frequency offset estimation proceedings ieee international workshop machine learning signal processing boston september tzikas g likas c galatsanos n p variational approximation bayesian inference ieee signal processing magazine pp november wang l dunson b fast bayesian inference dirichlet process mixture models journal computational graphical statistics'),\n",
       " (5978,\n",
       "  'covariancecontrolled adaptive langevin thermostat largescale bayesian sampling xiaocheng shang university edinburgh x shanged ac uk zhanxing zhu university edinburgh zhanxing zhued ac uk benedict leimkuhler university edinburgh b leimkuhlered ac uk amos j storkey university edinburgh storkeyed ac uk abstract monte carlo sampling bayesian posterior inference common approach used machine learning markov chain monte carlo procedures used often discretetime analogues associated stochastic differential equations sdes sdes guaranteed leave invariant required posterior distribution area current research addresses computational benefits stochastic gradient methods setting existing techniques rely estimating variance covariance subsampling error typically assume constant variance article propose covariancecontrolled adaptive langevin thermostat effectively dissipate parameterdependent noise maintaining desired target distribution proposed method achieves substantial speedup popular alternative schemes largescale machine learning applications introduction machine learning applications direct sampling entire largescale dataset computationally infeasible instance standard markov chain monte carlo mcmc methods [ ] well typical hybrid monte carlo hmc methods [ ] require calculation acceptance probability creation informed proposals based whole dataset order improve computational efficiency number stochastic gradient methods [ ] proposed setting bayesian sampling based random much smaller subsets approximate likelihood whole dataset thus substantially reducing computational cost practice welling teh proposed socalled stochastic gradient langevin dynamics sgld [ ] combining ideas stochastic optimization [ ] traditional brownian dynamics sequence stepsizes decreasing zero fixed stepsize often adopted practice choice article vollmer et al [ ] modified sgld msgld also introduced designed reduce sampling bias sgld generates samples first order brownian dynamics thus fixed timestep one show unable dissipate excess noise gradient approximations maintaining desired invariant distribution [ ] stochastic gradient hamiltonian monte carlo sghmc method proposed chen et al [ ] relies second order langevin dynamics incorporates parameterdependent diffusion matrix intended effectively offset stochastic perturbation gradient however difficult accommodate additional diffusion term practice first second authors contributed equally listed author order decided lot moreover pointed [ ] poor estimation may significant adverse influence sampling target distribution example effective system temperature may altered thermostat idea widely used molecular dynamics [ ] recently adopted stochastic gradient nosehoover thermostat sgnht ding et al [ ] order adjust kinetic energy simulation way canonical ensemble preserved e prescribed constant temperature distribution maintained fact sgnht method essentially equivalent adaptive langevin adlangevin thermostat proposed earlier jones leimkuhler [ ] molecular dynamics setting see [ ] discussions despite substantial interest generated methods mathematical foundation stochastic gradient methods incomplete underlying dynamics sgnht method [ ] taken leimkuhler shang [ ] together design discretization schemes high effective order accuracy sgnht methods designed based assumption constant noise variance article propose covariancecontrolled adaptive langevin ccadl thermostat handle parameterdependent noise improving robustness reliability practice effectively speed convergence desired invariant distribution largescale machine learning applications rest article organized follows section describe setting bayesian sampling noisy gradients briefly review existing techniques section considers construction novel ccadl method effectively dissipate parameterdependent noise maintaining correct distribution various numerical experiments performed section verify usefulness ccadl wide range largescale machine learning applications finally summarize findings section bayesian sampling noisy gradients typical setting bayesian sampling [ ] one interested drawing states posterior distribution defined x x rnd parameter vector interest x denotes entire dataset x likelihood prior distributions respectively introduce potential energy function u defining x exp u positive parameter interpreted proportional reciprocal temperature associated physical system e kb kb boltzmann constant temperature practice often set unity notational simplicity taking logarithm yields u log x log assuming data independent identically distributed logarithm likelihood calculated n x log x log xi n size entire dataset however already mentioned computationally infeasible deal entire largescale dataset timestep would typically required mcmc hmc methods instead order improve efficiency random much smaller e n n subset preferred stochastic gradient methods likelihood dataset given parameters approximated n nx log x log xri n xri ni represents random subset x thus noisy potential energy written n nx u log xri log n negative gradient potential referred noisy force e f u goal correctly sample gibbs distribution exp u [ ] gradient noise assumed gaussian mean zero unknown variance case one may rewrite noisy force p f u r typically diagonal matrix represents covariance matrix noise p r vector standard normal random variables note r actually equivalent n typical setting numerical integration withassociated stepsize hone p p hf h u r hu h h r therefore assuming constant covariance matrix e identity matrix sgnht method ding et al [ ] following underlying dynamics written standard ito stochastic differential equation sde system [ ] pdt p dp u dt hm dwpdt dwa p pnd kb dt colloquially dw dwa represent vectors independent wiener increments p often informally denoted n dti [ ] coefficient represents strength artificial noise added system improve ergodicity termed effective friction positive parameter proportional variance noise auxiliary variable r governed nosehoover device [ ] via negative feedback mechanism e instantaneous temperature average kinetic energy per degree freedom calculated kb pt pnd target temperature dynamical friction would decrease allowing increase temperature would increase temperature target coupling parameter referred thermal mass molecular dynamics setting proposition see jones leimkuhler [ ] sgnht method preserves modified gibbs stationary distribution p z exp h p exp z normalizing constant h p pt pu hamiltonian ah proposition tells us sgnht method adaptively dissipate excess noise pumped system maintaining correct distribution variance gradient noise need known priori long constant auxiliary variable able automatically find mean value fly however parameterdependent covariance matrix sgnht method would produce required target distribution ding et al [ ] claimed reasonable assume covariance matrix constant size dataset n large case variance posterior small magnitude posterior variance actually relate constancy however general constant simply assuming nonconstancy significant impact performance method notably stability measured largest usable stepsize therefore essential approach handle parameterdependent noise following section propose covariancecontrolled thermostat effectively dissipate parameterdependent noise maintaining target stationary distribution covariancecontrolled adaptive langevin thermostat mentioned previous section sgnht method dissipate noise constant covariance matrix covariance matrix becomes parameterdependent general parameterdependent covariance matrix imply required thermal equilibrium e system cannot expected converge desired invariant distribution typically resulting poor estimation functions parameters interest fact case clear whether exists invariant distribution order construct stochasticdynamical system preserves canonical distribution suggest adding suitable damping viscous term effectively dissipate parameterdependent gradient noise end propose following covariancecontrolled adaptive langevin ccadl thermostat pdt p p dp u dt h dw h pdtpdt dwa pt pnd kb dt proposition ccadl thermostat preserves modified gibbs stationary distribution p z exp h p exp proof fokkerplanck equation corresponding l p u p h p mp h p p p p p mp pt pnd kb insert fokkerplanck operator l see vanishes incorporation parameterdependent covariance matrix intended offset covariance matrix coming gradient approximation however practice one know priori thus instead one must estimate simulation task addressed section procedure related method used sghmc method proposed chen et al [ ] uses dynamics following form pdt p p dp u dt h dwapdt aih dwa shown sghmc method preserves gibbs canonical distribution p z exp h p although ccadl sghmc preserve respective invariant distributions let us note several advantages former latter practice ccadl sghmc require estimation covariance matrix simulation costly high dimension numerical experiments found simply using diagonal covariance matrix significantly reduced computational cost works quite well ccadl contrast difficult find suitable value parameter sghmc since one make sure matrix aih positive semidefinite one may attempt use large value effective friction andor small stepsize h however toolarge friction would essentially reduce sghmc sgld desirable pointed [ ] extremely small stepsize would significantly impact computational efficiency ii estimation covariance matrix unavoidably introduces additional noise ccadl sghmc nonetheless ccadl still effectively control system temperature e maintaining correct distribution momenta due use stabilizing nosehoover control sghmc poor estimation covariance matrix may lead significant deviations system temperature well distribution momenta resulting poor sampling parameters interest covariance estimation noisy gradients assumption noise stochastic gradient follows normal distribution apply similar method [ ] estimate covariance matrix associated noisy gradient let g x log x assume size subset n large enough central limit theorem hold n x g xri n ex [ g x ] n n cov[ g x ] covariance gradient given noisy stochastic pn gradient based current subset u n g xri log clean n algorithm covariancecontrolled adaptive langevin ccadl thermostat input h tt initialize p pt h estimate using eq pt pt u h h nn pt ht pt h ahn ptt pt nd h end full gradient u thus pn g xi log ex [ u ] ex [ u ] n u u n n e n n assuming change dramatically time use moving average update estimate v n x v g xri g g xri g n empirical covariance gradient g represents mean gradient log likelihood computed subset proved [ ] estimator convergence order n already mentioned estimating full covariance matrix computationally infeasible high dimension however found employing diagonal approximation covariance matrix e estimating variance along dimension noisy gradient works quite well practice demonstrated section procedure ccadl method summarized algorithm simply used nd order consistent original implementation sgnht [ ] note simple first order terms stepsize algorithm recent article [ ] introduced higher order accuracy schemes improve accuracy interest direct comparison underlying machinery sghmc sgnht ccadl avoid modifications enhancements related timestepping stage following section compare newly established ccadl method sghmc sgnht various machine learning tasks demonstrate benefits ccadl bayesian sampling noisy gradient numerical experiments bayesian inference gaussian distribution first compare performance newly established ccadl method sghmc sgnht simple task using synthetic data e bayesian inference mean variance onedimensional normal distribution apply experimental setting [ ] generated n samples standard normal distribution n used likelihood function n xi assigned normalgamma distribution prior distribution e n gam corresponding posterior distribution another normalgamma distribution e x n n n gam n n n x n xi x n x n x n n n n n n n pn x xi n random subset size n selected timestep approximate full gradient resulting following stochastic gradients n n n x n n x u n x ri u xr n n seen variance stochastic gradient noise longer constant actually depends size subset n values iteration directly violates constant noise variance assumption sgnht [ ] ccadl adjusts varying noise variance marginal distributions obtained various methods different combinations h compared plotted figure table consisting corresponding root mean square error rmse distribution autocorrelation time samples cases sgnht ccadl easily outperform sghmc method possibly due presence nosehoover device sghmc showing superiority small value h large value neither desirable practice discussed section sgnht newly proposed ccadl method latter achieves better performance cases investigated highlighting importance covariance control parameterdependent noise true sghmc sgnht ccadl h b h true sghmc sgnht ccadl true sghmc sgnht ccadl density true sghmc sgnht ccadl density density true sghmc sgnht ccadl density true sghmc sgnht ccadl true sghmc sgnht ccadl density density density true sghmc sgnht ccadl density c h h figure comparisons marginal distribution density top row bottom row various values h indicated column peak region highlighted inset table comparisons rmse autocorrelation time various methods bayesian inference mean variance gaussian distribution methods h h h h sghmc sgnht ccadl largescale bayesian logistic regression consider bayesian logistic regression model trained benchmark mnist dataset binary classification digits using training data points test set size dimensional random projection original features used used likeliqn hood function xi yi n w exp yi w xi prior distribution w exp w w subset size n used timestep since dimensionality problem high full covariance estimation used ccadl investigate figure top row convergence speed method measuring test log likelihood using posterior mean number passes entire dataset ccadl displays significant improvements sghmc sgnht different values h ccadl converges much faster two also indicates faster mixing speed shorter burnin period ccadl shows robustness different values effective friction sghmc sgnht relying relative large value especially sghmc method intended dominate gradient noise compare sample quality obtained method figure bottom row plots twodimensional marginal posterior distribution randomly selected dimensions based samples method burnin period e start collect samples test log likelihood stabilizes true reference distribution obtained sufficiently long run standard hmc implemented runs standard hmc found variation runs guarantees qualification true reference distribution ccadl shows much better performance sghmc sgnht note contour sghmc even fit region plot fact shows significant deviation even estimation mean sghmc sghmc sgnht sgnht ccadl ccadl number passes true hmc sghmc sgnht ccadl w h number passes x true hmc sghmc sgnht ccadl w sghmc sghmc sgnht sgnht ccadl ccadl w w x true hmc sghmc sgnht ccadl number passes x sghmc sghmc sgnht sgnht ccadl ccadl w test log likelihood test log likelihood test log likelihood b h w c h figure comparisons bayesian logistic regression various methods mnist dataset digits various values h top row test log likelihood using posterior mean number passes entire dataset bottom row twodimensional marginal posterior distribution randomly selected dimensions fixed based samples method burnin period e start collect samples test log likelihood stabilizes magenta circle true reference posterior mean obtained standard hmc crosses represent sample means computed various methods ellipses represent isoprobability contours covering probability mass note contour sghmc well beyond scale plot especially large stepsize regime case include discriminative restricted boltzmann machine drbm drbm [ ] selfcontained nonlinear classifier gradient discriminative objective explicitly computed due limited space refer readers [ ] details trained drbm different largescale multiclass datasets libsvm dataset collection including connect letter sensit vehicle acoustic detailed information datasets presented table selected number hidden units using crossvalidation achieve best results since dimension parameters nd relatively high used diagonal covariance matrix estimation ccadl significantly reduce computational cost e estimating variance along dimension size subset chosen obtain reasonable variance estimation dataset chose first total number passes entire dataset burnin period collected remaining samples prediction table datasets used drbm corresponding parameter configurations datasets connect letter acoustic trainingtest set classes features hidden units total number parameters nd error rates computed various methods test set using posterior mean number passes entire dataset plotted figure observed sghmc sgnht work well large value effective friction corresponds strong random walk effect thus slows convergence contrary ccadl works httpwww csie ntu edu tw cjlinlibsvmtoolsdatasetsmulticlass html reliably much better two wide range importantly large stepsize regime speeds convergence rate relation computational work performed easily seen performance sghmc heavily relies using small value h large value significantly limits usefulness practice number passes test error sghmc sghmc sgnht sgnht ccadl ccadl number passes number passes acoustic h number passes sghmc sghmc sgnht sgnht ccadl ccadl number passes number passes b acoustic h number passes c letter h sghmc sghmc sgnht sgnht ccadl ccadl c connect h b letter h sghmc sghmc sgnht sgnht ccadl ccadl test error sghmc sghmc sgnht sgnht ccadl ccadl letter h number passes sghmc sghmc sgnht sgnht ccadl ccadl b connect h test error connect h test error test error sghmc sghmc sgnht sgnht ccadl ccadl test error test error test error sghmc sghmc sgnht sgnht ccadl ccadl test error sghmc sghmc sgnht sgnht ccadl ccadl number passes c acoustic h figure comparisons drbm datasets connect top row letter middle row acoustic bottom row various values h indicated test error rates various methods using posterior mean number passes entire dataset conclusions future work article proposed novel ccadl formulation effectively dissipate parameterdependent noise maintaining desired invariant distribution ccadl combines ideas sghmc sgnht literature achieves significant improvements methods practice additional error introduced covariance estimation expected small relative sense e substantially smaller error arising noisy gradient findings verified largescale machine learning applications particular consistently observed sghmc relies small stepsize h large friction significantly reduces usefulness practice discussed techniques presented article could use general settings largescale bayesian sampling optimization leave future work naive nonsymmetric splitting method applied ccadl fair comparison article however point optimal design splitting methods ergodic sde systems explored recently mathematics community [ ] moreover shown [ ] certain type symmetric splitting method adlangevinsgnht method clean full gradient inherits superconvergence property e fourth order convergence invariant distribution configurational quantities recently demonstrated setting langevin dynamics [ ] leave exploration direction context noisy gradients future work references [ ] abdulle g vilmart k c zygalakis long time accuracy lietrotter splitting methods langevin dynamics siam journal numerical analysis [ ] ahn korattikara welling bayesian posterior sampling via stochastic gradient fisher scoring proceedings th international conference machine learning pages [ ] brooks gelman g jones x l meng handbook markov chain monte carlo crc press [ ] chen e b fox c guestrin stochastic gradient hamiltonian monte carlo proceedings st international conference machine learning pages [ ] n ding fang r babbush c chen r skeel h neven bayesian sampling using stochastic gradient thermostats advances neural information processing systems pages [ ] duane kennedy b j pendleton roweth hybrid monte carlo physics letters b [ ] frenkel b smit understanding molecular simulation algorithms applications second edition academic press [ ] w g hoover computational statistical mechanics studies modern thermodynamics elsevier science [ ] horowitz generalized guided monte carlo algorithm physics letters b [ ] jones b leimkuhler adaptive stochastic methods sampling driven molecular systems journal chemical physics [ ] h larochelle bengio classification using discriminative restricted boltzmann machines proceedings th international conference machine learning pages [ ] b leimkuhler c matthews rational construction stochastic numerical methods molecular sampling applied mathematics research express [ ] b leimkuhler c matthews molecular dynamics deterministic stochastic numerical methods springer [ ] b leimkuhler c matthews g stoltz computation averages equilibrium nonequilibrium langevin molecular dynamics ima journal numerical analysis [ ] b leimkuhler x shang adaptive thermostats noisy gradient systems siam journal scientific computing [ ] n metropolis w rosenbluth n rosenbluth h teller e teller equation state calculations fast computing machines journal chemical physics [ ] nose unified formulation constant temperature molecular dynamics methods journal chemical physics [ ] h robbins monro stochastic approximation method annals mathematical statistics [ ] c robert g casella monte carlo statistical methods second edition springer [ ] j vollmer k c zygalakis w teh non asymptotic properties stochastic gradient langevin dynamics arxiv preprint arxiv [ ] welling w teh bayesian learning via stochastic gradient langevin dynamics proceedings th international conference machine learning pages'),\n",
       " (5714,\n",
       "  'robust portfolio optimization fang han department biostatistics johns hopkins university baltimore md fhanjhu edu huitong qiu department biostatistics johns hopkins university baltimore md hqiujhu edu han liu department operations research financial engineering princeton university princeton nj hanliuprinceton edu brian caffo department biostatistics johns hopkins university baltimore md bcaffojhsph edu abstract propose robust portfolio optimization approach based quantile statistics proposed method robust extreme events asset returns accommodates large portfolios limited historical data specifically show risk estimated portfolio converges oracle optimal risk parametric rate weakly dependent asset returns theory rely higher order moment assumptions thus allowing heavytailed asset returns moreover rate convergence quantifies size portfolio management allowed scale exponentially sample size historical data empirical effectiveness proposed method demonstrated synthetic real stock data work extends existing ones achieving robustness high dimensions allowing serial dependence introduction markowitzs meanvariance analysis sets basis modern portfolio optimization theory [ ] however meanvariance analysis criticized sensitive estimation errors mean covariance matrix asset returns [ ] compared covariance matrix mean asset returns influential harder estimate [ ] therefore many studies focus global minimum variance gmv formulation involves estimating covariance matrix asset returns estimating covariance matrix asset returns challenging due high dimensionality heavytailedness asset return data specifically number assets management usually much larger sample size exploitable historical data hand extreme events typical financial asset prices leading heavytailed asset returns overcome curse dimensionality structured covariance matrix estimators proposed asset return data [ ] considered estimators based factor models observable factors [ ] studied covariance matrix estimators based latent factor models [ ] proposed shrink sample covariance matrix towards highly structured covariance matrices including identity matrix order autoregressive covariance matrices onefactorbased covariance matrix estimators estimators commonly based sample covariance matrix sub gaussian tail assumptions required guarantee consistency heavytailed data robust estimators covariance matrices desired classic robust covariance matrix estimators include estimators minimum volume ellipsoid mve minimum covari ance determinant mcd estimators sestimators estimators based data outlyingness depth [ ] estimators specifically designed data low dimensions large sample sizes generalizing robust estimators high dimensions [ ] proposed orthogonalized gnanadesikankettenring ogk estimator extends [ ] estimator reestimating eigenvalues [ ] studied shrinkage estimators based tylers estimator however although ogk computationally tractable high dimensions consistency guaranteed fixed dimension shrunken tylors estimator involves iteratively inverting large matrices moreover consistency guaranteed dimension order sample size aforementioned robust estimators analyzed independent data points performance time series data questionable paper build quantilebased scatter matrix estimator propose robust portfolio optimization approach contributions three aspects first show proposed method accommodates high dimensional data allowing dimension scale exponentially sample size secondly verify consistency proposed method achieved without tail conditions thus allowing heavytailed asset return data thirdly consider weakly dependent time series demonstrate degree dependence affects consistency proposed method background section introduce notation system provide review grossexposure constrained portfolio optimization exploited paper notation let v v vd ddimensional real vector [ mjk ] rd matrix mjk j k entry q define ` q vector norm v pd kvkq j vj q ` vector norm v kvk maxdj vj let matrix qp ` max norm kmkmax maxjk mjk frobenius norm kmkf jk mjk let x x xd yd two random vectors write x x identically distributed use denote vectors every entry grossexposure constrained gmv formulation gmv formulation [ ] found imposing noshortsale constraint improves portfolio efficiency [ ] relaxed noshortsale constraint grossexposure constraint showed portfolio efficiency improved let x rd random vector asset returns portfolio characterized vector investment allocations w w wd among assets grossexposure constrained gmv portfolio optimization formulated min wt w w kwk c w covariance matrix x w budget constraint kwk c grossexposure constraint c called gross exposure constant controls percentage long short positions allowed portfolio [ ] optimization problem converted quadratic programming problem solved standard software [ ] method section introduce quantilebased portfolio optimization approach let z r random variable distribution function f zt tt sequence observations z constant q [ ] define qquantiles z zt tt q z q q f q inf z p z z q n b q z k k min q q z scatter matrix defined matrix proportional covariance matrix constant z z order statistics zt tt say q z q unique b q unique exists unique exists unique z p z z q say q z k z z zt z z following estimator qn [ ] define population sample quantilebased scales e b q z q z z bq zt tt q z zt stt q q ze independent copy z based b define robust scatter matrices asset returns detail let x x xd rd random vector representing returns assets xt tt sequence observations x xt xt xtd define population sample quantilebased scatter matrices qne bq bq rq [ rq jk ] r [ rjk ] b q given entries rq r q q b q bq xtj tt rjj xj r jj h q q rq x x x x j k j k jk h q q b q r b x x x x tj tk tj tk jk b q since bq computed using log time [ ] computational complexity r q b log since practice r computed almost efficiently sample covariance matrix complexity let w w wd vector investment allocations among assets matrix define risk function r rd rdd r r w wt mw x covariance matrix r w var wt x variance portfolio return wt x employed objected function gmv formulation however estimating difficult due heavy tails asset returns paper adopt r w rq robust alternative momentbased risk metric r w consider following oracle portfolio optimization problem wopt argmin r w rq w kwk c w kwk c grossexposure constraint introduced section practice rq b q onto cone unknown estimated convexity risk function project r positive definite matrices q b r e q argminr r r max r rdd mt min id max id e q optimization min max set lower upper bounds eigenvalues r problem solved projection contraction algorithm [ ] summarize e q formulate empirical robust portfolio algorithm supplementary material using r optimization e q w kwk c e opt argmin r w r w w remark robust portfolio optimization approach involves three parameters min max c empirically setting min max proves work well c typically provided investors controlling percentages short positions datadriven choice desired refer [ ] crossvalidationbased approach remark rationale behind positive definite projection lies two aspects first order portfolio optimization convex well conditioned positive definite matrix lower bounded eigenvalues needed guaranteed setting min secondly projection robust compared ogk estimate [ ] ogk induces positive definiteness reestimating eigenvalues using variances principal components robustness lost data possibly containing outliers projected onto principal directions estimating principal components remark adopt quantile definitions q bq achieve breakdown point however note methodology theory carries replaced absolute constant q theoretical properties section provide theoretical analysis proposed portfolio optimization approach b opt based estimate r rq next lemma shows error optimized portfolio w opt b rq r wopt rq essentially related estimation error r risks r w opt b lemma let w solution min r w r w kwk c w arbitrary matrix r b opt rq r wopt rq c kr rq kmax r w opt w solution oracle portfolio optimization problem c grossexposure constant e opt rq relates rate convergence next derive rate convergence r w q q e r kmax end first introduce dependence condition asset return series kr xt definition let xt tz stationary process denote f fn xt n fileds generated xt xt tn respectively mixing coefficient defined n sup p b p p b bf afn process xt tz mixing limn n condition xt rd tz stationary process j k xtj tz xtj xtk tz xtj xtk tz mixing processes satisfying n n n constant parameter determines rate decay n characterizes degree dependence xt tz next introduce identifiability condition distribution function asset returns f x e x ed independent copy x j k condition let x ej xj xk x ej x ek let fj fjk fjk distribution functions xj x e e xj xk xj xk assume exist constants inf f yq f dy f fj fjk fjk j k condition guarantees identifiability quantiles standard literature quantile statistics [ ] based conditions present rates convergence b q r e q r theorem let xt tz absolutely continuous stationary process satisfying conditions suppose log dt large enough probability smaller b q rq kmax rt kr rate convergence rt defined r n h c log log c rt max r q h c log log c io max q q q q p max max xj q xj xk xj xk j k c moreover r defined k k e q rq kmax rt kr implications theorem follows q p parameters max scale rate convergence reduces op log dt thus number assets management allowed scale exponentially sample size compared similar rates convergence obtained samplecovariancebased estimators [ ] require moment tail conditions thus accommodating heavytailed asset return data effect serial dependence p rate convergence characterized c specif ically approaches c k k increases towards infinity inflating rt allowed scale c log rate convergence rt inversely related lower bound marginal density functions around quantiles small distribution functions flat around quantiles making population quantiles harder estimate e opt rq combining lemma theorem obtain rate convergence r w theorem let xt tz absolutely continuous stationary process satisfying conditions suppose log dt rq large enough e opt rq r wopt rq c rt r w rt defined c grossexposure constant theorem shows risk estimated portfolio converges oracle optimal risk parametric rate rt number assets allowed scale exponentially sample size moreover rate convergence rely tail conditions distribution asset returns rest section build connection proposed robust portfolio optimization momentbased counterpart specifically show consistent elliptical model definition [ ] random vector x rd follows elliptical distribution location rd scatter rdd exist nonnegative random variable r matrix rdr rank r random vector u rr independent uniformly distributed rdimensional sphere sr x au aa rank r denote x ecd called generating variate commonly used elliptical distributions include gaussian distribution tdistribution elliptical distributions widely used modeling financial return data since naturally capture many stylized properties including heavy tails tail dependence [ ] next theorem relates rq r w rq momentbased counterparts r w elliptical model theorem let x x xd ecd absolutely continuous elliptical f x e x ed independent copy x random vector x rq mq q constant depending distribution x moreover e rq cq r w rq cq r w q cov x covariance matrix x c constant given n x x x n x x ej ej x ek j j k cq q q var xj var xj xk n x x x ej x ek j k q var xj xk last two inequalities hold var xj xk var xj xk theorem elliptical model minimizing robust risk metric r w rq equivalent minimizing standard momentbased risk metric r w thus robust portfolio optimization equivalent momentbased counterpart population level plugging leads following theorem theorem let xt tz absolutely continuous stationary process satisfying conditions suppose x ecd follows elliptical distribution covariance matrix log dt c e opt r wopt q rt r w c c grossexposure constant cq defined rt defined e opt obtained robust portfolio thus elliptical model optimal portfolio w optimization also leads parametric rate convergence standard momentbased risk experiments section investigate empirical performance proposed portfolio optimization approach section demonstrate robustness proposed approach using synthetic heavytailed data section simulate portfolio management using standard poors sp stock index data proposed portfolio optimization approach qne compared three competitors competitors constructed replacing covariance matrix commonly used covariancescatter matrix estimators ogk orthogonalized gnanadesikankettenring estimator constructs pilot scatter matrix estimate using robust estimator scale reestimates eigenvalues using variances principal components [ ] factor principal factor estimator iteratively solves specific variances factor loadings [ ] shrink shrinkage estimator shrinkages sample covariance matrix towards onefactor covariance estimator[ ] synthetic data following [ ] construct covariance matrix asset returns using threefactor model xj bj f bj f bj f j j xj return jth stock bjk loadings jth stock factor fk j idiosyncratic noise independent three factors model covariance matrix stock returns given bf bt diag b [ bjk ] matrix consisting factor loadings f covariance matrix three factors j variance noise adopt covariance simulations following [ ] generate factor loadings b trivariate normal distribution nd b b mean b covariance b specified table factor loadings generated fixed parameters throughout simulations covariance matrix f three factors also given table standard deviations idiosyncratic noises generated independently truncated gamma distribution shape scale restricting support [ standard deviations fixed parameters generated according [ ] parameters obtained fitting threefactor model using threeyear daily return data industry portfolios may aug covariance matrix fixed throughout simulations since interested risk optimization set mean asset returns dimension stocks consideration fixed given covariance matrix generate asset return data following three distributions multivariate gaussian distribution nd table parameters generating covariance matrix equation parameters factor loadings risk grossexposure constant c factor shrink grossexposure constant c gaussian qne ogk factor shrink matching rate qne ogk elliptical lognormal matching rate factor shrink grossexposure constant c multivariate gaussian qne ogk grossexposure constant c factor shrink oracle qne ogk risk factor shrink oracle qne ogk factor shrink oracle qne ogk risk f matching rate parameters factor returns b b grossexposure constant c multivariate grossexposure constant c elliptical lognormal figure portfolio risks selected number stocks matching rates oracle optimal portfolios multivariate distribution degree freedom covariance matrix elliptical distribution lognormal generating variate log n covariance matrix distribution generate asset return series half year estimate covariancescatter matrices using qne three competitors plug optimize portfolio allocations also solve true covariance matrix obtain oracle optimal portfolios benchmarks range grossexposure constraint c results based simulations b matching rates optimized portfolios figure shows portfolio risks r w oracle optimal portfolios matching rate defined follows two portfolios p p let corresponding sets selected assets e assets weights ws nonzero matching rate p p defined r p p denotes cardinality set note two observations figure four estimators leads comparable portfolio risks gaussian model however heavytailed distributions qne achieves lower portfolio risk ii matching rates qne stable across three models higher competing methods heavytailed distributions thus conclude qne robust heavy tails risk minimization asset selection real data section simulate portfolio management using sp stocks collect adjusted daily closing prices stocks stayed sp index january due ` regularization grossexposure constraint solution generally sparse adjusted closing prices accounts corporate actions including stock splits dividends rights offerings table annualized sharpe ratios returns risks competing approaches using sp index data sharpe ratio c c c c c c qne ogk factor shrink return c c c c c c risk c c c c c c december using closing prices obtain daily returns daily growth rates prices manage portfolio consisting stocks january december days optimize portfolio allocations using past months stock return data sample points hold portfolio one day evaluate portfolio return day way obtain portfolio returns repeat process four methods comparison range grossexposure constant c since true covariance matrix stock returns unknown adopt sharpe ratio evaluating performances portfolios table summarizes annualized sharpe ratios mean returns empirical risks e standard deviations portfolio returns observe qne achieves largest sharpe ratios values grossexposure constant indicating lowest risks returns equivalently highest returns risk discussion paper propose robust portfolio optimization framework building quantilebased scatter matrix obtain nonasymptotic rates convergence scatter matrix estimators risk estimated portfolio relations proposed framework momentbased counterpart well understood main contribution robust portfolio optimization approach lies robustness heavy tails high dimensions heavy tails present unique challenges high dimensions compared low dimensions example asymptotic theory estimators guarantees consistency rate p op dn even nongaussian data [ ] n statistical error diminishes rapidly increasing n however n statistical error may scale rapidly dimension thus stringent tail conditions subgaussian conditions required guarantee consistency momentbased estimators high dimensions [ ] paper based quantile statistics achieve consistency portfolio risk without assuming tail conditions allowing scale nearly exponentially n another contribution work lies theoretical analysis serial dependence may affect consistency estimation measure degree serial dependence using mixing coefficient n show effect serial dependence pon rate convergence summarized parameter c characterizes size n n drop data avoid financial crisis stock prices likely violate stationary assumption c imposes upper bound percentage short positions practice percentage short positions usually strictly controlled much lower references [ ] harry markowitz portfolio selection journal finance [ ] michael j best robert r grauer sensitivity meanvarianceefficient portfolios changes asset means analytical computational results review financial studies [ ] vijay kumar chopra william ziemba effect errors means variances covariances optimal portfolio choice journal portfolio management [ ] robert c merton estimating expected return market exploratory investigation journal financial economics [ ] jarl g kallberg william ziemba misspecifications portfolio selection problems risk capital pages springer [ ] jianqing fan yingying fan jinchi lv high dimensional covariance matrix estimation using factor model journal econometrics [ ] james h stock mark w watson forecasting using principal components large number predictors journal american statistical association [ ] jushan bai kunpeng li et al statistical analysis factor models high dimension annals statistics [ ] jianqing fan yuan liao martina mincheva large covariance estimation thresholding principal orthogonal complements journal royal statistical society series b statistical methodology [ ] olivier ledoit michael wolf improved estimation covariance matrix stock returns application portfolio selection journal empirical finance [ ] olivier ledoit michael wolf wellconditioned estimator largedimensional covariance matrices journal multivariate analysis [ ] olivier ledoit michael wolf honey shrunk sample covariance matrix journal portfolio management [ ] peter j huber robust statistics wiley [ ] ricardo maronna ruben h zamar robust estimates location dispersion highdimensional datasets technometrics [ ] ramanathan gnanadesikan john r kettenring robust estimates residuals outlier detection multiresponse data biometrics [ ] yilun chen ami wiesel alfred hero robust shrinkage estimation highdimensional covariance matrices ieee transactions signal processing [ ] romain couillet matthew r mckay large dimensional analysis optimization robust shrinkage covariance matrix estimators journal multivariate analysis [ ] ravi jagannathan risk reduction large portfolios imposing wrong constraints helps journal finance [ ] jianqing fan jingjin zhang ke yu vast portfolio selection grossexposure constraints journal american statistical association [ ] peter j rousseeuw christophe croux alternatives median absolute deviation journal american statistical association [ ] h xu h shao solving matrix nearness problem maximum norm applying projection contraction method advances operations research [ ] alexandre belloni victor chernozhukov ` penalized quantile regression highdimensional sparse models annals statistics [ ] lan wang yichao wu runze li quantile regression analyzing heterogeneity ultrahigh dimension journal american statistical association [ ] peter j bickel elizaveta levina covariance regularization thresholding annals statistics [ ] tony cai cunhui zhang harrison h zhou optimal rates convergence covariance matrix estimation annals statistics [ ] kaitai fang samuel kotz kai wang ng symmetric multivariate related distributions chapman hall [ ] harry joe multivariate models dependence concepts chapman hall [ ] rafael schmidt tail dependence elliptically contoured distributions mathematical methods operations research [ ] svetlozar todorov rachev handbook heavy tailed distributions finance elsevier [ ] svetlozar rachev christian menn frank j fabozzi fattailed skewed asset return distributions implications risk management portfolio selection option pricing wiley [ ] kevin dowd measuring market risk wiley [ ] torben gustav andersen handbook financial time series springer [ ] jushan bai shuzhong shi estimating high dimensional covariance matrices applications annals economics finance [ ] sara van de geer sa van de geer empirical processes estimation cambridge university press cambridge [ ] alastair r hall generalized method moments oxford university press oxford [ ] peter buhlmann sara van de geer statistics highdimensional data methods theory applications springer'),\n",
       " (5937,\n",
       "  'logarithmic time online multiclass prediction anna choromanska courant institute mathematical sciences new york ny usa achoromacims nyu edu john langford microsoft research new york ny usa jclmicrosoft com abstract study problem multiclass classification extremely large number classes k goal obtaining train test time complexity logarithmic number classes develop topdown tree construction approaches constructing logarithmic depth trees theoretical front formulate new objective function optimized node tree creates dynamic partitions data pure terms class labels balanced demonstrate favorable conditions construct logarithmic depth trees leaves low label entropy however objective function nodes challenging optimize computationally address empirical problem new online decision tree construction procedure experiments demonstrate online algorithm quickly achieves improvement test error compared common logarithmic training time approaches makes plausible method computationally constrained largek applications introduction central problem paper computational complexity setting number classes k multiclass prediction large problems occur natural language translation best search result best detection tasks almost machine learning algorithms exception decision trees running times multiclass classification k canonical example oneagainstall classifiers [ ] setting efficient possible accurate approach given information theory [ ] essence multiclass classification algorithm must uniquely specify bits labels predicts correctly consequently krafts inequality [ ] equation implies expected computational complexity predicting correctly h per example h shannon entropy label worst case distribution k classes implies log k computation required hence goal achieving log k computational time per example training testing effectively using online learning algorithms minimize passes data goal logarithmic k complexity naturally motivates approaches construct logarithmic depth hierarchy labels one label per leaf hierarchy sometimes available prior knowledge many scenarios needs learned well naturally leads partition problem arises node hierarchy partition problem finding classifier c x divides examples two subsets purer set labels original set definitions purity vary canonical examples number labels remaining subset softer notions average shannon entropy class labels despite resulting classifier problem fundamentally different standard binary classification see note replacing c x c x bad binary classification impact quality partition partition problem fundamentally nonconvex throughout paper logarithmic time mean logarithmic time per example problem bears parallels clustering regard symmetric classes since average c x c x c x c x poor partition always function places points side choice partition matters problem dependent ways example consider examples line label position threshold classifiers case trying partition class labels class label results poor performance accuracy partition problem typically solved decision tree learning via enumerateandtest approach amongst small set possible classifiers see e g [ ] multiclass setting desirable achieve substantial error reduction node tree motivates using richer set classifiers nodes minimize number nodes thereby decrease computational complexity main theoretical contribution work establish boosting algorithm learning trees k nodes log k depth thereby addressing goal logarithmic time train test complexity main theoretical result presented section generalizes binary boostingbydecisiontree theorem [ ] multiclass boosting boosting results performance critically dependent quality weak learner supporting intuition need sufficiently rich partitioners nodes approach uses new objective decision tree learning optimize node tree objective theoretical properties presented section complete system multiple partitions lomtree vs oneagainstall could constructed top boost oaa ing theorem bottom filter tree [ ] lomtree bottom partition process appears impossi ble representational constraints shown section supplementary material focus topdown tree creation whenever representational constraints partitions linear classifiers finding strong partition function requires efficient search set classifiers ef ficient searches large function classes routinely performed via gradient descent tech niques supervised learning seem number classes like natural candidate existing literature figure comparison oneagainst examples exist problem oaa logarithmic online multi indeed binary prespeciclass tree lomtree oneagainstall con fied hierarchy labels need strained use training time find partitioners aligned hierarchy lomtree dataset truncation lomtree con neither cases applieswe multistrained use representation complex ple labels want dynamically create ity oneagainstall number class choice partition rather assuming labels grows problem becomes harder one handed us exist purity criterion amenable gradient descent aplomtree becomes dominant proach precise objective studied theory fails test due discrete nature even natural approximations challenging tractably optimize computational constraints result use theoretical objective motivation construct new logarithmic online multiclass tree lomtree algorithm empirical evaluation creating tree online fashion creates new class problems node initially created eventually proves useless examples go best results wasteful solution practice starves parts tree need representational complexity deal design efficient process recycling orphan nodes locations needed prove number times node recycled logarithmic number examples algorithm described section analyzed section effective given inherent nonconvexity partition problem unavoidably empirical question answer range datasets varying k classes section find constrained training times approach quite effective compared baselines dominating log k train time approaches whats new best knowledge splitting criterion boosting statement lomtree algorithm swapping guarantee experimental results new prior work authors address logarithmic time training filter tree [ ] addresses consistent robust multiclass classification showing possible statistical limit filter tree address partition problem shown experimental section often helpful partition finding problem addressed conditional probability tree [ ] paper addresses conditional probability estimation conditional probability estimation converted multiclass prediction [ ] logarithmic time operation quite authors addressed logarithmic testing time allowing training time k worse approaches intractable larger scale problems describe context partition problem addressed recursively applying spectral clustering confusion graph [ ] clustering approaches include [ ] empirically approach found sometimes lead badly imbalanced splits [ ] context ranking another approach uses kmeans hierarchical clustering recover label sets given partition [ ] recent work [ ] multiclass classification problem addresses via sparse output coding tuning highcardinality multiclass categorization bitbybit decoding problem authors decouple learning processes coding matrix bit predictors use probabilistic decoding decode optimal class label authors however specify class similarity k compute see section [ ] hence approach different complexity class also born experimentally variant popular error correcting output code scheme solving multilabel prediction problems large output spaces assumption output sparsity also considered [ ] approach general requires k running time decode since essence fit label predictions must checked k labels another approach [ ] proposes iterative leastsquaresstyle algorithms multiclass multilabel prediction relatively large number examples data dimensions work [ ] focusing particular costsensitive multiclass classification approaches however k training time decision trees naturally structured allow logarithmic time prediction traditional decision trees often difficulties large number classes splitting criteria wellsuited large class setting however newer approaches [ ] addressed effectively significant scales context multilabel classification multilabel learning missing labels also addressed [ ] specifically first work [ ] performs brute force optimization multilabel variant gini index defined set positive labels node assumes label independence random forest construction method makes fast predictions however high training costs [ ] second work [ ] optimizes rank sensitive loss function discounted cumulative gain additionally wellknown problem hierarchical classification performance significantly deteriorates lower hierarchy [ ] authors solve biasing training distribution reduce error propagation simultaneously combining bottomup topdown approaches training [ ] reduction approach use optimizing partitions implicitly optimizes differential objective nonreductive approach tried previously [ ] objectives yielding good results different context framework theoretical analysis section describe essential elements approach outline theoretical properties resulting framework begin highlevel ideas setting employ hierarchical approach learning multiclass decision tree structure training structure topdown fashion assume receive examples x x rd labels k also assume access hypothesis class h h h binary classifier h x overall objective learn tree depth log k node tree consists classifier h classifiers trained way hn x hn denotes classifier node n tree means example x sent right subtree node n hn x sends x left subtree reach leaf predict according label highest frequency amongst examples reaching leaf paper skip index n whenever clear context consider fixed tree node interest computational complexity want encourage number examples going left right fairly balanced good statistical accuracy want send examples class almost exclusively either left right subtree thereby refining purity class distributions subsequent levels tree purity tree node therefore measure whether examples class reaching node mostly sent one child node pure split otherwise children impure split formal definitions balancedness purity introduced section objective expressing criteria resulting theoretical properties illustrated following sections key consideration picking objective want effectively optimize hypotheses h h streaming examples online fashion seems unsuitable standard decision tree objectives shannon gini entropy leads us design new objective time show section suitable assumptions optimizing objective also leads effective reduction average shannon entropy entire tree objective analysis resulting partitions define criterion measure quality hypothesis h h creating partitions fixed node n tree let denotes proportion label amongst examples reaching node let p h x p h x denote fraction examples reaching n h x marginally conditional class respectively define objective k x j h p h x p h x aim maximize objective j h obtain high quality partitions intuitively objective encourages fraction examples going right class substantially different background fraction class concrete simple scenario p h x hypothesis h objective prefers p h x close possible class leading pure partitions make intuitions formal definition purity hypothesis h h induces pure split k x min p h x p h x [ called purity factor particular partition called maximally pure meaning class sent exclusively left right define similar definition balancedness split definition balancedness hypothesis h h induces balanced split c p h x c z c ] called balancing factor partition called maximally balanced meaning equal number examples sent left right children partition balancing factor purity factor related shown lemma proofs lemma following lemma lemma deferred supplementary material lemma hypothesis h distribution examples x purity factor balancing factor satisfy min j h partition called maximally pure balanced satisfies see j h hypothesis h inducing maximally pure balanced partition captured next lemma course expect hypotheses producing maximally pure balanced splits practice lemma hypothesis h x objective j h satisfies j h [ ] furthermore h induces maximally pure balanced partition j h want objective achieve optimum simultaneously pure balanced split standard entropybased criteria shannon gini entropy well criterion propose posed equation satisfy requirement entropybased criteria see [ ] criterion see lemma algorithm could also implemented batch streaming case latter one example make one pass data per every tree level however massive datasets making multiple passes data computationally costly justifying need online approach proposed objective function exhibits similarities socalled carnaps measure [ ] used probability inductive logic quality entire tree section helps us understand quality individual split produced effectively maximizing j h next reason quality entire tree add nodes measure quality trees using average entropy leaves tree track decrease entropy function number nodes analysis extends theoretical analysis [ ] originally developed show boosting properties decision trees binary classification problems multiclass classification setting given tree consider entropy function gt measure quality tree k x x gt wl li ln li li probabilities randomly chosen data point x drawn p p fixed target distribution x label given x reaches node l l denotes set tree leaves denotes number internal tree nodes wl weight p leaf l defined probability randomly chosen x drawn p reaches leaf l note wl next state main theoretical result paper captured theorem adopt weak learning framework weak hypothesis assumption captured definition posits node tree hypothesis h hypothesis class h guarantees simultaneously weak purity weak balancedness split distribution p x assumption one use new decision tree approach drive error threshold definition weak hypothesis assumption let denote node tree let p hm x pmi p hm x furthermore let r min ] say weak hypothesis assumption satisfied distribution p x node tree exists hypothesis hm h pk j hm mi pmi theorem weak hypothesis assumption [ ] obtain gt suffices make ln k splits defer proof theorem supplementary material provide sketch analysis studies tree construction algorithm recursively find leaf node highest weight choose split two children let n heaviest leaf time consider splitting two children contribution node n tree entropy changes splits change entropy reduction corresponds gap jensens inequality applied concave function thus lowerbounded use fact shannon entropy strongly concave respect ` norm see e g example shalevshwartz [ ] obtained lowerbound turns depend proportionally j hn implies larger objective j hn time larger entropy reduction ends reinforces intuitions maximize j general might possible find hypothesis large enough objective j hn guarantee sufficient progress point appeal weak learning assumption assumption used lowerbound entropy reduction prove theorem lomtree algorithm objective function section another convenient form yields simple online algorithm tree construction training note equation written details shown section supplementary material j h ei [ ex [ h x ] ex [ h x ] ] maximizing objective discrete optimization problem relaxed follows j h ei [ ex [ h x ] ex [ h x ] ] ex [ h x ] expected score class next explain empirical approach maximizing relaxed objective empirical estimates expectations easily stored updated online every tree node decision whether send example reaching node left right child node based sign difference two expectations ex [ h x ] ex [ h x ] label data point e ex [ h x ] ex [ h x ] data point sent left else sent right procedure conveniently demonstrated toy example section supplement training algorithm assigns unique label node tree currently leaf label highest frequency amongst examples reaching leaf algorithm lomtree algorithm online tree training input regression algorithm r max number tree nonleaf nodes swap resistance rs subroutine setnode v mv mv sum scores class lv lv number points class reaching v nv nv number points class used train regressor v ev ev expected score class ev expected total score cv size smallest leaf subtree root v subroutine updatec v v r cparent v cv v parent v cv min cleft v cright v subroutine swap v find leaf cs cr spaparent sgpa grandpa ssibsibling spa left sgpa left sgpa ssib else right sgpa ssib updatec ssib setnode left v setnode spa right v spa create root r setnode r example x set j r lj mj lj nj ej lj j leaf lj least nonzero entries tt cj maxi lj rs cr tt setnode left j setnode right j else swap j cleft j bcj c cright j cj cleft j updatec left j j leaf ej ej c else c train hj example x c r x c pk mj nj mj hj x ej mj nj ej pi k nj set j child j corresponding hj else cj break testing test example pushed tree along path root leaf nonleaf node path regressor directs example either left right child node test example labeled label assigned leaf example descended training algorithm detailed algorithm tree node contains classifier use linear classifiers e hj regressor stored node j hj x value prediction hj example x stopping criterion expanding tree number nonleaf nodes reaches threshold swapping consider scenario current training example descends leaf j leaf split create two children examples reached past coming least two different smallest leaf one smallest total number data points reaching past parent v left v right v denote resp parent left right child node v grandpa v sibling v denote respectively grandparent node v sibling node v e node parent v implementation sums stored variables thus updating ev takes computations also refer prediction value score section r j r sgpa ssib j spa spa sgpa ssib figure illustration swapping procedure left swap right swap classes however number nonleaf nodes tree reaches threshold nodes expanded thus j cannot create children since tree construction done online nodes created early stages training may end useless examples reach later prevents potentially useful splits leaf j problem solved recycling orphan nodes subroutine swap algorithm general idea behind node recycling allow nodes split certain condition met particular node j splits following holds cj max k lj rs cr r denotes root entire tree cj size smallest leaf subtree root j smallest leaf one smallest total number data points reaching past lj kdimensional vector nonnegative integers ith element count number data points label reaching leaf j past finally rs swap resistance subtraction maxi k lj equation ensures pure node recycled condition inequality satisfied swap nodes performed orphan leaf reached smallest number examples past parent spa detached tree become children node j whereas old sibling ssib orphan node becomes direct child old grandparent sgpa swapping procedure shown figure condition captured inequality allows us prove number times given node recycled upperbounded logarithm number examples whenever swap resistance lemma lemma let swap resistance rs greater equal sequences examples number times algorithm recycles given node upperbounded logarithm base sequence length experiments address several hypotheses experimentally lomtree algorithm achieves true logarithmic time computation practice lomtree algorithm competitive better logarithmic traintest time algorithms multiclass classification lomtree algorithm statistical performance close common k approaches address hypotheses contable dataset sizes ducted experiments variety isolet sector aloi imnet odp benchmark multiclass datasets isosize mb mb mbgb gb let sector aloi imagenet im features k net odp details examples k datasets provided table datasets divided training classes k k testing furthermore training dataset used validation set baselines compared lomtree balanced random tree logarithmic depth rtree filter tree [ ] computationally feasible also compared oneagainstall classifier oaa representative k approach methods implemented vowpal wabbit [ ] learning system similar levels optimization regressors tree nodes lomtree rtree filter tree well oaa regressors trained online gradient descent explored step sizes chosen set used compressed details source dataset provided supplementary material linear regressors method investigated training passes data selected best setting parameters step size number passes one minimizing validation error additionally lomtree investigated different settings stopping criterion tree expansion k k k k k k k swap resistance rs table report respectively train time perexample test time best performer indicated bold training time later reported test error provided oaa imagenet odp due intractability petabyte scale computations table training time selected problems table perexample test time problems isolet sector aloi isolet sector aloi imnet odp lomtree lomtree ms ms ms ms ms oaa oaa ms ms ms log time ratio first hypothesis consistent experimental results timewise lomtree significantly outperforms oaa due building closeto logarithmic depth trees improvement training time increases number classes classification problem instance aloi training lomtree times faster oaa said test time perexample test time aloi imagenet odp respectively times faster oaa significant advantage lomtree oaa also captured figure next table best logarithmic time perlomtree vs oneagainstall former indicated bold report test error logarithmic traintest time algorithms also show binomial symmetrical confidence intervals results clearly sec ond hypothesis also consistent experimental results since rtree imposes random label partition resulting error ob tains generally worse error obtained competitor methods including lomtree learns label partitioning directly data time lomtree beats fil ter tree every dataset though imagenet log number classes figure logarithm ratio perexample odp high level noise advantage lomtree significant test times oaa lomtree problems table test error confidence interval problems lomtree rtree filter tree oaa isolet sector aloi imnet na odp na third hypothesis weakly consistent empirical results time advantage lomtree comes loss statistical accuracy respect oaa oaa tractable conclude lomtree significantly closes gap logarithmic time methods oaa making plausible approach computationally constrained largek applications conclusion lomtree algorithm reduces multiclass problem set binary problems organized tree structure partition every tree node done optimizing new partition criterion online criterion guarantees pure balanced splits leading logarithmic training testing time tree classifier provide theoretical justification approach via boosting statement empirically evaluate multiple multiclass datasets empirically find best available logarithmic time approach multiclass classification problems note however mechanics testing datastes much easier one simply test effectively untrained parameters examples measure test speed thus perexample test time oaa imagenet odp provided also best knowledge exist stateoftheart results oaa performance datasets published literature acknowledgments would like thank alekh agarwal dean foster robert schapire matus telgarsky valuable discussions references [ ] r rifkin klautau defense onevsall classification j mach learn res [ ] cover j thomas elements information theory john wiley sons inc [ ] l breiman j h friedman r olshen c j stone classification regression trees crc press llc boca raton florida [ ] kearns mansour boosting ability topdown decision tree learning algorithms journal computer systems sciences also stoc [ ] beygelzimer j langford p ravikumar errorcorrecting tournaments alt [ ] beygelzimer j langford lifshits g b sorkin l strehl conditional probability tree estimation analysis algorithms uai [ ] c bishop pattern recognition machine learning springer [ ] bengio j weston grangier label embedding trees large multiclass tasks nips [ ] g madzarov gjorgjevikj chorbev multiclass svm classifier utilizing binary decision tree informatica [ ] j deng satheesh c berg l feifei fast balanced efficient label tree learning large scale object recognition nips [ ] j weston makadia h yee label partitioning sublinear ranking icml [ ] b zhao e p xing sparse output coding largescale visual recognition cvpr [ ] hsu kakade j langford zhang multilabel prediction via compressed sensing nips [ ] agarwal kakade n karampatziakis l song g valiant least squares revisited scalable approaches multiclass prediction icml [ ] beijbom saberian kriegman n vasconcelos guessaverse loss functions costsensitive multiclass boosting icml [ ] r agarwal gupta prabhu varma multilabel learning millions labels recommending advertiser bid phrases web pages www [ ] prabhu varma fastxml fast accurate stable treeclassifier extreme multilabel learning acm sigkdd [ ] h f yu p jain p kar dhillon largescale multilabel learning missing labels icml [ ] liu yang h wan h j zeng z chen w support vector machines classification largescale taxonomy sigkdd explorations [ ] p n bennett n nguyen refined experts improving classification large taxonomies sigir [ ] montillo j tu j shotton j winn j e iglesias n metaxas criminisi entanglement differentiable information gain maximization decision forests computer vision medical image analysis [ ] k tentori v crupi n bonini osherson comparison confirmation measures cognition [ ] r carnap logical foundations probability nd ed chicago university chicago press par pp [ ] shalevshwartz online learning online convex optimization found trends mach learn [ ] j langford l li strehl httphunch net vw [ ] nesterov introductory lectures convex optimization basic course applied optimization kluwer academic publ [ ] j deng w dong r socher l j li k li l feifei imagenet largescale hierarchical image database cvpr'),\n",
       " (5802,\n",
       "  'planar ultrametrics image segmentation charless c fowlkes department computer science university california irvine fowlkesics uci edu julian yarkony experian data lab san diego ca julian yarkonyexperian com abstract study problem hierarchical clustering planar graphs formulate terms finding closest ultrametric specified set distances solve using lp relaxation leverages minimum cost perfect matching subroutine efficiently explore space planar partitions apply algorithm problem hierarchical image segmentation introduction formulate hierarchical image segmentation perspective estimating ultrametric distance set image pixels agrees closely input set noisy pairwise distances ultrametric space replaces usual triangle inequality ultrametric inequality u v max u w v w captures transitive property clustering u w cluster v w cluster u v must also cluster thresholding ultrametric immediately yields partition sets whose diameter less given threshold varying distance threshold naturally produces hierarchical clustering clusters high thresholds composed clusters lower thresholds inspired approach [ ] method represents ultrametric explicitly hierarchical collection segmentations determining appropriate segmentation single distance threshold equivalent finding minimumweight multicut graph positive negative edge weights [ ] finding ultrametric imposes additional constraint multicuts hierarchically consistent across different thresholds focus case input distances specified planar graph arises naturally domain image segmentation elements pixels superpixels distances defined neighbors allows us exploit fast combinatorial algorithms partitioning planar graphs yield tighter lp relaxations local polytope relaxation often used graphical inference [ ] paper organized follows first introduce closest ultrametric problem relation multicuts ultrametrics describe lp relaxation uses delayed column generation approach exploits planarity efficiently find cuts via classic reduction minimumweight perfect matching [ ] apply algorithm task natural image segmentation demonstrate algorithm converges rapidly produces optimal nearoptimal solutions practice closest ultrametric multicuts let g v e weighted graph nonnegative edge weights indexed edges e u v e goal find ultrametric distance uv vertices graph p close sense distortion uv e k uv uv k minimized begin reformulating closest ultrametric problem terms finding set nested multicuts family weighted graphs specify partitioning multicut vertices graph g components using binary vector x e xe indicates edge e u v cut vertices u v associated edge separate components partition use mcut g denote set binary indicator vectors x represent valid multicuts graph g notational simplicity remainder paper frequently omit dependence g given fixed input necessary sufficient condition indicator vector x define valid multicut g every cycle edges one edge cycle cut least one edge cycle must also cut let c denote set cycles g cycle c c set edges c e set edges cycle c excluding edge e express mcut terms cycle inequalities x e xe xe c c e c mcut x ece hierarchical clustering graph described nested collection multicuts denote space valid hierarchical partitions l layers l represent set l edgeindicator vectors x x x x x l cut edge remains cut finer layers hierarchy l x x x l x l mcut x l x l l given valid hierarchical clustering x ultrametric specified vertices graph choosing sequence real values l indicate distance threshold associated level l hierarchical clustering ultrametric distance specified pair x assigns distance pair vertices uv based coarsest level clustering remain separate clusters pairs corresponding edge graph u v e e write explicitly terms multicut indicator vectors l x de max l xel l [ xel xel ] l l l xel xe pairs u v correspond assume convention edge original graph still assigned unique distance based coarsest level l lie different connected components cut specified x l compute quality ultrametric respect input set edge weights measure squared l difference edge weights ultrametric distance k dk write compactly terms multicut pm indicator vectors construct set weights edge layer denoted el l el ke k weights given explicitly telescoping series e ke k l use r e el ke l k ke l k denote vector containing el l e e fixed number levels l fixed set thresholds problem finding closest ultrametric written integer linear program ilp edge cut indicators l l x x xx l l l min [ xe xe ] min ke l k xel xel e x l x l ee ee l l l x x l l l l l ke k ke k xe ke k xe min ke k xe x l min x l ee l x x l ee l el xel min x l l x l x l l optimization corresponds solving collection minimumweight multicut problems multicuts constrained hierarchically consistent linear combination cut vectors b hierarchical cuts figure partitioning x represented linear superposition cuts z cut isolates connected component partition assigned weight [ ] introducing auxiliary slack variables able represent larger set valid indicator vectors x using fewer columns z b introducing additional slack variables layer hierarchical segmentation efficiently represent many hierarchical segmentations x x x consistent layer layer using small number cut indicators columns z computing minimumweight multicuts also known correlation clustering np hard even case planar graphs [ ] direct approach finding approximate solution eq relax integrality constraints x l instead optimize whole polytope defined set cycle inequalities use l denote corresponding relaxation l resulting polytope convex hull mcut integral vertices correspond exactly set valid multicuts [ ] practice found applying straightforward cuttingplane approach successively adds violated cycle inequalities relaxation eq requires far many constraints slow useful instead develop column generation approach tailored planar graphs allows efficient accurate approximate inference cut cone planar multicuts consider partition planar graph two disjoint sets nodes denote space indicator vectors corresponding twoway cuts cut cut may yield two connected components produce every possible multicut e g split triangle three nodes three separate components let z e cut indicator matrix column specifies valid twoway cut zek edge e cut twoway cut k indicator vector multicut planar graph generated suitable linear combination cuts columns z isolate individual components rest graph weight cut let r cut vector specifying positive weighted combination cuts set cut z conic hull cut cut cone since multicut expressed superposition cuts cut cone identical conic hull mcut equivalence suggests lp relaxation minimumcost multicut given min z z vector r e specifies edge weights case planar graphs solution lp relaxation satisfies cycle inequalities see supplement [ ] expanded multicut objective since matrix z contains exponential number cuts eq still intractable instead consider approximation using constraint set z subset columns z previous work [ ] showed since optimal multicut may longer lie span reduced cut matrix z useful allow values z exceed see figure example introduce slack vector tracks presence overcut edges prevents contributing objective corresponding edge weight negative let e min e denote nonpositive component e expanded multicut objective given min z z edge e e decrease objective overcutting amount e exactly compensated objective term e e z contains cuts e z z eq eq equivalent [ ] minimizer eq z contains subset columns edge indicator vector given x min z still satisfies cycle inequalities see supplement details expanded lp finding closest ultrametric develop lp relaxation closest ultrametric problem replace multicut problem layer l expanded multicut objective described eq let l l denote collection weights slacks levels hierarchy let el max el el min el denote positive negative components l enforce hierarchical consistency layers would like add constraint z l z l however constraint rigid z include possible cuts thus computationally useful introduce additional slack vector associated level l edge e denote l introduction el allows cuts represented z l violate hierarchical constraint modify objective violations original hierarchy constraint paid proportion el introduction allows us find valid ultrametrics using smaller number columns z used would otherwise required illustrated figure b call relaxed closest ultrametric problem including slack variable expanded closest ultrametric objective written min l l l x x x l z l l l l l l l z l l z l l l l z l l l l convention define l dropped constant l term eq given solution recover relaxed solution closest ultrametric problem eq l setting xel min maxml z e supplement demonstrate obeys constraints eq thresholding operation yields solution x lies l achieves lower objective value dual objective optimize dual objective eq using efficient column generation approach based perfect matching introduce two sets lagrange multipliers l l corresponding within layer constraints respectively algorithm dual closest ultrametric via cutting planes z l l residual residual solve eq given z residual l l z l arg minzcut l l l l z residual residual l l l l z l z z z isocuts z l z l z l z z z end end notational convenience let dual objective written max l x l l l l l l l l l l l l l z l dual lp interpreted finding small modification original edge weights l every possible twoway cut resulting graph level l nonnegative weight observe introduction two slack terms primal problem eq results bounds lagrange multipliers dual problem eq practice dual constraints turn essential efficient optimization constitute core contribution paper solving dual via cutting planes chief complexity dual lp contained constraints including z encodes nonnegativity exponential number cuts graph represented columns z circumvent difficulty explicitly enumerating columns z employ cutting plane method efficiently searches additional violated constraints columns z successively added let z denote current working set columns dual optimization algorithm iterates following three steps solve dual lp z find violated constraint form l l l l z layer l append column matrix z cut found terminate violated constraints exist computational budget exceeded finding violated constraints identifying columns add z carried layer l separately finding violated constraint full problem corresponds computing minimumweight cut graph edge weights l l l l cut nonnegative weight constraints satisfied otherwise add corresponding cut indicator vector additional column z generate new constraint layer l based current lagrange multipliers solve x z l arg min el le el el ze zcut ee subsequently add new constraints layers lp z [ z z z z l ] unlike multicut problem finding twoway cut planar graph solved exactly reduction minimumweight perfect matching classic result e g provides exact solution ground state lattice ising model without ferromagnetic field [ ] n log n time [ ] ub lb bound counts time sec objective ratio ucm um figure average convergence upper blue lowerbounds red function running time values plotted gap bound best lowerbound computed termination given problem instance relative gap averaged problem instances yet converged given time point indicate percentage problem instances yet terminate using black bars marking [ ] percent b histogram ratio closest ultrametric objective values algorithm um baseline clustering produced ucm ratios less showing instances um produce worse solution ucm computing lower bound given iteration prior adding newly generated set constraints p compute total residual constraint violation layers hierarchy l l l l l z l supplement demonstrate value dual objective plus lowerbound relaxed closest ultrametric problem eq thus costs minimumweight matchings approach zero objective reduced problem z approaches accurate lowerbound optimization l expanding generated cut constraints given cut z l produces two connected components found useful add constraint corresponding component following approach [ ] let number connected components z l denoted components add one column z corresponding cut isolates connected component rest allows flexibility representing final optimum multicut superpositions components addition also found useful practice maintain separate set constraints z l layer l maintaining independent constraints z z z l result smaller overall lp speeding convergence found adding explicit penalty term objective encourages small values speeds convergence dramatically loss solution quality experiments penalty scaled parameter chosen extremely small magnitude relative values influence forces acting given term primal decoding algorithm gives summary dual solver produces lowerbound well set cuts described constraint matrices z l subroutine isocuts z l computes set cuts isolate connected component z l generate hierarchical clustering solve primal eq using reduced set z order recover fractional solution xel min maxml z e use lp solver ibm cplex provides primal solution free solving dual alg round fractional primal solution x discrete hierarchical clustering thresholding xel [ xel ] repair uncut cut edges lie inside connected component implementation test discrete thresholds take threshold yields x lowest cost pass loop alg compute upperbounds retain optimum solution observed thus far precision maximum fmeasure ucm ucml um recall um ucml ucm time sec figure boundary detection performance closest ultrametric algorithm um baseline ultrametric contour maps algorithm ucm without ucml length weighting [ ] bsds black circles indicate thresholds used closest um optimization b anytime performance fmeasure bsds benchmark function runtime um ucm without length weighting achieve maximum fmeasure respectively experiments applied algorithm segmenting images berkeley segmentation data set bsds [ ] use superpixels generated performing oriented watershed transform output global probability boundary gpb edge detector [ ] construct planar graph whose vertices superpixels edges connecting neighbors image plane whose base distance derived gp b let gp local estimate boundary contrast given averaging gp b classifier output boundary pair neighboring superpixels truncate extreme values enforce gp gp [ ] set e log gp log additive offset assures e experiments use fixed set eleven distance threshold levels l chosen uniformly span useful range threshold values [ ] finally weighted edges proportionally length corresponding boundary image performed dual cutting plane iterations convergence seconds passed lowerbounds bsds segmentations order terminate total residual greater codes written matlab using blossom v implementation minimumweight perfect matching [ ] ibm ilog cplex lp solver default options baseline compare results hierarchical clusterings produced ultrametric contour map ucm [ ] ucm performs agglomerative clustering superpixels assigns lengthweighted averaged gp b value distance pair merged regions ucm explicitly designed find closest ultrametric provides strong baseline hierarchical clustering compute closest llevel ultrametric corresponding ucm clustering result solve minimization eq restricting multicut partition level ucm hierarchy convergence timing figure shows average behavior convergence function runtime found upperbound given cost decoded integer solution lowerbound estimated dual lp close integrality gap typically within lowerbound never convergence dual achieved quite rapidly instances require less iterations converge roughly linear growth size lp iteration cutting planes added fig display histogram computed test image problem instances cost ucm solutions relative produced closest ultrametric um estimated method ratio less indicates approach generated solution lower distortion ultrametric problem instance ucm outperform um algorithm um mc um mc figure proposed closest ultrametric um enforces consistency across levels performing independent multicut clustering mc threshold guarantee hierarchical segmentation c f first image columns second image hierarchical segmentation um better preserves semantic parts two birds correctly merging background regions segmentation quality figure shows segmentation benchmark accuracy closest ultrametric algorithm denoted um along baseline ultrametric contour maps algorithm ucm without length weighting [ ] terms segmentation accuracy um performs nearly identically state art ucm algorithm small gains highprecision regime worth noting bsds benchmark provide strong penalties small leaks two segments total number boundary pixels involved small algorithm may find strong application domains local boundary signal noisier e g biological imaging undersegmentation heavily penalized cuttingplane approach slower agglomerative clustering necessary wait convergence order produce high quality results found upper lower bounds decrease function time clustering performance measured precisionrecall often nearly optimal ten seconds remains stable figure shows plot fmeasure achieved um function time importance enforcing hierarchical constraints although independently finding multicuts different thresholds often produces hierarchical clusterings means guaranteed ran algorithm setting el allowing layer solved independently fig shows examples hierarchical constraints layers improves segmentation quality relative independent clustering threshold conclusion introduced new method approximating closest ultrametric planar graphs applicable hierarchical image segmentation contribution dual cutting plane approach exploits introduction novel slack terms allow representing much larger space solutions relatively cutting planes yields efficient algorithm provides rigorous bounds quality resulting solution empirically observe algorithm rapidly produces compelling image segmentations along lower upperbounds nearly tight benchmark bsds test data set acknowledgements jy acknowledges support experian cf acknowledges support nsf grants iis dbi references [ ] nir ailon moses charikar fitting tree metrics hierarchical clustering phylogeny foundations computer science pages [ ] bjoern andres joerg h kappes thorsten beier ullrich kothe fred hamprecht probabilistic image segmentation closedness constraints proc iccv pages [ ] bjoern andres thorben kroger kevin l briggman winfried denk natalya korogod graham knott ullrich kothe fred hamprecht globally optimal closedsurface segmentation connectomics proc eccv [ ] bjoern andres julian yarkony b manjunath stephen kirchhoff engin turetken charless fowlkes hanspeter pfister segmenting planar superpixel adjacency graphs w r nonplanar superpixel affinity graphs proc emmcvpr [ ] pablo arbelaez michael maire charless fowlkes jitendra malik contour detection hierarchical image segmentation ieee trans pattern anal mach intell may [ ] yoram bachrach pushmeet kohli vladimir kolmogorov morteza zadimoghaddam optimal coalition structure generation cooperative graph games proc aaai [ ] shai bagon meirav galun large scale correlation clustering corr abs [ ] f barahona computational complexity ising spin glass models journal physics mathematical nuclear general april [ ] f barahona cuts matchings planar graphs mathematical programming november [ ] f barahona mahjoub cut polytope mathematical programming september [ ] thorsten beier thorben kroeger jorg h kappes ullrich kothe fred hamprecht cut glue cut fast approximate solver multicut partitioning computer vision pattern recognition cvpr ieee conference pages [ ] michel deza monique laurent geometry cuts metrics volume springer science business media [ ] michael fisher dimer solution planar ising models journal mathematical physics [ ] sungwoong kim sebastian nowozin pushmeet kohli chang dong yoo higherorder correlation clustering image segmentation advances neural information processing systems pages [ ] vladimir kolmogorov blossom v new implementation minimum cost perfect matching algorithm mathematical programming computation [ ] david martin charless fowlkes doron tal jitendra malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics proc iccv pages [ ] david martin charless c fowlkes jitendra malik learning detect natural image boundaries using local brightness color texture cues ieee trans pattern anal mach intell may [ ] julian yarkony analyzing planarcc nips workshop [ ] julian yarkony thorsten beier pierre baldi fred hamprecht parallel multicut segmentation via dual decomposition new frontiers mining complex patterns [ ] julian yarkony alexander ihler charless fowlkes fast planar correlation clustering image segmentation proc eccv [ ] chong zhang julian yarkony fred hamprecht cell detection segmentation using correlation clustering miccai volume pages'),\n",
       " (5776,\n",
       "  'expressing image stream sequence natural sentences cesc chunseong park gunhee kim seoul national university seoul korea park chunseonggunhee snu ac kr httpsgithub comcescparkcrcn abstract propose approach retrieving sequence natural sentences image stream since general users often take series pictures special moments would better take consideration whole image stream produce natural language descriptions almost previous studies dealt relation single image single natural sentence work extends input output dimension sequence images sequence sentences end design multimodal architecture called coherence recurrent convolutional network crcn consists convolutional neural networks bidirectional recurrent neural networks entitybased local coherence model approach directly learns vast usergenerated resource blog posts textimage parallel training data demonstrate approach outperforms stateoftheart candidate methods using quantitative measures e g bleu topk recall user studies via amazon mechanical turk introduction recently hike interest automatically generating natural language descriptions images research computer vision natural language processing machine learning e g [ ] existing work aims discovering relation single image single natural sentence extend input output dimension sequence images sequence sentences may obvious next step toward joint understanding visual content images language descriptions albeit underaddressed current literature problem setup motivated general users often take series pictures memorable moments example many people visit new york city nyc would capture experiences large image streams thus would better take whole photo stream consideration translation natural language description figure intuition problem statement new york city example aim expressing image stream sequence natural sentences leverage natural blog posts learn relation image streams sentence sequences b propose coherence recurrent convolutional networks crcn integrate convolutional networks bidirectional recurrent networks entitybased coherence model fig illustrates intuition problem statement example visiting nyc objective given photo stream automatically produce sequence natural language sentences best describe essence input image set propose novel multimodal architecture named coherence recurrent convolutional networks crcn integrate convolutional neural networks image description [ ] bidirectional recurrent neural networks language model [ ] local coherence model [ ] smooth flow multiple sentences since problem deals learning semantic relations long streams images text challenging obtain appropriate textimage parallel corpus previous research single sentence generation idea issue directly leverage online natural blog posts textimage parallel training data usually blog consists sequence informative text multiple representative images carefully selected authors way storytelling see example fig evaluate approach blog datasets nyc disneyland consisting k blog posts k associated images although focus tourism topics experiments approach completely unsupervised thus applicable domain large set blog posts images demonstrate superior performance approach comparing stateoftheart alternatives including [ ] evaluate quantitative measures e g bleu topk recall user studies via amazon mechanical turk amt related work due recent surge volume literature subject generating natural language descriptions image data discuss representative selection ideas closely related work one popular approaches pose text generation retrieval problem learns ranking embedding caption test image transferred sentences similar training images [ ] approach partly involves text retrieval search candidate sentences image query sequence training database however create final paragraph considering compatibilities individual images text coherence captures text relatedness level sentencetosentence transitions also videosentence works e g [ ] key novelty explicitly include coherence model unlike videos consecutive images streams may show sharp changes visual content cause abrupt discontinuity consecutive sentences thus coherence model demanded make output passages fluent many recent works exploited multimodal networks combine deep convolutional neural networks cnn [ ] recurrent neural network rnn [ ] notable architectures category integrate cnn bidirectional rnns [ ] longterm recurrent convolutional nets [ ] longshort term memory nets [ ] deep boltzmann machines [ ] dependencytree rnn [ ] variants multimodal rnns [ ] although method partly take advantage recent progress multimodal neural networks major novelty integrate coherence model unified endtoend architecture retrieve fluent sequential multiple sentences following compare previous work bears particular resemblance among multimodal neural network models longterm recurrent convolutional net [ ] related objective framework explicitly models relations sequential inputs outputs however model applied video description task creating sentence given short video clip address generation multiple sequential sentences hence unlike mechanism coherence sentences work [ ] addresses retrieval image sequences query paragraph opposite direction problem propose latent structural svm framework learn semantic relevance relations text image sequences however model specialized image sequence retrieval thus applicable natural sentence generation contributions highlight main contributions paper follows best knowledge work first address problem expressing image streams sentence sequences extend input output elaborate forms respect whole body existing methods image streams instead individual images sentence sequences instead individual sentences develop multimodal architecture coherence recurrent convolutional networks crcn integrates convolutional networks image representation recurrent networks sentence modeling local coherence model fluent transitions sentences evaluate method large datasets unstructured blog posts consisting k blog posts k associated images quantitative evaluation user studies show approach successful stateoftheart alternatives verbalizing image stream textimage parallel dataset blog posts discuss transform blog posts training set b imagetext parallel data streams l l sequence imagesentence pairs b l il tl l tn l b training set size denoted l b fig shows summary preprocessing steps blog posts blog preprocessing assume blog authors augment text multiple images semantically meaningful manner order decompose blog sequence images associated text first perform text segmentation text summarization purpose text segmentation divide input blog text set text segments associated single image thus number segments identical number images blog objective text summarization reduce text segment single key sentence result l l two processes transform blog form b l il tl l tn l text segmentation first divide blog passage text blocks according paragraphs apply standard paragraph tokenizer nltk [ ] uses rulebased regular expressions detect paragraph divisions use heuristics based imagetotext block distances proposed [ ] simply assign text block image minimum index distance text block image counted single index distance blog text summarization summarize text segment single key sentence apply latent semantic analysis lsa based summarization method [ ] uses singular value decomposition obtain concept dimension sentences recursively finds representative sentences maximize intersentence similarity topic text segment data augmentation data augmentation wellknown technique convolutional neural networks improve image classification accuracies [ ] basic idea artificially increase number training examples applying transformations horizontal reflection adding noise training images empirically observe idea leads better performance problem l l well imagesentence sequence b l il tl l tn l augment l sentence tn multiple sentences training perform lsabased text summarization select top highest ranked summary sentences among topranked one becomes summary sentence associated image top ones used training model slight abuse notation let tnl denote single summary sentence augmented sentences choose thorough empirical tests text description represent text segment sentences extract paragraph vector [ ] represent content text paragraph vector neuralnetwork based unsupervised algorithm learns fixedlength feature representation variablelength pieces passage learn dimensional dense vector representation separately two classes blog dataset using gensim docvec code use pn denote paragraph vector representation text tn extract parsed tree tn identify coreferent entities grammatical roles words use stanford core nlp library [ ] parse trees used local coherence model discussed section architecture many existing sentence generation models e g [ ] combine words phrases training data generate sentence novel image approach one level higher use sentences training database author sequence sentences novel image stream although model easily extended use words phrases basic building blocks granularity makes sequences long train language model may cause several difficulties learning rnn models example vanishing gradient effect wellknown hardship backpropagate error signal longrange temporal interval therefore design approach retrieves individual candidate sentences query image training database crafts best sentence sequence considering fitness individual imagetosentence pairs coherence consecutive sentences figure illustration preprocessing steps blog posts b proposed crcn architecture fig b illustrates structure crcn consists three main components convolutional neural networks cnn [ ] image representation bidirectional recurrent neural networks brnn [ ] sentence sequence modeling local coherence model [ ] smooth flow multiple sentences data stream variablelength sequence denoted tn use n denote position sentenceimage sequence define cnn brnn model position separately coherence model whole data stream cnn component choice vggnet [ ] represents images dimensional vectors discuss details brnn coherence model section section respectively finally present combine output three components create single compatibility score section brnn model role brnn model represent content flow text sequences problem brnn suitable normal rnn brnn simultaneously model forward backward streams allow us consider previous next sentences sentence make content whole sequence interact one another shown fig b brnn five layers input layer forwardbackward layer output layer relu activation layer finally merged coherence model two fully connected layers note text represented dimensional paragraph vector pt discussed section exact form brnn follows see fig b together better understanding xft f wif pt bfi hft f xft wf hft xbt f wib pt bbi bf hbt f xbt wb hbt bb ot wo hft hbt bo brnn takes sequence text vectors pt input compute xft xbt activations input units forward backward units unlike brnn models separate input activation forward backward ones different sets parameters wif wib empirically leads better performance set activation function f rectified linear unit relu f x max x create two independent forward backward hidden units denoted hft hbt final activation brnn ot regarded description content sentence location also implicitly encodes flow sentence surrounding context sequence parameter sets learn include weights wif wib wf wb wo r biases bfi bbi bf bb bo r local coherence model brnn model capture flow text content lacks learning coherence passage reflects distributional syntactic referential information discourse entities thus explicitly include local coherence model based work [ ] focuses resolving patterns local transitions discourse entities e coreferent noun phrases whole text shown fig b first extract parse trees every summarized text denoted zt concatenate sequenced parse trees one large one make entity grid whole sequence entity grid table row corresponds discourse entity column represents sentence grammatical role expressed three categories one absent e referenced sentence subjects objects x subject object absent making entity grid enumerate transitions grammatical roles entities whole text set history parameter three means obtain transition descriptions e g oox computing ratio occurrence frequency transition finally create dimensional representation captures coherence sequence finally make descriptor dimensional vector zeropadding forward relu layer done brnn output combination cnn rnn coherence model relu activation layers rnn coherence model output e ot n q goes two fully connected fc layers whose role decide proper combination brnn language factors coherence factors drop bias terms fullyconnected layers dimensions variables wf r wf r ot q r st g r rn rn [ ] [ sn ] wf wf [ q ] [ g ] use shared parameters q output mixes well interaction content flows coherency tests joint learning outperforms learning two terms separate parameters note multiplication wf wf last two fc layers reduce single linear mapping thanks dropout assign dropout rates two layers empirically improves generalization performance much single fc layer dropout training crcn train crcn model first define compatibility score image stream paragraph sequence score function inspired karpathy et al [ ] two major differences first score function [ ] deals sentence fragments image fragments thus algorithm considers combinations find best matching hand define score ordered paired compatibility sentence sequence image sequence second also add term measures relevance relation coherency image sequence text sequence finally score skl sentence sequence k image stream l defined skl x skt vtl g k vtl n vtl denotes cnn feature vector tth image stream l define cost function train crcn model follows [ ] c xhx k max skl skk l x max slk skk l skk denotes score training pair corresponding image sentence sequence objective based maxmargin structured loss encourages aligned imagesentence sequence pairs higher score margin misaligned pairs positive training example randomly sample ne examples training set since contrastive example random length sampled dataset wide range content extremely unlikely negative examples length content order sentences positive examples optimization use backpropagation time bptt algorithm [ ] train model apply stochastic gradient descent sgd minibatches data streams among many sgd techniques select rmsprop optimizer [ ] leads best performance experiments initialize weights crcn model using method et al [ ] robust deep rectified models observe better simple gaussian random initialization although model extremely deep use dropout regularization layers except brnn dropout last fc layer remaining layers retrieval sentence sequences test time objective retrieve best sentence sequence given query image stream iq iqn first select knearest images query image training database using ` distance cnn vggnet fc features [ ] experiments k successful generate set sentence sequence candidates c concatenating sentences associated knearest images location finally use learned crcn model compute compatibility score query image stream sequence candidate according rank candidates however one major difficulty scenario exponentially many candidates e c k n resolve issue use approximate divideandconquer strategy recursively halve problem subproblems size subproblem manageable example halve search candidate length q times search space subproblem q becomes k n using beam search idea first find topm best sequence candidates subproblem lowest level recursively increase candidate lengths maximum candidate size limited set though approximate search experiments assure achieves almost optimal solutions plausible combinatorial search mainly local fluency coherence undoubtedly necessary global one order whole sentence sequence fluent coherent subparts must well experiments compare performance approach stateoftheart candidate methods via quantitative measures user studies using amazon mechanical turk amt please refer supplementary material results details implementation experimental setting experimental setting dataset collect blog datasets two topics nyc disneyland reuse blog data disneyland dataset [ ] newly collect data nyc using crawling method [ ] first crawl blog posts associated pictures two popular blog publishing sites blogspot wordpress changing query terms google search manually select travelogue posts describe stories events multiple images finally dataset includes unique blog posts images nyc blog posts images disneyland task quantitative evaluation randomly split dataset training set validation others test set test post use image sequence query iq sequence summarized sentences groundtruth tg algorithm retrieves best sequences training database query image sequence ideally retrieved sequences match well tg since training test data disjoint algorithm retrieve similar identical sentences best quantitative measures exploit two types metrics language similarity e bleu [ ] cider [ ] meteor [ ] scores retrieval accuracies e topk recall median rank popularly used text generation literature [ ] topk recall rk recall rate groundtruth retrieval given top k candidates median rank indicates median ranking value first retrieved groundtruth better performance indicated higher bleu cider meteor rk scores lower median rank values baselines since sentence sequence generation image streams addressed yet previous research instead extend several stateoftheart singlesentence models publicly available codes baselines including logbilinear multimodal models kiros et al [ ] recurrent convolutional models karpathy et al [ ] vinyals et al [ ] [ ] use three variants introduced paper standard logbilinear model lbl two multimodal extensions modalitybased lbl mlblb factored threeway lbl mlblf use neuraltalk package authored karpathy et al baseline [ ] denoted cnnrnn [ ] denoted cnnlstm simplest baseline also compare global matching glomatch [ ] baselines create final sentence sequences concatenating sentences generated image query stream b b cnnlstm [ ] cnnrnn [ ] mlblf [ ] mlblb [ ] lbl [ ] glomatch [ ] nn rcn crcn cnnlstm [ ] cnnrnn [ ] mlblf [ ] mlblb [ ] lbl [ ] glomatch [ ] nn rcn crcn language metrics retrieval metrics b b cider meteor r r r medrank new york city disneyland table evaluation sentence generation two datasets new york city disneyland language similarity metrics bleu retrieval metrics rk median rank better performance indicated higher bleu cider meteor rk scores lower median rank values also compare different variants method validate contributions key components method test knearest search nn without rnn part simplest variant image test query find k similar training images simply concatenate associated sentences second variant brnnonly method denoted rcn excludes entitybased coherence model approach complete method denoted crcn comparison quantifies improvement coherence model fair use vggnet fc feature [ ] algorithms quantitative results table shows quantitative results experiments using language retrieval metrics approach crcn rcn outperform large margins stateoftheart baselines generate passages without consideration sentencetosentence transitions unlike mlblf shows best performance among three models [ ] albeit small margin partly share word dictionary training among mrnnbased models cnnlstm significantly outperforms cnnrnn lstm units help learn models irregular lengthy data natural blogs robustly also observe crcn outperforms nn rcn especially retrieval metrics shows integration two key components brnn coherence model indeed contributes performance improvement crcn slightly better rcn language metrics significantly better retrieval metrics means rcn fine retrieving fairly good solutions good ranking correct solution high compared crcn small margins language metrics also attributed inherent limitation example bleu focuses counting matches ngram words thus good comparing sentences even worse paragraphs fully evaluating fluency coherency fig illustrates several examples sentence sequence retrieval set show query image stream text results created method baselines except fig show parts sequences rather long illustration qualitative examples demonstrate approach successful verbalize image sequences include variety content user studies via amazon mechanical turk perform user studies using amt observe general users preferences text sequences different algorithms since evaluation involves multiple images long passages text design amt task sufficiently simple general turkers background knowledge figure examples sentence sequence retrieval nyc top disneyland bottom set present part query image stream corresponding text output method baseline baselines glomatch cnnlstm mlblb rcn rcn n nyc disneyland table results amt pairwise preference tests present percentages responses turkers vote crcn baselines length query streams except last column first randomly sample test streams two datasets first set maximum number images per query query longer uniformly sample amt test show query image stream iq pair passages generated method crcn one baseline random order ask turkers choose agreed text sequence iq design test pairwise comparison instead multiplechoice question make answering analysis easier questions look similar examples fig obtain answers three different turkers query compare four baselines choose mlblb among three variants [ ] cnnlstm among mrnnbased methods also select glomatch rcn variants method table shows results amt tests validate amt annotators prefer results baselines glomatch worst uses weak image representation e gist tiny images differences crcn rcn e th column table significant previous quantitative measures mainly query image stream sampled relatively short coherence becomes critical passage longer justify argument run another set amt tests use images per query shown last column table performance margins crcn rcn become larger lengths query image streams increase result assures passages longer coherence becomes important thus crcn output preferred turkers conclusion proposed approach retrieving sentence sequences image stream developed coherence recurrent convolutional network crcn consists convolutional networks bidirectional recurrent networks entitybased local coherence model quantitative evaluation users studies using amt large collections blog posts demonstrated crcn approach outperformed stateoftheart candidate methods acknowledgements research partially supported hancom basic science research program national research foundation korea rcaa references [ ] r barzilay lapata modeling local coherence entitybased approach acl [ ] bird e loper e klein natural language processing python oreilly media inc [ ] x chen c l zitnick minds eye recurrent visual representation image caption generation cvpr [ ] f choi p wiemerhastings j moore latent semantic analysis text segmentation emnlp [ ] j donahue l hendricks guadarrama rohrbach venugopalan k saenko darrell longterm recurrent convolutional networks visual recognition description cvpr [ ] gong l wang hodosh j hockenmaier lazebnik improving imagesentence embeddings using large weakly annotated photo collections eccv [ ] k x zhang ren j sun delving deep rectifiers surpassing humanlevel performance imagenet classification arxiv [ ] hodosh p young j hockenmaier framing image description ranking task data models evaluation metrics jair [ ] karpathy l feifei deep visualsemantic alignments generating image descriptions cvpr [ ] g kim moon l sigal joint photo stream blog post summarization exploration cvpr [ ] g kim moon l sigal ranking retrieval image sequences multiple paragraph queries cvpr [ ] r kiros r salakhutdinov r zemel multimodal neural language models icml [ ] krizhevsky sutskever g e hinton imagenet classification deep convolutional neural networks nips [ ] g kulkarni v premraj dhar li choi c berg l berg baby talk understanding generating image descriptions cvpr [ ] p kuznetsova v ordonez l berg choi treetalk composition compression trees image descriptions tacl [ ] b lavie meteor automatic metric mt evaluation improved correlation human judgments acl [ ] q le mikolov distributed representations sentences documents icml [ ] c manning surdeanu j bauer j finkel j bethard mcclosky stanford corenlp natural language processing toolkit acl [ ] j mao w xu yang j wang z huang l yuille deep captioning multimodal recurrent neural networks mrnn iclr [ ] mikolov statistical language models based neural networks ph thesis brno university technology [ ] v ordonez g kulkarni l berg imtext describing images using million captioned photographs nips [ ] k papineni roukos ward w j zhu bleu method automatic evaluation machine translation acl [ ] rohrbach w qiu titov thater pinkal b schiele translating video content natural language descriptions iccv [ ] schuster k k paliwal bidirectional recurrent neural networks ieee tsp [ ] k simonyan zisserman deep convolutional networks largescale image recognition iclr [ ] r socher karpathy q v le c manning ng grounded compositional semantics finding describing images sentences tacl [ ] n srivastava r salakhutdinov multimodal learning deep boltzmann machines nips [ ] tieleman g e hinton lecture rmsprop coursera [ ] r vedantam c l zitnick parikh cider consensusbased image description evaluation arxiv [ ] vinyals toshev bengio erhan show tell neural image caption generator cvpr [ ] p j werbos generalization backpropagation application recurrent gas market model neural networks [ ] r xu c xiong w chen j j corso jointly modeling deep video compositional text bridge vision language unified framework aaai'),\n",
       " (5814,\n",
       "  'parallel correlation clustering big graphs xinghao pan dimitris papailiopoulos samet oymak benjamin recht kannan ramchandran michael jordan amplab eecs uc berkeley statistics uc berkeley abstract given similarity graph items correlation clustering cc groups similar items together dissimilar ones apart one popular cc algorithms kwikcluster algorithm serially clusters neighborhoods vertices obtains approximation ratio unfortunately practice kwikcluster requires large number clustering rounds potential bottleneck large graphs present c clusterwild two algorithms parallel correlation clustering run polylogarithmic number rounds provably achieve nearly linear speedups c uses concurrency control enforce serializability parallel clustering process guarantees approximation ratio clusterwild coordination free algorithm abandons consistency benefit better scaling leads provably small loss approximation ratio demonstrate experimentally algorithms outperform state art terms clustering accuracy running time show algorithms cluster billionedge graphs seconds cores achieving speedup introduction clustering items according notion similarity major primitive machine learning correlation clustering serves basic means achieve goal given similarity measure items goal group similar items together dissimilar items apart contrast clustering approaches number clusters determined priori good solutions aim balance tension grouping items together versus isolating simplest cc variant described complete signed graph input graph g n vertices weights edges similar items edges dissimilar ones goal generate partition vertices disjoint sets minimizes number disagreeing edges equals number edges cut clusters plus number edges inside clusters metric commonly called number disagreements figure give toy example cc instance cluster cluster cost edges inside clusters edges across clusters figure graph solid edges denote similarity dashed dissimilarity number disagreeing edges clustering clustering color bad edges red entity deduplication archetypal motivating example correlation clustering applications chat disentanglement coreference resolution spam detection [ ] input set entities say results keyword search pairwise classifier indicates errorsimilarities entities two results keyword search might refer item might look different come different sources building similarity graph entities applying cc hope cluster duplicate entities group context keyword search implies meaningful compact list results cc applied finding communities signed networks classifying missing edges opinion trust networks [ ] gene clustering [ ] consensus clustering [ ] kwikcluster simplest cc algorithm achieves provable approximation ratio [ ] works following way pick vertex v random cluster center create cluster v positive neighborhood n v e vertices connected v positive edges peel vertices associated edges graph repeat vertices clustered beyond theoretical guarantees experimentally kwikcluster performs well combined local heuristics [ ] kwikcluster seems like inherently sequential algorithm cases interest requires many peeling rounds happens small number vertices clustered per round bottleneck large graphs recently efforts develop scalable variants kwikcluster [ ] [ ] distributed peeling algorithm presented context mapreduce using elegant analysis authors establish approximation polylogarithmic number rounds algorithm employs simple step rejects vertices executed parallel conflicting however see experiments seemingly minor coordination step hinders scaleups parallel core setting [ ] sketch distributed algorithm presented algorithm achieves approximation kwikcluster logarithmic number rounds expectation however performs significant redundant work per iteration effort detect parallel vertices become cluster centers contributions present c clusterwild two parallel cc algorithms provable performance guarantees practice outperform state art terms running time clustering accuracy c parallel version kwikcluster uses concurrency control establish approximation ratio clusterwild simple implement coordinationfree algorithm abandons consistency benefit better scaling provably small loss approximation ratio c achieves approximation ratio polylogarithmic number rounds enforcing consistency concurrently running peeling threads consistency enforced using concurrency control notion extensively studied databases transactions recently used parallelize inherently sequential machine learning algorithms [ ] clusterwild coordinationfree parallel cc algorithm waives consistency favor speed cost pay arbitrarily small loss clusterwild accuracy show clusterwild achieves opt n log n approximation polylogarithmic number rounds provable nearly linear speedups main theoretical innovation clusterwild analyzing coordinationfree algorithm serial variant kwikcluster runs noisy graph experimental evaluation demonstrate algorithms gracefully scale graphs billions edges large graphs algorithms output valid clustering less seconds threads order magnitude faster kwikcluster observe unexpectedly clusterwild faster c quite surprisingly abandoning coordination parallel setting amounts relative loss clustering accuracy furthermore compare state art parallel cc algorithms showing consistently outperform algorithms terms running time clustering accuracy notation g denotes graph n vertices edges g complete edges denote dv positive degree vertex e number vertices connected v positive edges denotes positive maximum degree g n v denotes positive neighborhood v moreover let cv v n v two vertices u v termed friends u n v vice versa denote permutation n two parallel algorithms correlation clustering formal definition correlation clustering given correlation clustering given graph g n vertices partition vertices arbitrary number k disjoint subsets c ck sum negative edges within subsets plus sum positive edges across subsets minimized opt min kn min ci \\\\cj ij [ k ci n k x e ci ci k k x x ji e ci cj e e sets positive negative edges g kwikcluster remarkably simple algorithm approximately solves combinatorial problem operates follows random vertex v picked cluster cv created v positive neighborhood vertices cv peeled graph process repeated vertices clustered kwikcluster equivalently executed noted [ ] substitute random choice vertex per peeling round random order preassigned vertices see alg select random permutation vertices peel vertex indexed friends remove vertices cv repeat process order among vertices makes discussion parallel algorithms convenient c parallel cc using concurency control algorithm kwikcluster suppose wish run parallel version kwikcluster say two threads one thread random permutation n picks vertex v indexed v thread picks u indexed concurrently select vertex v indexed cv v n v vertices cluster centers remove clustered vertices g iff friends g v u con end nected positive edge vertex smallest order wins concurency rule assume v u friends g v u become cluster centers moreover assume v u common unclustered friend say w w clustered v u need follow would happen kwikcluster alg w go vertex smallest permutation number case v concurency rule following simple rules develop c serializable parallel cc algorithm since c constructs clusters kwikcluster given ordering inherits approximation idea identifying cluster centers rounds first used [ ] obtain parallel algorithm maximal independent set mis c shown alg starts assigning random permutation vertices samples active set n unclustered vertices sample taken prefix sampling p threads picks vertex smallest order checks vertex become cluster center first enforce concurrency rule adjacent vertices cannot cluster centers time c enforces making thread check friends vertex say v picked thread check attemptcluster whether vertex v preceding friends cluster centers none go ahead label v cluster center proceed creating cluster preceding friend v cluster center v labeled cluster center preceding friend v call u yet received label e u currently processed yet labeled cluster center thread processing v wait u receive label major technical detail showing wait time bounded show log n threads conflict time using new subgraph sampling lemma [ ] since c serializable respect concurrency rule vertex u adjacency two cluster centers gets assigned one smaller permutation order accomplished createcluster processing vertices threads synchronized bulk clustered vertices removed new active set sampled process repeated everything clustered following section present theoretical guarantees c algorithm c clusterwild createcluster v clusterid v v u v \\\\ clusterid u min clusterid u v end input g clusterid clusterid n random permutation n v attemptcluster v maximum vertex degree g v clusterid u iscenter v first n vertices v [ ] createcluster v parallel end v first element v iscenter v c concurrency control u v check friends order attemptcluster v u v precede wait else clusterwild coordination free wait clusterid u till clustered createcluster v iscenter u end return friend center cant end end remove clustered vertices v end end end output clusterid clusterid n return earlier friends centers clusterwild coordinationfree correlation clustering clusterwild speeds computation ignoring first concurrency rule uniformly samples unclustered vertices builds clusters around without respecting rule cluster centers cannot friends g clusterwild threads bypass attemptcluster routine eliminates waiting part c clusterwild samples set vertices prefix thread picks first ordered vertex remaining using vertex cluster center creates cluster around peels away clustered vertices repeats process next remaining vertex end processing vertices threads synchronized bulk clustered vertices removed new active set sampled parallel clustering repeated careful analysis along lines [ ] shows number rounds e bulk synchronization steps polylogarithmic quite unsurprisingly clusterwild faster c interestingly abandoning consistency incur much loss approximation ratio show error introduced accuracy solution bounded characterize error theoretically show practice translates relative loss objective main intuition clusterwild introduce much error chance two randomly selected vertices friends small hence concurrency rules infrequently broken theoretical guarantees section bound number rounds required algorithms establish theoretical speedup one obtain p parallel threads proceed present approximation guarantees would like remind reader thatas relevant literaturewe consider graphs complete signed unweighted omitted proofs found appendix number rounds running time analysis follows [ ] [ ] main idea track fast maximum degree decreases remaining graph end round lemma c clusterwild terminate log n log rounds w h p analyze running time algorithms simplified bsp model main idea running time super step e round determined straggling thread e one gets assigned amount work plus time needed synchronization end round assumption assume threads operate asynchronously within round synchronize end round memory cell writtenread concurrently multiple threads time spent per round algorithm proportional time slowest thread cost thread synchronization end batch takes time p p number threads total computation cost proportional sum time spent rounds plus time spent bulk synchronization step simplified model show algorithms obtain nearly linear speedup clusterwild faster c precisely due lack coordination main tool analyzing c recent graphtheoretic result [ ] theorem guarantees one samples n subset vertices graph sampled subgraph connected component size log n combining appendix show following result theorem theoretical running time c p cores upper bounded mn log n p log n log long number cores p smaller mini nii p nii size batch ith round algorithm running time clusterwild p cores upper bounded mn p log n log p approximation ratio proceed establishing approximation ratios c clusterwild c serializable straightforward c obtains precisely approximation ratio kwikcluster one simply show permutation kwikcluster c output clustering indeed true two simple concurrency rules mentioned previous section sufficient c equivalent kwikcluster theorem c achieves approximation ratio expectation clusterwild serial procedure noisy graph analyzing clusterwild bit involved guarantees based fact clusterwild treated one running peeling algorithm noisy graph since adjacent active vertices still become cluster centers clusterwild one view edges deleted somewhat unconventional adversary analyze new noisy graph establish theoretical result theorem clusterwild achieves opto nlog n approximation expectation provide sketch proof delegate details appendix since clusterwild ignores edges among active vertices treat edges deleted main result quantify loss clustering accuracy caused ignoring edges proceed define bad triangles combinatorial structure used measure clustering quality peeling algorithm definition bad triangle g set three vertices two pairs joined positive edge one pair joined negative edge let tb denote set bad triangles g quantify cost clusterwild make observation lemma cost greedy algorithm picks vertex v irrespective sampling order creates cv peels away repeats equal number bad triangles adjacent cluster center v lemma let g denote random graph induced deleting edges active vertices per round given run clusterwild let new denote number additional bad triangles thatp g compared g expected cost clusterwild upper bounded e ttb pt new pt event triangle end points j k bad least one end points becomes active still part original unclustered graph proof begin bounding second term e new considering number new bad triangles newi created round e newi x uv e p u v ai n u [ n v x uv e e n using result clusterwild terminates log n log rounds get e new n log n p p left bound e ttb pt ttb pt use following lemma p p pt lemma pt satisfies e tettb ttb pt op proof let b one many sets thatpattribute thep cost optimal p possibly p p edges pt pt pt algorithm opt eb eb tettb ttb b \\\\ ttb z [ ] p simply boundsthe expectation bad triangles adjacent edge u v uv ttb pt let suv uv ttb union sets nodes bad triangles contain vertices u v observe w s\\\\ u v becomes active u v cost e cost bad triangle u v w incurred hand either u v selected pivots round cuv high e equal bad triangles containing edge u v let auv u v activated vertices suv c e [ cuv ] e [ cuv auv ] p auv e cuv ac uv p auv p u v \\\\ \\\\ p v \\\\ \\\\ last inequality obtained union bound u v bound following probability p v \\\\ p v p \\\\ v p v p \\\\ p \\\\ p v p \\\\ observe p v hence need upper bound p \\\\ probability per round positive neighbors become activated upper bounded np np np n p p p p n n n n e p hence probability upper bounded p v \\\\ \\\\ e know also n hence p e cuv exp ttb pt new overall expectation bounded e e opt n ln n log approximation ratio clusterwild opt n log n establishes bsp algorithms proxy asynchronous algorithms would like note analysis bsp model useful proxy performance completely asynchronous variants algorithms specifically see alg remove synchronization barriers difference asynchronous execution alg compared alg complete lack bulk synchronization end processing active set although analysis bsp variants algorithms tractable unfortunately analyzing precisely speedup asynchronous c approximation guarantees asynchronous clusterwild challenging however experimental section test completely asynchronous algorithms bsp algorithms previous section observe perform quite similarly terms accuracy clustering running times skip constants simplify presentation however smaller related work correlation clustering formally introduced bansal et al [ ] general case minimizing disagreements nphard hard approximate within arbitrarily small constant apxhard [ ] two variations problem cc complete graphs edges present weights ii cc general graphs arbitrary edge weights problems hard however general graph setup seems fundamentally harder best known approximation ratio latter log n reduction minimum multicut problem indicates improvement requires fundamental breakthroughs theoretical algorithms [ ] algorithm c clusterwild asynchronous execution input g clusterid clusterid n random permutation n v v first element v v v v c concurrency control attemptcluster v else clusterwild coordination free createcluster v end remove clustered vertices v end output clusterid clusterid n case complete unweighted graphs long series results establishes approximation via rounded linear program lp [ ] recent result establishes approximation using elegant rounding lp relaxation [ ] avoiding expensive lp using rounding procedure [ ] basis greedy algorithm yields kwikcluster approximation cc complete unweighted graphs variations cost metric cc change algorithmic landscape maximizing agreements dual measure disagreements [ ] maximizing difference number agreements disagreements [ ] come different hardness approximation results also several variants chromatic cc [ ] overlapping cc [ ] small number clusters cc added constraints suitable biology applications [ ] way c finds cluster centers seen variation mis algorithm [ ] main difference case passively detect mis locking memory variables waiting preceding ordered threads means vertex pushes cluster id status cluster centerclusteredunclustered friends versus pulling asking friends cluster status saves substantial amount computational effort experiments parallel algorithms implemented scalawe defer full discussion implementation details appendix c ran experiments amazon ecs r xlarge vcpus gb memory instances using threads real graphs listed table graph dblp enwiki uk webbase vertices edges description dblp coauthorship network [ ] link graph english part wikipedia [ ] crawl uk domain [ ] crawl domain [ ] crawl webbase crawler [ ] table graphs used evaluation parallel algorithms tested different random orderings measured runtimes speedups ratio runtime thread runtime p threads objective values obtained parallel algorithms comparison also implemented algorithm presented [ ] denote cdk short values used c bsp clusterwild bsp cdk interest space present representative plots results full results given appendix code available httpsgithub compxinghaoparallelcorrelationclustering cdk tested smaller graphs dblp enwiki cdk prohibitively slow often orders magnitude slower c clusterwild even kwikcluster mean runtime uk mean runtime ms mean runtime ms mean runtime serial c c bsp cw cw bsp mean speedup webbase serial c c bsp cw cw bsp ideal c c bsp cw cw bsp speedup number threads mean runtimes uk blocked vertices number rounds mean number synchronization rounds bsp algorithms number threads number threads c mean speedup webbase objective value relative serial dblp c bsp min c bsp mean c bsp max c bsp min c bsp mean c bsp max blocked vertices enwiki ccw bsp uk ccw bsp ccw bsp webbase ccw bsp dblp cdk dblp ccw bsp enwiki cdk enwiki number threads b mean runtimes mean number rounds algo obj value serial obj value e percent blocked vertices c enwiki bsp run cw bsp mean cw bsp median cw mean cw median cdk mean cdk median number threads f median objective values dblp cw bsp cdk run figure figures cw short clusterwild bsp short bulksynchronous variants parallel algorithms short asynchronous variants runtimes speedups c clusterwild initially slower serial due overheads required atomic operations parallel setting however parallel algorithms outperform kwikcluster threads threads added asychronous variants become faster bsp counterparts synchronization barrriers difference bsp asychronous variants greater smaller clusterwild also always faster c since coordination overheads asynchronous algorithms able achieve speedup x threads bsp algorithms poorer speedup ratio nevertheless achieve x speedup synchronization rounds main overhead bsp algorithms lies need synchronization rounds increases amount synchronization decreases algorithms less synchronization rounds small considering size graphs multicore setting blocked vertices additionally c incurs overhead number vertices blocked waiting earlier vertices complete note overhead extremely small practice graphs less vertices blocked larger sparser graphs drops less e vertices objective value design c algorithms also return output thus objective value kwikcluster find clusterwild bsp worse serial across graphs values behavior asynchronous clusterwild worsens threads added reaching worse serial one graphs finally smaller graphs able test cdk cdk returns worse median objective value clusterwild variants conclusions future directions paper presented two parallel algorithms correlation clustering nearly linear speedups provable approximation ratios overall two approaches support otherwhen c relatively fast relative clusterwild may prefer c guarantees accuracy clusterwild accurate relative c may prefer clusterwild speed future intend implement algorithms distributed environment synchronization communication often account highest cost c clusterwild wellsuited distributed environment since polylogarithmic number rounds references [ ] ahmed k elmagarmid panagiotis g ipeirotis vassilios verykios duplicate record detection survey knowledge data engineering ieee transactions [ ] arvind arasu christopher dan suciu largescale deduplication constraints using dedupalog data engineering icde ieee th international conference pages ieee [ ] micha elsner warren schudy bounding comparing methods correlation clustering beyond ilp proceedings workshop integer linear programming natural langauge processing pages association computational linguistics [ ] bilal hussain oktie hassanzadeh fei chiang hyun chul lee renee j miller evaluation clustering algorithms duplicate detection technical report [ ] francesco bonchi david garciasoriano edo liberty correlation clustering theory practice proceedings th acm sigkdd international conference knowledge discovery data mining pages acm [ ] flavio chierichetti nilesh dalvi ravi kumar correlation clustering mapreduce proceedings th acm sigkdd international conference knowledge discovery data mining pages acm [ ] bo yang william k cheung jiming liu community mining signed social networks knowledge data engineering ieee transactions [ ] n cesabianchi c gentile f vitale g zappella et al correlation clustering approach link classification signed networks annual conference learning theory pages microtome [ ] amir bendor ron shamir zohar yakhini clustering gene expression patterns journal computational biology [ ] nir ailon moses charikar alantha newman aggregating inconsistent information ranking clustering journal acm jacm [ ] xinghao pan joseph e gonzalez stefanie jegelka tamara broderick michael jordan optimistic concurrency control distributed unsupervised learning advances neural information processing systems pages [ ] guy e blelloch jeremy fineman julian shun greedy sequential maximal independent set matching parallel average proceedings twentyfourth annual acm symposium parallelism algorithms architectures pages acm [ ] michael krivelevich phase transition site percolation pseudorandom graphs arxiv preprint arxiv [ ] nikhil bansal avrim blum shuchi chawla correlation clustering ieee th annual symposium foundations computer science pages ieee computer society [ ] moses charikar venkatesan guruswami anthony wirth clustering qualitative information foundations computer science proceedings th annual ieee symposium pages ieee [ ] erik demaine dotan emanuel amos fiat nicole immorlica correlation clustering general weighted graphs theoretical computer science [ ] shuchi chawla konstantin makarychev tselil schramm grigory yaroslavtsev near optimal lp rounding algorithm correlation clustering complete complete kpartite graphs proceedings fortyseventh annual acm symposium theory computing stoc pages [ ] chaitanya swamy correlation clustering maximizing agreements via semidefinite programming proceedings fifteenth annual acmsiam symposium discrete algorithms pages society industrial applied mathematics [ ] ioannis giotis venkatesan guruswami correlation clustering fixed number clusters proceedings seventeenth annual acmsiam symposium discrete algorithm pages acm [ ] moses charikar anthony wirth maximizing quadratic programs extending grothendiecks inequality foundations computer science proceedings th annual ieee symposium pages ieee [ ] noga alon konstantin makarychev yury makarychev assaf naor quadratic forms graphs inventiones mathematicae [ ] francesco bonchi aristides gionis francesco gullo antti ukkonen chromatic correlation clustering proceedings th acm sigkdd international conference knowledge discovery data mining pages acm [ ] francesco bonchi aristides gionis antti ukkonen overlapping correlation clustering data mining icdm ieee th international conference pages ieee [ ] gregory j puleo olgica milenkovic correlation clustering constrained cluster sizes extended weights bounds arxiv preprint arxiv [ ] p boldi vigna webgraph framework compression techniques www [ ] p boldi rosa santini vigna layered label propagation multiresolution coordinatefree ordering compressing social networks www acm press [ ] p boldi b codenotti santini vigna ubicrawler scalable fully distributed web crawler software practice experience')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns the topics to the documents in corpus\n",
    "lda_corpus = lda[mm]\n",
    "\n",
    "# Find the threshold, let's set the threshold to be 1/#clusters,\n",
    "# To prove that the threshold is sane, we average the sum of all probabilities:\n",
    "scores = list(chain(*[[score for topic_id,score in topic] for topic in [doc for doc in lda_corpus]]))\n",
    "\n",
    "[lda_corpus[i] for i in range(3)]\n",
    "threshold = sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4986839433200657\n",
      "[[(5, 0.9996052)], [(4, 0.9995748)], [(5, 0.014555332), (6, 0.98511606)], [(1, 0.07769058), (7, 0.92066765)], [(7, 0.99730295)], [(1, 0.3020056), (4, 0.016355509), (6, 0.2006227), (7, 0.47860438)], [(4, 0.851073), (5, 0.14112858)], [(2, 0.65653944), (5, 0.030730132), (6, 0.16753528), (9, 0.14220329)], [(2, 0.96354526), (8, 0.03614871)], [(2, 0.9924008)]]\n"
     ]
    }
   ],
   "source": [
    "print (threshold)\n",
    "print([i for i in lda_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996052"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#documents[0]\n",
    "#i[0]\n",
    "lda_corpus[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 0.9996052)],\n",
       " [(4, 0.9995748)],\n",
       " [(5, 0.018832704), (6, 0.9808387)],\n",
       " [(1, 0.079630256), (7, 0.91801137)],\n",
       " [(7, 0.997264)],\n",
       " [(1, 0.303963), (4, 0.0126937395), (6, 0.2150712), (7, 0.46588954)],\n",
       " [(4, 0.85632217), (5, 0.13532759)],\n",
       " [(2, 0.6546755), (5, 0.02993259), (6, 0.15622109), (9, 0.1559884)],\n",
       " [(2, 0.9627798), (8, 0.03691426)],\n",
       " [(2, 0.99109924)]]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(5, 0.9995992)],\n",
       "  (5677,\n",
       "   'double nothing multiplicative incentive mechanisms crowdsourcing nihar b shah university california berkeley nihareecs berkeley edu dengyong zhou microsoft research dengyong zhoumicrosoft com abstract crowdsourcing gained immense popularity machine learning applications obtaining large amounts labeled data crowdsourcing cheap fast suffers problem lowquality data address fundamental challenge crowdsourcing propose simple payment mechanism incentivize workers answer questions sure skip rest show surprisingly mild natural nofreelunch requirement mechanism one incentivecompatible payment mechanism possible also show among possible incentivecompatible mechanisms may may satisfy nofreelunch mechanism makes smallest possible payment spammers interestingly unique mechanism takes multiplicative form simplicity mechanism added benefit preliminary experiments involving several hundred workers observe significant reduction error rates unique mechanism lower monetary expenditure introduction complex machine learning tools deep learning gaining increasing popularity applied wide variety problems tools however require large amounts labeled data [ hdy ryz dds cbw ] large labeling tasks performed coordinating crowds semiskilled workers internet known crowdsourcing crowdsourcing means collecting labeled training data become indispensable engineering intelligent systems workers crowdsourcing experts consequence labels obtained crowdsourcing typically significant amount error [ kkkmf vdve wlc ] recent efforts focused developing statistical techniques postprocess noisy labels order improve quality e g [ ryz zlp kos ipsw ] however inputs algorithms erroneous difficult guarantee processed labels reliable enough subsequent use machine learning applications order avoid garbage garbage take complementary approach problem cleaning data time collection consider crowdsourcing settings workers paid services popular crowdsourcing platforms amazon mechanical turk others commercial platforms gained substantial popularity due support diverse range tasks machine learning labeling varying image annotation text recognition speech captioning machine translation consider problems objective nature definite answer figure depicts example question worker shown set images image worker required identify image depicts golden gate bridge golden gate bridge golden gate bridge yes yes im sure b figure different interfaces crowdsourcing setup conventional interface b option skip approach builds simple insight typical crowdsourcing setups workers simply paid proportion amount tasks complete result workers attempt answer questions sure thereby increasing error rate labels questions worker sure answers could unreliable [ wlc kkkmf vdve jsv ] ensure acquisition highquality labels wish encourage worker skip questions unsure instance providing explicit im sure option every question see figure b goal develop payment mechanisms encourage worker select option unsure term payment mechanism incentivizes worker incentive compatible addition incentive compatibility preventing spammers another desirable requirement incentive mechanisms crowdsourcing spammers workers answer randomly without regard question asked hope earning free money known exist large numbers crowdsourcing platforms [ wlc boh kkkmf vdve ] thus interest deter spammers paying low possible intuitive objective end ensure zero expenditure spammers answer randomly paper however impose strictly significantly weaker condition show one one incentivecompatible mechanism satisfy weak condition requirement referred nofreelunch axiom says questions attempted worker answered incorrectly payment must zero propose payment mechanism aforementioned setting incentive compatibility plus nofreelunch show surprisingly possible mechanism also show additionally mechanism makes smallest possible payment spammers among possible incentive compatible mechanisms may may satisfy nofreelunch axiom payment mechanism takes multiplicative form evaluation workers response question certain score final payment product scores mechanism additional appealing features simple compute also simple explain workers mechanism applicable type objective questions including multiple choice annotation questions transcription tasks etc order test whether mechanism practical assess quality final labels obtained conducted experiments amazon mechanical turk crowdsourcing platform preliminary experiments involved several hundred workers found quality data improved twofold unique mechanism total monetary expenditure lower compared conventional baseline problem setting crowdsourcing setting consider one workers perform task task consists multiple questions questions objective mean question precisely one correct answer examples objective questions include multiplechoice classification questions figure questions transcribing text audio images etc possible answer question define workers confidence answer probability according belief answer correct words one assume worker mind probability distribution possible answers question confidence answer probability answer correct shorthand also define confidence question confidence answer worker confident question assume workers confidences different questions independent goal every question worker incentivized skip confidence certain predefined threshold otherwise select answer thinks confident formally let predefined value goal design payment mechanisms incentivize worker skip questions confidence lower attempt confidence higher moreover questions attempts answer must incentivized select answer believes likely correct threshold may chosen based various factors problem hand example downstream machine learning algorithms using crowdsourced data knowledge statistics worker abilities etc paper assume threshold given us let n denote total number questions task among assume existence gold standard questions set questions whose answers known requester let g g n denote number gold standard questions g gold standard questions assumed distributed uniformly random pool n questions course worker know g n questions form gold standard payment worker task computed receiving responses questions task payment based workers performance gold standard questions since payment based known answers payments different workers depend thereby allowing us consider presence one worker without loss generality employ following standard notation positive integer k set k denoted [ k ] indicator function denoted e z z true otherwise notation r denotes set nonnegative real numbers let x xg denote evaluations answers worker gives g gold standard questions denotes worker skipped question denotes worker attempted answer question answer incorrect denotes worker attempted answer question answer correct let f g r denote payment function namely function determines payment worker based evaluations x xg note crowdsourcing platforms today mandate payments nonnegative let denote budget e maximum amount paid individual worker task max f x xg x xg amount thus amount compensation paid perfect agent work assume budget condition throughout rest paper assume worker attempts maximize overall expected payment follows expression workers expected payment refer expected payment workers point view expectation taken respect workers confidences answers uniformly random choice g gold standard questions among n questions task question [ n ] let yi worker attempts question set yi otherwise every question [ n ] yi let pi confidence worker answer selected question every question [ n ] yi let pi arbitrary value let e g g workers perspective expected payment selected answers confidencelevels g x x f yj g yjg pji pji n g j jg e g n expression outermost summation corresponds expectation respect randomness arising unknown choice gold standard questions inner summation corresponds expectation respect workers beliefs correctness responses event confidence question exactly equal worker may equally incentivized answer skip call payment function f incentivecompatible mechanism expected payment worker payment function strictly maximized worker responds manner desired main results incentivecompatible mechanism guarantees section present main results paper namely design incentivecompatible mechanisms practically useful properties end impose following natural requirement payment function f motivated practical considerations budget constraints discouraging spammers miscreants [ boh kkkmf vdve wlc ] term requirement nofreelunch axiom axiom nofreelunch axiom answers attempted worker gold standard wrong payment zero formally every set evaluations x xg satisfy pg pg xi xi require payment satisfy f x xg observe nofreelunch extremely mild requirement fact significantly weaker imposing zero payment workers answer randomly instance questions binarychoice format randomly choosing among two options question would result answers correct expectation nofreelunch axiom applicable none turns correct proposed multiplicative mechanism present proposed payment mechanism algorithm algorithm multiplicative incentivecompatible mechanism inputs threshold budget evaluations x xg g workers answers g gold standard questions pg pg let c xi w xi payment f x xg g c w proposed mechanism multiplicative form answer gold standard given score based whether correct score incorrect score skipped score final payment simply product scores scaled mechanism easy describe workers instance g cents description reads reward starts cents every correct answer gold standard questions reward double however questions answered incorrectly reward become zero please use im sure option wisely observe payment rule similar popular double nothing paradigm [ dou ] algorithm makes zero payment one attempted answers gold standard wrong note property significantly stronger property nofreelunch originally required wanted zero payment attempted answers wrong surprisingly prove shortly algorithm incentivecompatible mechanism satisfies nofreelunch following theorem shows proposed payment mechanism indeed incentivizes worker skip questions confidence answering confidence greater latter case worker incentivized select answer thinks likely correct theorem payment mechanism algorithm incentivecompatible satisfies nofreelunch condition payment function based gold standard questions also called strictly proper scoring rule [ gr ] proof theorem presented appendix easy see mechanism satisfies nofreelunch proof incentive compatibility also hard consider arbitrary worker arbitrary belief distributions compute expected payment worker case choices task follow requirements show choice leads strictly smaller expected payment started weak condition nofreelunch making zero payment attempted answers wrong mechanism proposed algorithm significantly strict makes zero payment attempted answers wrong natural question arises design alternative mechanism satisfying incentive compatibility nofreelunch operates somewhere uniqueness mechanism previous section showed proposed multiplicative mechanism incentive compatible satisfies intuitive requirement nofreelunch turns perhaps surprisingly mechanism unique respect theorem payment mechanism algorithm incentivecompatible mechanism satisfies nofreelunch condition theorem gives strong result despite imposing weak requirements see recall earlier discussion deterring spammers incurring low expenditure workers answer randomly instance task comprises binarychoice questions one may wish design mechanisms make zero payment responses questions gold standard incorrect nofreelunch axiom much weaker requirement mechanism satisfy requirement mechanism algorithm proof theorem available appendix b proof relies following key lemma establishes condition incentivecompatible mechanism must necessarily satisfy lemma applies incentivecompatible mechanism satisfying nofreelunch lemma incentivecompatible payment mechanism f must satisfy every g every yi yi yg g f yi yi yg f yi yi yg f yi yi yg proof lemma provided appendix c given lemma proof theorem completed via induction number skipped questions optimality spamming behavior discussed earlier crowdsouring tasks especially multiple choice questions often encounter spammers answer randomly without heed question asked instance binarychoice setup spammer choose one two options uniformly random every question highly desirable objective crowdsourcing settings deter spammers end one may wish impose condition zero payment responses attempted questions gold standard incorrect second desirable metric could minimize expenditure worker simply skips questions aforementioned requirements deterministic functions workers responses one may alternatively wish impose requirements depend distribution workers answering process instance third desirable feature would minimize expected payment worker answers questions uniformly random show interestingly unique multiplicative payment mechanism simultaneously satisfies requirements result stated assuming multiplechoice setup extends trivially nonmultiplechoice settings theorem distributional consider value g among incentivecompatible mechanisms may may satisfy nofreelunch algorithm strictly minimizes expenditure worker skips questions gold standard chooses answers remaining g questions uniformly random theorem b deterministic consider value b ] among incentivecompatible mechanisms may may satisfy nofreelunch algorithm strictly minimizes expenditure worker gives incorrect answers fraction b questions attempted gold standard proof theorem presented appendix see result multiplicative payment mechanism algorithm thus possesses useful properties geared deter spammers ensuring good worker paid high enough amount illustrate point let us compare mechanism algorithm popular additive class payment mechanisms example consider popular class additive mechanisms payments worker added across gold standard questions additive payment mechanism offers reward g every correct answer gold standard g every question skipped every incorrect answer importantly final payment worker sum rewards across g gold standard questions one verify additive mechanism incentive compatible one also see guaranteed theory additive payment mechanism satisfy nofreelunch axiom suppose question involves choosing two options let us compute expenditure two mechanisms make spamming behavior choosing answer randomly question given likelihood question correct compute additive mechanism makes payment expectation hand mechanism pays expected amount g payment spammers thus reduces exponentially number gold standard questions mechanism whereas reduce additive mechanism consider different means exploiting mechanism worker simply skips questions end observe worker skips questions additive payment mechanism incur expenditure hand proposed payment mechanism algorithm pays exponentially smaller amount g recall simulations experiments section present synthetic simulations realworld experiments evaluate effects setting mechanism final label quality synthetic simulations employ synthetic simulations understand effects various kinds labeling errors crowdsourcing consider binarychoice questions set simulations whenever worker answers question confidence correct answer drawn distribution p independent else investigate effects following five choices distribution p uniform distribution support [ ] triangular distribution lower endpoint upper endpoint mode beta distribution parameter values hammerspammer distribution [ kos ] uniform discrete set truncated gaussian distribution truncation n interval [ ] worker confidence p drawn distribution p attempts question probability making error equals p compare setting workers attempt every question b setting workers skip questions confidence certain threshold set simulations set either setting aggregate labels obtained workers question via majority vote two classes ties broken choosing one two options uniformly random figure error different interfaces synthetic simulations five distributions workers error probabilities figure depicts results simulations bar represents fraction questions labeled incorrectly average across trials standard error mean small visible see skipbased setting consistently outperforms conventional setting gains obtained moderate high depending underlying distribution workers errors particular gains quite striking hammerspammer model result surprising since mechanism ideally screens spammers leaves hammers answer perfectly experiments amazon mechanical turk conducted preliminary experiments amazon mechanical turk commercial crowdsourcing platform mturk com evaluate proposed scheme realworld scenarios complete data including interface presented workers tasks results obtained workers ground truth solutions available website first author goal delving details first note certain caveats relating study mechanism design crowdsourcing platforms worker encounters mechanism small amount time handful tasks typical research experiments small amount money dollars typical crowdsourcing tasks cannot expect worker completely understand mechanism act precisely required instance wouldnt expect experimental results change significantly even upon moderate modifications promised amounts furthermore expect outcomes noisy incentive compatibility kicks worker encounters mechanism across longer term example proposed mechanism adopted standard platform higher amounts involved would expect workers others e g bloggers researchers design strategies game mechanism theoretical guarantee incentive compatibility strict properness prevents gaming long run thus regard experiments preliminary intentions towards experimental exercise evaluate potential algorithms work practice b investigate effect proposed algorithms net error collected labelled data experimental setup conducted five following experiments tasks amazon mechanical turk identifying golden gate bridge pictures b identifying breeds dogs pictures c identifying heads countries identifying continents flags belong e identifying textures displayed images tasks comprised multi figure error different interfaces mechanisms five experiments conducted mechanical turk ple choice questions experiment compared baseline setting figure additive payment mechanism pays fixed amount per correct answer ii skipbased setting figure b multiplicative mechanism algorithm experiment two settings workers independently perform task upon completion tasks amazon mechanical turk aggregated data following manner mechanism experiment subsampled workers took majority vote responses averaged accuracy across questions across iterations subsampleandaggregate procedure results figure reports error aggregate data five experiments see cases skipbased setting results higher quality data many instances reduction twofold higher experiments observed substantial reduction amount error labelled data expending lower amounts receiving negative comments workers observations suggest proposed skipbased setting coupled multiplicative payment mechanisms potential work practice underlying fundamental theory ensures system cannot gamed long run discussion conclusions extended version paper [ sz ] generalize skipbased setting considered one also elicit workers confidence answers moreover companion paper [ szp ] construct mechanisms elicit support workers beliefs mechanism offers additional benefits pattern skips workers provide reasonable estimate difficulty question practice questions estimated difficult may delegated expert additional nonexpert workers secondly theoretical guarantees mechanism may allow better postprocessing data incorporating confidence information improving overall accuracy developing statistical aggregation algorithms augmenting existing ones e g [ ryz kos lpi zlp ] purpose useful direction research thirdly simplicity mechanisms may facilitate easier adoption among workers conclusion given uniqueness optimality theory simplicity good performance observed practice envisage multiplicative payment mechanisms interest practitioners well researchers employ crowdsourcing see extended version paper [ sz ] additional experiments involving freeform responses text transcription references [ boh ] john bohannon social science pennies science [ cbw ] andrew carlson justin betteridge richard c wang estevam r hruschka jr tom mitchell coupled semisupervised learning information extraction acm wsdm pages [ dds ] jia deng wei dong richard socher lijia li kai li li feifei imagenet largescale hierarchical image database ieee conference computer vision pattern recognition pages [ dou ] double nothing httpwikipedia orgwikidouble_or_nothing last accessed july [ gr ] tilmann gneiting adrian e raftery strictly proper scoring rules prediction estimation journal american statistical association [ hdy ] geoffrey hinton li deng dong yu george e dahl abdelrahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara n sainath et al deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine panagiotis g ipeirotis foster provost victor sheng jing wang repeated labeling using multiple noisy labelers data mining knowledge discovery [ ipsw ] [ jsv ] srikanth jagabathula lakshminarayanan subramanian ashwin venkataraman reputationbased worker filtering crowdsourcing advances neural information processing systems pages [ kkkmf ] gabriella kazai jaap kamps marijn koolen natasa milicfrayling crowdsourcing book search evaluation impact hit design comparative system ranking acm sigir pages [ kos ] david r karger sewoong oh devavrat shah iterative learning reliable crowdsourcing systems advances neural information processing systems pages [ lpi ] qiang liu jian peng alexander ihler variational inference crowdsourcing nips pages [ ryz ] vikas c raykar shipeng yu linda h zhao gerardo hermosillo valadez charles florin luca bogoni linda moy learning crowds journal machine learning research [ sz ] nihar b shah dengyong zhou double nothing multiplicative incentive mechanisms crowdsourcing arxiv [ szp ] nihar b shah dengyong zhou yuval peres approval voting incentives crowdsourcing international conference machine learning icml [ vdve ] jeroen vuurens arjen p de vries carsten eickhoff much spam take analysis crowdsourcing results increase accuracy acm sigir workshop crowdsourcing information retrieval pages [ wlc ] paul wais shivaram lingamneni duncan cook jason fennell benjamin goldenberg daniel lubarov david marin hari simons towards building highquality workforce mechanical turk nips workshop computational social science wisdom crowds [ zlp ] dengyong zhou qiang liu john c platt christopher meek nihar b shah regularized minimax conditional entropy crowdsourcing arxiv preprint arxiv')),\n",
       " ([(4, 0.9995748)],\n",
       "  (5941,\n",
       "   'learning symmetric label noise importance unhinged brendan van rooyen aditya krishna menon australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation de facto approach binary classification however long servedio [ ] proved symmetric label noise sln minimisation convex potential linear function class result classification performance equivalent random guessing ostensibly shows convex losses slnrobust paper propose convex classificationcalibrated loss prove slnrobust loss avoids long servedio [ ] result virtue negatively unbounded loss modification hinge loss one clamp zero hence call unhinged loss show optimal unhinged solution equivalent strongly regularised svm limiting solution convex potential implies strong ` regularisation makes standard learners slnrobust experiments confirm unhinged loss slnrobustness borne practice apologies wilde [ ] truth rarely pure simple learning symmetric label noise binary classification canonical supervised learning problem given instance space x samples distribution x goal learn scorer x r low misclassification error future samples drawn interest realistic scenario learner observes samples corruption labels constant probability flipped goal still perform well respect problem known learning symmetric label noise sln learning [ angluin laird ] long servedio [ ] showed exist linearly separable learner observes corruption symmetric label noise nonzero rate minimisation convex potential linear function class results classification performance equivalent random guessing ostensibly establishes convex losses slnrobust motivates use nonconvex losses [ stempfel ralaivola masnadishirazi et al ding vishwanathan denchev et al manwani sastry ] paper propose convex loss prove slnrobust loss avoids result long servedio [ ] virtue negatively unbounded loss modification hinge loss one clamp zero thus call unhinged loss loss several appealing properties unique convex loss satisfying notion strong slnrobustness proposition classificationcalibrated proposition consistent minimised proposition simple optimal solution difference two kernel means equation finally show optimal solution equivalent strongly regularised svm proposition twicedifferentiable convex potential proposition implying strong ` regularisation endows standard learners slnrobustness classifier resulting minimising unhinged loss new [ devroye et al chapter ] [ scholkopf smola section ] [ shawetaylor cristianini section ] however establishing classifiers strong slnrobustness uniqueness thereof equivalence highly regularised svm solution knowledge novel background problem setup fix instance space x denote distribution x random variables x may expressed via classconditionals p q p x p x base rate p via marginal p x classprobability function x p x x interchangeably write dpq dm classifiers scorers risks scorer function x r loss function ` r r use ` ` refer ` ` ` conditional risk l ` [ ] r r defined l ` v ` v ` v given distribution ` risk scorer defined ld ` [ ` x ] e xy ld ` e [ l ` x x ] set l ` set ` risks scorers xm function class f rx given f set restricted bayesoptimal scorers loss ` scorers f minimise ` risk sdf argmin ld ` ` sf set unrestricted bayesoptimal scorers sd sdf f rx restricted ` ` ` regret scorer excess risk restricted bayesoptimal scorer regretdf ld ` inf l ` ` tf binary classification concerned zeroone loss ` v jyv k jv k loss ` classificationcalibrated bayesoptimal scorers also optimal zeroone loss sd sd convex potential loss ` v yv r r ` convex nonincreasing differentiable [ long servedio definition ] convex potentials classificationcalibrated [ bartlett et al theorem ] learning symmetric label noise sln learning problem learning symmetric label noise sln learning following [ angluin laird kearns blum mitchell natarajan et al ] notional clean distribution would like observe instead observe samples corrupted distribution sln [ distribution sln marginal distribution instances unchanged label independently flipped probability goal learn scorer corrupted samples ld small quantity denote corrupted counterparts sln bar e g corrupted marginal distribution corrupted classprobability function additionally clear context occasionally refer sln easy check corrupted marginal distribution [ natarajan et al lemma ] x x x x slnrobustness formalisation consider learners ` f loss ` function class f learning search f minimises ` risk informally ` f robust symmetric label noise slnrobust minimising ` f gives classifier clean distribution learner would like observe sln [ learner actually observes formalise notion review known slnrobust learners slnrobust learners formal definition fixed instance space x let denote set distributions x given notional clean distribution nsln returns set possible corrupted versions learner may observe labels flipped unknown probability nsln sln equipped define notion slnrobustness definition slnrobustness say learner ` f slnrobust df df nsln ld ld ` ` slnrobustness requires level label noise observed distribution classification performance wrt learner learner directly observes unfortunately widely adopted class learners slnrobust see convex potentials linear function classes slnrobust fix x rd consider learners convex potential ` function class linear scorers flin x hw xi w rd captures e g linear svm logistic regression widely studied theory applied practice disappointingly learners slnrobust long servedio [ theorem ] give example learning symmetric label noise convex potential ` corrupted ` risk minimiser flin classification performance equivalent random guessing implies ` flin slnrobust per definition proposition long servedio [ theorem ] let x rd pick convex potential ` ` flin slnrobust fallout learners slnrobust light proposition two ways proceed order obtain slnrobust learners either change class losses ` change function class f first approach pursued large body work embraces nonconvex losses [ stempfel ralaivola masnadishirazi et al ding vishwanathan denchev et al manwani sastry ] losses avoid conditions proposition automatically imply slnrobust used flin appendix b present evidence losses fact slnrobust used flin second approach consider suitably rich f contains bayesoptimal scorer e g employing universal kernel choice one still use convex potential loss fact owing equation classificationcalibrated loss proposition pick classificationcalibrated ` ` rx slnrobust approaches drawbacks first approach computational penalty requires optimising nonconvex loss second approach statistical penalty estimation rates rich f require larger sample size thus appears slnrobustness involves computationalstatistical tradeoff however variant first option pick loss convex convex potential loss would afford computational statistical advantages minimising convex risks linear scorers manwani sastry [ ] demonstrated square loss ` v yv one loss show simpler loss convex slnrobust class convex potentials virtue negatively unbounded derive loss first reinterpret robustness via noisecorrection procedure even content difference [ ] clean corrupted minimisers performance long servedio [ theorem ] implies worst case noisecorrected loss perspective slnrobustness reexpress slnrobustness reason optimal scorers distribution two different losses help characterise set strongly slnrobust losses reformulating slnrobustness via noisecorrected losses given [ natarajan et al [ lemma ] showed associate loss ` noisecorrected counterpart ` ld ` l ` loss ` defined follows definition noisecorrected loss given loss ` [ noisecorrected loss ` v r ` v ` v ` v since ` depends unknown parameter directly usable design slnrobust learner nonetheless useful theoretical device since construction f sdf ` sdf sdf means sufficient condition ` f slnrobust sdf ` ` ` ghosh et al [ theorem ] proved sufficient condition ` holds namely c r v r ` v ` v c interestingly equation necessary stronger notion robustness explore characterising stronger notion slnrobustness first step towards stronger notion robustness rewrite slight abuse notation ld ` e xy [ ` x ] e ys r ds [ ` ] l ` r r distribution labels scores standard slnrobustness requires label noise change ` risk minimisers e l ` r l ` r relation holds place strong slnrobustness strengthens notion requiring label noise affect ordering pairs joint distributions labels scores course trivially implies slnrobustness definition given distribution r labels scores let r corresponding distribution labels flipped probability strong slnrobustness made precise follows definition strong slnrobustness call loss ` strongly slnrobust every [ r r l ` r l ` r l ` r l ` r reexpress strong slnrobustness using notion order equivalence loss pairs simply requires two losses order distributions labels scores identically order equivalent definition order equivalent loss pairs call pair losses ` ` r r l ` r l ` r l ` r l ` r clearly order equivalence ` ` implies sdf sdf turn implies slnrobustness ` ` thus surprising relate order equivalence strong slnrobustness ` proposition loss ` strongly slnrobust iff every [ ` ` order equivalent connection lets us exploit classical result decision theory order equivalent losses affine transformations combined definition ` lets us conclude sufficient condition equation also necessary strong slnrobustness ` proposition loss ` strongly slnrobust satisfies equation return original goal find convex ` slnrobust flin ideally general function classes suggests reasonable consider losses satisfy equation unfortunately evident ` convex nonconstant bounded zero cannot possibly admissible sense show removing boundedness restriction allows existence convex admissible loss unhinged loss convex strongly slnrobust loss consider following simple nonstandard convex loss unh ` unh v v ` v v compared hinge loss loss clamp zero e hinge thus peculiarly negatively unbounded issue discuss thus call unhinged loss loss number attractive properties immediate slnrobustness unhinged loss strongly slnrobust unh unh since ` unh strongly slnrobust thus v ` v proposition implies ` unh ` f slnrobust f following uniqueness property hard show proposition pick convex loss ` c r ` v ` v c b r ` v v b ` v v scaling translation ` unh convex loss strongly slnrobust returning case linear scorers implies ` unh flin slnrobust contradict proposition since ` unh convex potential negatively unbounded intuitively property allows loss offset penalty incurred instances misclassified high margin awarding gain instances correctly classified high margin unhinged loss classification calibrated slnrobustness insufficient learner useful example loss uniformly zero strongly slnrobust useless classificationcalibrated fortunately unhinged loss classificationcalibrated establish technical reasons see operate fb [ b b ] x set scorers range bounded b [ proposition fix ` ` unh dm b [ ` dfb x b sign x thus every b [ restricted bayesoptimal scorer fb sign bayesoptimal classifier loss limiting case f rx optimal scorer attainable operate extended reals r ` unh classificationcalibrated enforcing boundedness loss classificationcalibration ` unh encouraging proposition implies unrestricted bayesrisk thus regret every nonoptimal scorer identically hampers analysis consistency orthodox decision theory analogous theoretical issues arise attempting establish basic theorems unbounded losses [ ferguson pg ] sidestep issue restricting attention bounded scorers ` unh effectively bounded proposition affect classificationcalibration loss context linear scorers boundedness scorers achieved regularisation instead work ing flin one instead use flin x hw xi w flin fr r supxx x observe ` unh f slnrobust f ` unh flin slnrobust shall see working flin also lets us establish slnrobustness hinge loss large unhinged loss minimisation corrupted distribution consistent using bounded scorers makes possible establish surrogate regret bound unhinged loss shows classification consistency unhinged loss minimisation corrupted distribution loss considered sriperumbudur et al [ ] reid williamson [ ] context maximum mean discrepancy see appendix analysis slnrobustness knowledge novel proposition fix ` ` unh [ b [ scorer fb dfb regretd regret ` dfb regret ` standard rates convergence via generalisation bounds also trivial derive see appendix learning unhinged loss kernels show optimal solution unhinged loss employing regularisation kernelised scorers simple form sheds light slnrobustness regularisation centroid classifier optimises unhinged loss consider minimising unhinged risk class kernelised scorers fh x hw x ih w h x h feature mapping reproducing kernel hilbert space h kernel k equivalently given distribution want wunh argmin e [ hw x ] hw wih xy wh firstorder optimality condition implies wunh e [ x ] xy kernel mean map [ smola et al ] thus optimal unhinged scorer sunh x e [ k x x ] x e [ k x x ] e [ k x x ] xp xq xy equation unhinged solution equivalent nearest centroid classifier [ manning et al pg ] [ tibshirani et al ] [ shawetaylor cristianini section ] equation gives simple way understand slnrobustness ` unh fh optimal scorers clean corrupted distributions differ scaling see appendix e k x x x x e [ k x x ] xy xy interestingly servedio [ theorem ] established nearest centroid classifier termed average robust general class label noise required assumption uniform unit sphere result establishes sln robustness classifier holds without assumptions fact ghosh et al [ theorem ] lets one quantify unhinged loss performance general noise model see appendix discussion practical considerations note several points relating practical usage unhinged loss kernelised scorers first crossvalidation required select since changing changes magnitude scores sign thus purposes classification one simply use second easily extend scorers use bias regularised strength b tuning b equivalent computing sunh per equation tuning threshold holdout set third h rd small store wunh explicitly use make predictions high infinite dimensional h either make predictions directly via equation use random fourier features [ rahimi recht ] approximately embed h low dimensional rd store wunh usual latter requires translationinvariant kernel show assumptions wunh coincides solution two established methods appendix discusses relationships e g maximum mean discrepancy given training sample dn use plugin estimates appropriate equivalence highly regularised svm convex potentials interesting equivalence unhinged solution highly regularised svm noted e g hastie et al [ section ] showed svms approach nearest centroid classifier course optimal unhinged solution proposition pick x h r supxx x h let whinge argmin wh e xy [ max hw x ih ] hw wih softmargin svm solution r whinge wunh since ` unh fh slnrobust follows ` hinge v max yv ` hinge fh similarly slnrobust provided sufficiently large strong ` regularisation bounded feature map endows hinge loss slnrobustness proposition generalised show wunh limiting solution twice differentiable convex potential shows strong ` regularisation endows learners slnrobustness intuitively strong regularisation one considers behaviour loss near zero since convex potential behave similarly linear approximation around zero viz unhinged loss proposition pick bounded feature mapping x h twice differentiable convex potential [ ] bounded let w minimiser regularised risk w wunh lim w h wunh h h equivalence fisher linear discriminant whitened data binary classification dm fisher linear discriminant fld finds weight vector proportional minimiser square loss ` sq v yv [ bishop section ] wsq exm [ xxt ] e xy [ x ] wsq changed scaling equation fact corrupted marginal factor label noise provides alternate proof fact ` sq flin slnrobust [ manwani sastry theorem ] clearly unhinged loss solution wunh equivalent fld square loss solution wsq input data whitened e e xx xm wellspecified f e g universal kernel unhinged square loss asymptotically recover optimal classifier unhinged loss require matrix inversion misspecified f one cannot general argue superiority unhinged loss square loss viceversa universally good surrogate loss [ reid williamson appendix ] appendix illustrate examples losses may underperform slnrobustness unhinged loss empirical illustration illustrate unhinged loss slnrobustness empirically manifest reiterate high regularisation unhinged solution equivalent svm limit classificationcalibrated loss solution thus aim assert unhinged loss better losses rather demonstrate slnrobustness purely theoretical first show unhinged risk minimiser performs well example long servedio [ ] henceforth ls figure shows distribution x r marginal distribution three instances deterministically positive pick unhinged minimiser perfectly classifies three points regardless level label noise figure hinge minimiser perfect noise even small amount noise achieves error rate long servedio [ section ] show ` regularisation endow slnrobustness square loss escapes result long servedio [ ] since monotone decreasing unhinged hinge noise hinge noise hinge tlogistic unhinged table mean standard deviation error trials ls grayed cells denote best performer noise rate figure ls dataset next consider empirical risk minimisers random training sample construct training set instances injected varying levels label noise evaluate classification performance test set instances compare hinge tlogistic [ ding vishwanathan ] unhinged minimisers using linear scorer without bias term regularisation strength table even label noise unhinged classifier able find perfect solution contrast losses suffer even moderate noise rates next report results uci datasets additionally tune threshold ensure best training set accuracy table summarises results sample four datasets appendix contains results datasets performance metrics losses even noise close unhinged loss often able learn classifier discriminative power hinge tlogistic unhinged hinge tlogistic unhinged iris b housing hinge tlogistic unhinged c uspsv hinge tlogistic unhinged splice table mean standard deviation error trials uci datasets conclusion future work proposed convex classificationcalibrated loss proved robust symmetric label noise slnrobust showed unique loss satisfies notion strong slnrobustness established optimised nearest centroid classifier showed convex potentials svm also slnrobust highly regularised apologies wilde [ ] truth rarely pure simple acknowledgments nicta funded australian government department communications australian research council ict centre excellence program authors thank cheng soon ong valuable comments draft paper references dana angluin philip laird learning noisy examples machine learning peter l bartlett michael jordan jon mcauliffe convexity classification risk bounds journal american statistical association christopher bishop pattern recognition machine learning springerverlag new york inc avrim blum tom mitchell combining labeled unlabeled data cotraining conference computational learning theory colt pages vasil denchev nan ding hartmut neven v n vishwanathan robust classification adiabatic quantum optimization international conference machine learning icml pages luc devroye laszlo gyorfi gabor lugosi probabilistic theory pattern recognition springer nan ding v n vishwanathan tlogistic regression advances neural information processing systems nips pages curran associates inc thomas ferguson mathematical statistics decision theoretic approach academic press aritra ghosh naresh manwani p sastry making risk minimization tolerant label noise neurocomputing trevor hastie saharon rosset robert tibshirani ji zhu entire regularization path support vector machine journal machine learning research december issn michael kearns efficient noisetolerant learning statistical queries journal acm november philip long rocco servedio random classification noise defeats convex potential boosters machine learning issn christopher manning prabhakar raghavan hinrich schutze introduction information retrieval cambridge university press new york ny usa isbn naresh manwani p sastry noise tolerance risk minimization ieee transactions cybernetics june hamed masnadishirazi vijay mahadevan nuno vasconcelos design robust classifiers computer vision ieee conference computer vision pattern recognition cvpr nagarajan natarajan inderjit dhillon pradeep ravikumar ambuj tewari learning noisy labels advances neural information processing systems nips pages ali rahimi benjamin recht random features largescale kernel machines advances neural information processing systems nips pages mark reid robert c williamson composite binary losses journal machine learning research december mark reid robert c williamson information divergence risk binary experiments journal machine learning research mar bernhard scholkopf alexander j smola learning kernels volume mit press rocco servedio pac learning using winnow perceptron perceptronlike algorithm conference computational learning theory colt john shawetaylor nello cristianini kernel methods pattern analysis cambridge uni press alex smola arthur gretton le song bernhard scholkopf hilbert space embedding distributions algorithmic learning theory alt bharath k sriperumbudur kenji fukumizu arthur gretton gert r g lanckriet bernhard scholkopf kernel choice classifiability rkhs embeddings probability distributions advances neural information processing systems nips guillaume stempfel liva ralaivola learning svms sloppily labeled data artificial neural networks icann volume pages springer berlin heidelberg robert tibshirani trevor hastie balasubramanian narasimhan gilbert chu diagnosis multiple cancer types shrunken centroids gene expression proceedings national academy sciences oscar wilde importance earnest')),\n",
       " ([(5, 0.015908485), (6, 0.9837629)],\n",
       "  (6019,\n",
       "   'algorithmic stability uniform generalization ibrahim alabdulmohsin king abdullah university science technology thuwal saudi arabia ibrahim alabdulmohsinkaust edu sa abstract one central questions statistical learning theory determine conditions agents learn experience includes necessary sufficient conditions generalization given finite training set new observations paper prove algorithmic stability inference process equivalent uniform generalization across parametric loss functions provide various interpretations result instance relationship proved stability data processing reveals algorithmic stability improved postprocessing inferred hypothesis augmenting training examples artificial noise prior learning addition establish relationship algorithmic stability size observation space provides formal justification dimensionality reduction methods finally connect algorithmic stability size hypothesis space recovers classical pac result size complexity hypothesis space controlled order improve algorithmic stability improve generalization introduction one fundamental goal learning algorithm strike right balance underfitting overfitting mathematical terms often translated two separate objectives first would like learning algorithm produce hypothesis reasonably consistent empirical evidence e small empirical risk second would like guarantee empirical risk training error valid estimate true unknown risk test error former condition protects underfitting latter condition protects overfitting rationale behind two objectives understood define generalization risk rgen absolute difference empirical true risks rgen remp rtrue elementary observe true risk rtrue bounded sum remp rgen hence minimizing empirical risk underfitting generalization risk overfitting one obtains inference procedure whose true risk minimal minimizing empirical risk alone carried using empirical risk minimization erm procedure [ ] approximations however generalization risk often impossible deal directly instead common practice bound analyticaly establish conditions guaranteed small establishing conditions generalization one hopes design better learning algorithms perform well empirically generalize well novel observations future prominent example approach support vector machines svm algorithm binary classification [ ] however bounding generalization risk quite intricate approached various angles fact several methods proposed past prove generalization bounds including uniform convergence algorithmic stability rademacher gaussian complexities generic chaining bounds pacbayesian framework robustnessbased analysis [ ] concentration measure inequalities form building blocks rich theories proliferation generalization bounds understood look general setting learning introduced vapnik [ ] setting observation space z hypothesis space h learning algorithm henceforth denoted l h uses finite set z observations infer hypothesis h h general setting inference process endtoend influenced three key factors nature observation space z nature hypothesis space h details learning algorithm l imposing constraints three components one may able derive new generalization bounds example vapnikchervonenkis vc theory derives generalization bounds assuming constraints h stability bounds e g [ ] derived assuming constraints l given different generalization bounds established imposing constraints z h l intriguing ask exists single view generalization ties different components together paper answer question affirmative establishing algorithmic stability alone equivalent uniform generalization informally speaking inference process said generalize uniformly generalization risk vanishes uniformly across bounded parametric loss functions limit large training sets precise definition presented sequel show constraints imposed either h z l improve uniform generalization interpreted methods improving stability learning algorithm l similar spirit result kearns ron showed finite vc dimension hypothesis space h implies certain notion algorithmic stability inference process [ ] statement however general applies learning algorithms fall vapniks general setting learning well beyond uniform convergence rest paper follows first review current literature algorithmic stability generalization learnability introduce key definitions repeatedly used throughout paper next prove central theorem reveals algorithmic stability equivalent uniform generalization provide various interpretations result afterward related work perhaps two fundamental concepts statistical learning theory learnability generalization [ ] two concepts distinct discussed details next whereas learnability concerned measuring excess risk within hypothesis space generalization concerned estimating true risk order define learnability generalization suppose observation space z probability distribution observations p z bounded stochastic loss function l h z [ ] h h inferred hypothesis note l implicitly function parameterized h well define true risk hypothesis h h risk functional rtrue h ezp z l z h learning algorithm called consistent true risk inferred hypothesis h converges optimal true risk within hypothesis space h limit large training sets problem called learnable admits consistent learning algorithm [ ] known learnability supervised classification regression problems equivalent uniform convergence [ ] however shalevshwartz et al recently showed uniform convergence necessary vapniks general setting learning proposed algorithmic stability alternative key condition learnability [ ] unlike learnability question generalization concerned primarily representative empirical risk remp true risk rtrue elaborate suppose finite training set sm zi comprises observations zi p z define empirical risk hypothesis h respect sm x remp h sm l zi h zi sm also let rtrue h true risk defined eq learning algorithm l said generalize empirical risk inferred hypothesis converges true risk similar learnability uniform convergence definition sufficient generalization [ ] necessary learning algorithm always restrict search space smaller subset h artificially speak contrast known whether algorithmic stability necessary generalization shown various notions algorithmic stability defined sufficient generalization [ ] however known whether appropriate notion algorithmic stability defined necessary sufficient generalization vapniks general setting learning paper answer question showing stability inference process sufficient generalization fact equivalent uniform generalization notion generalization stronger one traditionally considered literature preliminaries simplify discussion always assume sets countable including observation space z hypothesis space h similar assumptions used previous works [ ] however main results presented section readily generalized addition assume learning algorithms invariant permutations training set hence order training examples irrelevant moreover x p x random variable drawn alphabet x f x function p x write exp x f x mean xx p x f x often simply write ex f x mean exp x f x distribution x clear context x takes values finite set uniformly random write x denote distribution x x boolean random variable x x true otherwise x general random variables denoted capital letters instances random variables denoted small letters alphabets denoted calligraphic typeface also given two probability mass functions p q defined alphabet write hp qi denote overlapping p coefficient e intersection p q hp qi aa min p q note hp qi p q p q total variation distance last write b k n nk k nk denote binomial distribution paper consider general setting learning introduced vapnik [ ] reiterate observation space z hypothesis space h learning algorithm l receives set observations sm zi z generated fixed unknown distribution p z picks hypothesis h h probability pl h h sm formally l h z stochastic map paper allow hypothesis h summary statistic training set measure central tendency unsupervised learning mapping input space output space supervised learning fact even allow h subset training set formal terms l stochastic map two random variables h h sm z exact interpretation random variables irrelevant learning task assume nonnegative bounded loss function l z h z [ ] used measure quality inferred hypothesis h h observation z z importantly assume l h z [ ] parametric definition parametric loss functions loss function l h z [ ] called parametric independent training set sm given inferred hypothesis h parametric loss function satisfies markov chain sm h l h fixed hypothesis h h define true risk rtrue h eq define empirical risk training set sm denoted remp h sm eq also define true empirical risks learning algorithm l expected risk inferred hypothesis rtrue l esm eh pl h sm rtrue h esm eh sm rtrue h remp l esm eh pl h sm remp h sm esm eh sm remp h sm simplify notation write rtrue remp instead rtrue l remp l consider following definition generalization definition generalization learning algorithm l h parametric z loss function l h z [ ] generalizes distribution p z z limm remp rtrue rtrue remp given eq eq respectively words learning algorithm l generalizes according definition empirical performance training loss becomes unbiased estimator true risk next define uniform generalization definition uniform generalization learning algorithm l h generalizes z uniformly exists distributions p z z parametric loss functions sample sizes remp l rtrue l uniform generalization stronger original notion generalization definition particular learning algorithm generalizes uniformly generalizes according definition well converse however true even though uniform generalization appears quite strong condition first sight key contribution paper show strong condition equivalent simple condition namely algorithmic stability main results prove algorithmic stability equivalent uniform generalization introduce probabilistic notion mutual stability two random variables order abstract away labeling information random variables might possess e g observation space may may metric space define stability impact observations probability distributions definition mutual stability let x x two random variables mutual stability x defined x hp x p p x ex hp p x ey hp x p x recall hp qi overlapping coefficient two probability distributions p q see x given definition indeed probabilistic measure mutual stability measures stable distribution observing instance x vice versa small value x means probability distribution x heavily perturbed single observation random variable perfect mutual stability achieved two random variables independent probabilistic notion mutual stability mind define stability learning algorithm l mutual stability inferred hypothesis random training example h learning algorithm receives definition algorithmic stability let l z finite set training examples sm zi z drawn fixed distribution p z let h pl h sm hypothesis inferred l let ztrn sm single random training example define stability l l inf p z h ztrn infimum taken possible distributions observations p z learning algorithm called algorithmically stable limm l note definition algorithmic stability rather weak requires contribution single training example overall inference process negligible sample size increases addition welldefined even learning algorithm deterministic hypothesis h deterministic function entire training set observations remains stochastic function individual observation illustrate concept following example example suppose observations zi bernoulli p trials p zi hypothesis produced l empirical average h zi p h km ztrn b k p h km ztrn b k shown using stirlings approximation [ ] algorithmic stability learning algorithm asymptotically given l achieved general statement proved later section next show notion algorithmic stability definition equivalent notion uniform generalization definition first state following lemma lemma data processing inequality let b c three random variables satisfy markov chain b c b c proof proof consists two steps first note markov chain implies p c b p c b b c b direct substitution definition second similar informationcannothurt inequality information theory [ ] shown b c c random variables b c proved using algebraic manipulationand minimum sums always larger &#9; thep p fact p sum minimums e min min combining results yields b b c c desired result ready state main result paper theorem learning algorithm l h algorithmic stability given defm z inition necessary sufficient uniform generalization see definition addition rtrue remp h ztrn l rtrue remp true empirical risks learning algorithm defined eq respectively proof outline proof first parametric loss function l h z [ ] random variable satisfies markov chain sm h l h independent ztrn sm hence empirical risk given remp el h eztrn l h l ztrn h contrast true risk given rtrue el h eztrn p z l ztrn h difference rtrue remp el h eztrn l ztrn h eztrn l h l ztrn h sandwich righthand side upper lower bound note p z p z two distributions defined alphabet z f z [ ] bounded loss function ezp z f z ezp z f z p z p z p q total variation distance proof result immediately deduced considering two regions z z p z p z z z p z p z separately used deduce inequalities rtrue remp l h ztrn h ztrn l second inequality follows data processing inequality lemma whereas last inequality follows definition algorithmic stability see definition proves l algorithmically stable e l rtrue remp converges zero uniformly across parametric loss functions therefore algorithmic stability sufficient uniform generalization converse proved showing bounded exists parametric loss distribution p z l rtrue remp l therefore algorithmic stability also necessary uniform generalization interpreting algorithmic stability uniform generalization section provide several interpretations algorithmic stability uniform generalization addition show theorem recovers classical results learning theory algorithmic stability data processing relationship algorithmic stability data processing presented lemma given random variables b c markov chain b c always b c presents us qualitative insights design machine learning algorithms first suppose two different hypotheses h h say h contains less informative h markov chain sm h h holds example observations zi bernoulli trials h r empirical average given example h label occurs often training set h h hypothesis h contains strictly less information original training set h formally sm h h case h enjoys better uniform generalization bound h dataprocessing intuitively know result hold h less tied original training set h brings us following remark detailed proofs available supplementary file remark improve uniform generalization bound equivalently algorithmic stability learning algorithm postprocessing inferred hypothesis h manner conditionally independent original training set given h example postprocessing hypotheses common technique used machine learning includes sparsifying coefficient vector w rd linear methods wj set zero small absolute magnitude also includes methods proposed reduce number support vectors svm exploiting linear dependence [ ] data processing inequality methods improve algorithmic stability uniform generalization needless mention better generalization immediately translate smaller true risk empirical risk may increase inferred hypothesis postprocessed independently original training set second markov chain b c holds also obtain c b c applying data processing inequality reverse markov chain c b result improve algorithmic stability contaminating training examples artificial noise prior learning sm perturbed version training set sm sm sm h implies ztrn h ztrn h ztrn sm ztrn sm random training examples drawn uniformly random training set respectively brings us following remark remark improve algorithmic stability learning algorithm introducing artificial noise training examples applying learning algorithm perturbed training set example corrupting training examples artificial noise recent dropout method popular techniques neural networks improve generalization [ ] data processing inequality methods indeed improve algorithmic stability uniform generalization algorithmic stability size observation space next look size observation space z influences algorithmic stability first start following definition definition lazy learning learning algorithm l called lazy hypothesis h h mapped onetoone training set sm e mapping h sm injective lazy learner called lazy hypothesis equivalent original training set information content hence learning actually takes place one example instancebased learning h sm despite simple nature lazy learners useful practice useful theoretical tools well particular equivalence h sm data processing inequality algorithmic stability lazy learner provides lower bound stability possible learning algorithm therefore relate algorithmic stability uniform generalization size observation space quantifying algorithmic stability lazy learners size z usually infinite however introduce following definition effective set size definition countable space z endowed probability mass function p z effective p p size z w r p z defined ess [ z p z ] p z p z zz one extreme p z uniform finite alphabet z ess [ z p z ] z extreme p z kronecker delta distribution ess [ z p z ] proved next notion effective set size determines rate convergence empirical probability mass function true distribution distance measured total variation sense result allows us relate algorithmic stability property observation space z theorem let z countable space endowed probability mass function p z let sm set samples zi p z define psm z empirical probability mass function q induced drawing samples uniformly random sm esm p z psm z ess [ z p z ] ess [ z p z ] z effective size z see defm inition q addition learning algorithm l z h h ztrn p z ] ess [ z bound achieved lazy learners see definition special case theorem proved de moivre showed pempirical mean bernoulli trials probability success converges true mean rate proof outline proof first know p sm p p multinomial coefficient using relation p q p q multinomial series de moivres formula mean deviation binomial random variable [ ] shown algebraic manipulations x k esm p z psm z pk pk pmp k pk pk k using stirlings approximation factorial [ ] obtain simple asymptotic expression r r x pk pk ess [ z p z ] esm p z psm z k tight due tightness stirling approximation rest theorem follows markov chain sm sm h data processing inequality definition corollary given conditions theorem q z addition finite e z learning algorithm l l z proof finite observation space z maximum effective set size see definition z attained uniform distribution p z z intuitively speaking theorem corollary state order guarantee good uniform generalization possible learning algorithms number observations must sufficiently large cover entire effective size observation space z needless mention difficult achieve practice algorithmic stability machine learning algorithms must controlled order guarantee good generalization empirical observations similarly uniform generalization bound improved reducing effective size observation space using dimensionality reduction methods algorithmic stability complexity hypothesis space finally look hypothesis space influences algorithmic stability first look role size hypothesis space formalized following theorem theorem denote h h hypothesis inferred learning algorithm l z h following bound algorithmic stability always holds r r h h log h l h shannon entropy measured nats e using natural logarithms proof proof informationtheoretic let x mutual information r v x let sm z z zm random choice training set hx h sm h h sm h sm h h zi h z h h z z h conditioning reduces entropy e h b h r v b sm h x [ h zi h zi h ] [ h ztrn h ztrn h ] therefore ztrn h sm h average believed first appearance squareroot law statistical inference literature [ ] effective set size bernoulli distribution according definition given theorem agrees fact generalizes de moivres result next use pinskers q inequality [ ] states probability distributions p p q q p q p q total variation distance p q kullbackleibler divergence measured nats e using natural logarithms recall sm h p sm p h p sm h mutual information sm h p sm h p sm p h deduce pinskers inequality eq ztrn h p ztrn p h p ztrn h r r r r ztrn h sm h h h log h last line used fact x h x random variables x theorem reestablishes classical pac result finite hypothesis space [ ] terms algorithmic stability learning algorithm enjoy high stability size hypothesis space small terms uniform generalization states generalizationp risk learning algorithm bounded uniformly across parametric loss functions h h p log h h h shannon entropy h next relate algorithmic stability vapnikchervonenkis vc dimension despite fact vc dimension defined binaryvalued functions whereas algorithmic stability functional probability distributions exists connection two concepts show first introduce notion induced concept class exists learning algorithm l definition concept class c induced learning algorithm l h defined z set total boolean functions c z p ztrn z h p ztrn z h h intuitively every hypothesis h h induces total partition observation space z given boolean function definition h splits z two disjoint sets set values z posteriori less likely present training set given inferred hypothesis h set values complexity richness induced concept class c related algorithmic stability via vc dimension theorem let l h learning algorithm induced concept class c let z dv c c vc dimension c following bound holds dv c c p dv c c log l particular l algorithmically stable induced concept class c finite vc dimension proof bounded n proof relies fact algorithmic stability l supp z esm suphh ezp z ch z ezsm ch z ch z p ztrn z h p ztrn z final bound follows applying uniform convergence results [ ] conclusions paper showed probabilistic notion algorithmic stability equivalent uniform generalization informal terms learning algorithm called algorithmically stable impact single training example probability distribution final hypothesis always vanishes limit large training sets words inference process never depends heavily single training example algorithmic stability holds learning algorithm generalizes well regardless choice parametric loss function also provided several interpretations result instance relationship algorithmic stability data processing reveals algorithmic stability improved either postprocessing inferred hypothesis augmenting training examples artificial noise prior learning addition established relationship algorithmic stability effective size observation space provided formal justification dimensionality reduction methods finally connected algorithmic stability complexity richness hypothesis space reestablished classical pac result complexity hypothesis space controlled order improve stability hence improve generalization references [ ] v n vapnik overview statistical learning theory neural networks ieee transactions vol september [ ] c cortes v vapnik supportvector networks machine learning vol pp [ ] blumer ehrenfeucht haussler k warmuth learnability vapnikchervonenkis dimension journal acm jacm vol pp [ ] talagrand majorizing measures generic chaining annals probability vol pp [ ] mcallester pacbayesian stochastic model selection machine learning vol pp [ ] bousquet elisseeff stability generalization journal machine learning research jmlr vol pp [ ] p l bartlett mendelson rademacher gaussian complexities risk bounds structural results journal machine learning research jmlr vol pp [ ] j audibert bousquet combining pacbayesian generic chaining bounds journal machine learning research jmlr vol pp [ ] h xu mannor robustness generalization machine learning vol pp [ ] elisseeff pontil et al leaveoneout error stability learning algorithms applications natoasi series learning theory practice science series sub series iii computer systems sciences [ ] kutin p niyogi almosteverywhere algorithmic stability generalization error proceedings eighteenth conference uncertainty artificial intelligence uai [ ] poggio r rifkin mukherjee p niyogi general conditions predictivity learning theory nature vol pp [ ] kearns ron algorithmic stability sanitycheck bounds leaveoneout crossvalidation neural computation vol pp [ ] shalevshwartz shamir n srebro k sridharan learnability stability uniform convergence journal machine learning research jmlr vol pp [ ] l devroye l gyorfi g lugosi probabilistic theory pattern recognition springer [ ] v vapnik chapelle bounds error expectation support vector machines neural computation vol pp [ ] h robbins remark stirlings formula american mathematical monthly pp [ ] cover j thomas elements information theory wiley sons [ ] downs k e gates masters exact simplification support vector solutions jmlr vol pp [ ] wager wang p liang dropout training adaptive regularization nips pp [ ] stigler history statistics measurement uncertainty harvard university press [ ] p diaconis zabell closed form summation classical distributions variations theme de moivre statlstlcal science vol pp [ ] shalevshwartz bendavid understanding machine learning theory algorithms cambridge university press')),\n",
       " ([(1, 0.06737505), (7, 0.93096423)],\n",
       "  (6035,\n",
       "   'adaptive lowcomplexity sequential inference dirichlet process mixture models theodoros tsiligkaridis keith w forsythe massachusetts institute technology lincoln laboratory lexington usa ttsilill mit edu forsythell mit edu abstract develop sequential lowcomplexity inference procedure dirichlet process mixtures gaussians online clustering parameter estimation number clusters unknown apriori present easily computable closed form parametric expression conditional likelihood hyperparameters recursively updated function streaming data assuming conjugate priors motivated largesample asymptotics propose novel adaptive lowcomplexity design dirichlet process concentration parameter show number classes grow logarithmic rate prove largesample limit conditional likelihood data predictive distribution become asymptotically gaussian demonstrate experiments synthetic real data sets approach superior online stateoftheart methods introduction dirichlet process mixture models dpmm widely used clustering data neal rasmussen traditional finite mixture models often suffer overfitting underfitting data due possible mismatch model complexity amount data thus model selection model averaging required find correct number clusters model appropriate complexity requires significant computation highdimensional data sets large samples bayesian nonparametric modeling alternative approaches parametric modeling example dpmms automatically infer number clusters data via bayesian inference techniques use markov chain monte carlo mcmc methods dirichlet process mixtures made inference tractable neal however methods exhibit slow convergence convergence tough detect alternatives include variational methods blei jordan deterministic algorithms convert inference optimization approaches take significant computational effort even moderate sized data sets largescale data sets lowlatency applications streaming data need inference algorithms much faster require multiple passes data work focus lowcomplexity algorithms adapt sample arrive making highly scalable online algorithm learning dpmms based sequential variational approximation sva proposed lin authors wang dunson recently proposed sequential maximum aposterior map estimator class labels given streaming data algorithm called sequential updating greedy search sugs iteration composed greedy selection step posterior update step choice concentration parameter critical dpmms controls number clusters antoniak fast dpmm algorithms use fixed fearnhead daume kurihara et al imposing prior distribution sampling provides flexibility approach still heavily relies experimentation prior knowledge thus many fast inference methods dirichlet process mixture models proposed adapt data including works escobar west learning incorporated gibbs sampling analysis blei jordan gamma prior used conjugate manner directly variational inference algorithm wang dunson also account model uncertainty concentration parameter bayesian manner directly sequential inference procedure approach computationally expensive discretization domain needed stability highly depends initial distribution range values best knowledge first analytically study evolution stability adapted sequence online learning setting paper propose adaptive nonbayesian approach adapting motivated largesample asymptotics call resulting algorithm asugs adaptive sugs basic idea behind asugs directly related greedy approach sugs main contribution novel lowcomplexity stable method choosing concentration parameter adaptively new data arrive greatly improves clustering performance derive upper bound number classes logarithmic number samples prove sequence concentration parameters results adaptive design almost bounded finally prove conditional likelihood primary tool used bayesianbased online clustering asymptotically gaussian largesample limit implying clustering part asugs asymptotically behaves gaussian classifier experiments show method outperforms stateoftheart methods online learning dpmms paper organized follows section review sequential inference framework dpmms build upon introduce notation propose adaptive modification section probabilistic data model given sequential inference steps shown section contains growth rate analysis number classes adaptivelydesigned concentration parameters section contains gaussian largesample approximation conditional likelihood experimental results shown section conclude section sequential inference framework dpmm review sugs framework wang dunson online clustering nonparametric nature dirichlet process manifests modeling mixture models countably infinite components let observations given yi rd denote class label ith observation latent variable define available information time yi online sequential updating greedy search sugs algorithm summarized next completeness set calculate choose best class label yi arg maxhki p h update posterior distribution f yi using yi h parameters class h f yi h observation density conditioned class h ki number classes created time algorithm sequentially allocates observations yi classes based maximizing conditional posterior probability calculate posterior probability p h define variables def def lih yi p yi h ih p h bayes rule p h lih yi ih h ki considered fixed iteration updated fully bayesian manner according dirichlet process prediction predictive probability assigning observation yi class h mi h h ki ih h ki algorithm adaptive sequential updating greedy search asugs input streaming data yi rate parameter set k calculate ki update concentration parameter log n l yi ih b choose best label yi qh p ih l yi h c update posterior distribution end ih ih f yi pi mi h l l h counts number observations labeled class h time concentration parameter adaptation concentration parameter well known concentration parameter strong influence growth number classes antoniak experiments show sequential framework choice even critical choosing fixed online sva algorithm lin requires crossvalidation computationally prohibitive largescale data sets furthermore streaming data setting estimate data complexity exists impractical perform crossvalidation although parameter handled fully bayesian treatment wang dunson prespecified grid possible values take say l l l along prior distribution needs chosen advance storage updating matrix size ki l marginalization needed compute p h iteration thus propose alternative datadriven method choosing works well practice simple compute theoretical guarantees idea start prior distribution favors small shape posterior distribution using data define pi p posterior distribution formed time used asugs time let p p denote prior e g exponential distribution p e dependence trivial first step bayes rule pi p yi p pi ii ii given update made selection used next selection step mean distribution pi e e[ ] shown section distribution pi approximated gamma distribution shape parameter ki rate parameter log approximation ki log requiring storage update one scalar parameter ki iteration asugs algorithm summarized algorithm selection step may implemented sampling probability mass function qh posterior update step efficiently performed updating hyperparameters function streaming data case conjugate distributions section derives updates case multivariate gaussian observations conjugate priors parameters sequential inference unknown mean unknown covariance consider general case unknown mean covariance class probabilistic model parameters class given yi n n co w v n denotes multivariate normal distribution mean precision matrix w v wishart distribution degrees freedom scale matrix v follow normalwishart joint distribution model leads parameters rd closedform expressions lih yi due conjugacy tzikas et al calculate class posteriors conditional likelihoods yi given assignment class h previous class assignments need calculated first conditional likelihood yi given assignment class h history given z lih yi f yi h h dh due conjugacy distributions posterior h always form h n h h ch th w th h vh h ch h vh hyperparameters recursively computed new samples come form recursive computation hyperparameters derived appendix ease interpretation numerical stability define h w h vh vh h h inverse mean wishart distribution matrix natural interpretation covariance matrix class h iteration th component chosen parameter updates th class become ci ci ci c ci ci yi ci yi starting matrix h positive definite matrices h remain positive definite let us return calculation conditional likelihood iterated integration follows rh h det h lih yi h h rh h yi h yi h h def def rh ch ch detailed mathematical derivation conditional likelihood included appendix b remark new class h ki liki form initial choice hyperparameters r growth rate analysis number classes stability section derive model posterior distribution pn using largesample approximations allow us derive growth rates number classes sequence concentration parameters showing number classes grows e[ kn ] log n arbitarily small certain mild conditions probability density parameter updated jth step following fashion innovation class chosen j pj pj otherwise j dependent factors update shown independent factors absorbed normalization probability density choosing innovation class pushes mass toward infinity choosing class pushes mass toward zero thus possibility innovation probability grows undesired manner assess growth number def innovations rn kn simple assumptions likelihood functions appear naturally asugs algorithm assuming initial distribution p e distribution used step n qn proportional rn j j e make use limiting relation theorem following asymptotic behavior holds limn log qn j j log n proof see appendix c using theorem largesample model pn rn e log n suitably normalized recognizing gamma distribution shape parameter rn rate parameter log n rn mean given n log n use mean form choose class membership alg asymptotic approximation leads simple scalar update concentration parameter need discretization tracking evolution continuous probability distributions experiments approximation accurate recall innovation class labeled k kn nth step modeled updates randomly select previous class innovation new class sampling probability distrip k n bution qk p n k n n k note n kk mn k mn k represents number members class k time n assume data follows gaussian mixture distribution def pt k x h n h h h h prior probabilities h h parameters gaussian clusters define mixturemodel probability density function plays role predictive distribution x mn k def lnk lnk n kk probabilities choosing previous class innovation using equ proporp mn k n n tional kk n lnk yn n lnk yn n lnk yn respecn n n tively n denotes innovation probability step n n lnk yn n lnk yn n n n n n n n n positive proportionality factor n define likelihood ratio lr beginning stage n def ln lnk lnk conceptually mixture represents modeled distribution fitting currently observed data modes data observed reasonable expect lnk good model future observations lr ln yn large future observations wellmodeled fact expect lnk pt n discussed section ln yn n ln yn n lemma following bound holds n nl min n n yn n proof result follows directly simple calculation innovation random variable rn described random process associated probabilities transition n k rn p rn k rn n k rn def l lnk independent n depends initial choice hyperparameters discussed sec expectation rn majorized expectation similar random process rn based def transition probability n min rna instead n appendix shows random n sequence given ln yn n log n latter described modification polya urn process selection probability n asymptotic behavior rn related variables described following theorem theorem let n sequence realvalued random variables n satisfying n rna n n n ln yn n log n nonnegative integervalued random variables rn evolve according assume following n n ln yn pt k lnk p k q kullbackleibler divergence distributions p q n rn op log n n op log n proof see appendix e theorem bounds growth rate mean number class innovations concentration parameter n terms sample size n parameter bounded lr bounded kl divergence conditions thm manifest rate exponents experiments section shows conditions thm hold iterations n n n n fact assuming correct clustering mixture distribution lnkn converges true mixture distribution pt implying number class innovations grows log n sequence concentration parameters log n arbitrarily small asymptotic normality conditional likelihood section derive asymptotic expression conditional likelihood order gain insight steadystate algorithm let h denote true prior probability class h using bounds gamma function theorem batir follows lima ed normal convergence conditions algorithm pruning merging steps included classes h k correctly identified populated approximately ni h h observations time thus conditional class prior class h converges h ni h h virtue ih h according rh ch op log expect since h also expect h h according also h ed h ed h parameter updates imply h h h h follows strong law large numbers updates recursive implementations sample mean sample covariance matrix thus largesample approximation conditional likelihood becomes h h lim h h h lih yi limi det h e yi h h yi h det h used limu uc u ec conditional likelihood corresponds multivariate gaussian distribution mean h covariance matrix h similar asymptotic normality result recently obtained tsiligkaridis forsythe gaussian observations von n n h mises prior asymptotics mn h h h h h lnh n h h n n imply mixture distribution lnk converges true gaussian mixture distribution pt thus small expect pt k lnk n n validating assumption theorem experiments apply asugs learning algorithm synthetic class example real data set verify stability accuracy method experiments show value adaptation dirichlet concentration parameter online clustering parameter estimation since possible multiple clusters similar classes might created due outliers due particular ordering streaming data sequence add pruning merging step asugs algorithm done lin compare asugs asugspm sugs sugspm sva svapm proposed lin since shown lin sva svapm outperform blockbased methods perform iterative updates entire data set including collapsed gibbs sampling mcmc splitmerge truncationfree variational inference synthetic data set consider learning parameters class gaussian mixture equal variance training set made iid samples test set made iid samples clustering results shown fig showing asugsbased approaches stable svabased algorithms asugspm performs best identifies correct number clusters parameters fig b shows data loglikelihood test set averaged monte carlo trials mean variance number classes iteration asugsbased approaches achieve higher loglikelihood svabased approaches asymptotically fig provides numerical verification assumptions theorem expected predictive likelihood lik converges true mixture distribution pt likelihood ratio li yi bounded enough samples processed svapm asugs asugspm mean number classes avg joint loglikelihood asugs asugspm sugs sugspm sva svapm iteration iteration variance number classes sva iteration b figure clustering performance sva svapm asugs asugspm synthetic data set asugspm identifies clusters correctly b joint loglikelihood synthetic data mean variance number classes function iteration likelihood values evaluated heldout set samples asugspm achieves highest loglikelihood lowest asymptotic variance number classes real data set applied online nonparametric bayesian methods clustering image data used mnist data set consists training samples test samples sample ik pt k kl li yi sample l sample figure likelihood ratio li yi lik yi left l distance lik true ik mixture distribution pt right synthetic example see image handwritten digit total dimensions perform pca preprocessing reduce dimensionality dimensions kurihara et al use random subset consisting random samples training training set contains data digits approximately uniform proportion fig shows predictive loglikelihood test set mean images clusters obtained using asugspm svapm respectively note asugspm achieves higher loglikelihood values finds digits correctly using clusters svapm finds digits using clusters predictive loglikelihood asugspm sugspm svapm iteration b c figure predictive loglikelihood test set mean images clusters found using asugspm b svapm c mnist data set discussion although sva asugs methods similar computational complexity use decisions information obtained processing previous samples order decide class innovations mechanics methods quite different asugs uses adaptive motivated asymptotic theory sva uses fixed furthermore sva updates parameters components iteration weighted fashion asugs updates parameters mostlikely cluster thus minimizing leakage unrelated components parameter asugs affect performance much threshold parameter sva often leads instability requiring lots pruning merging steps increasing latency critical large data sets streaming applications crossvalidation would required set appropriately observe higher loglikelihoods better numerical stability asugsbased methods comparison sva mathematical formulation asugs allows theoretical guarantees theorem asymptotically normal predictive distribution conclusion developed fast online clustering parameter estimation algorithm dirichlet process mixtures gaussians capable learning single data pass motivated largesample asymptotics proposed novel lowcomplexity datadriven adaptive design concentration parameter showed leads logarithmic growth rates number classes experiments synthetic real data sets show method achieves better performance fast stateoftheart online learning dpmm methods references antoniak c e mixtures dirichlet processes applications bayesian nonparametric problems annals statistics batir n inequalities gamma function archiv der mathematik blei jordan variational inference dirichlet process mixtures bayesian analysis daume h fast search dirichlet process mixture models conference artificial intelligence statistics escobar west bayesian density estimation inference using mixtures journal american statistical association june fearnhead p particle filters mixture models uknown number components statistics computing kurihara k welling vlassis n accelerated variational dirichlet mixture models advances neural information processing systems nips lin dahua online learning nonparametric mixture models via sequential variational approximation burges c j c bottou l welling ghahramani z weinberger k q eds advances neural information processing systems pp curran associates inc neal r bayesian mixture modeling proceedings workshop maximum entropy bayesian methods statistical analysis volume pp neal r markov chain sampling methods dirichlet process mixture models journal computational graphical statistics june rasmussen c e infinite gaussian mixture model advances neural information processing systems pp mit press tsiligkaridis forsythe k w sequential bayesian inference framework blind frequency offset estimation proceedings ieee international workshop machine learning signal processing boston september tzikas g likas c galatsanos n p variational approximation bayesian inference ieee signal processing magazine pp november wang l dunson b fast bayesian inference dirichlet process mixture models journal computational graphical statistics')),\n",
       " ([(7, 0.9964748)],\n",
       "  (5978,\n",
       "   'covariancecontrolled adaptive langevin thermostat largescale bayesian sampling xiaocheng shang university edinburgh x shanged ac uk zhanxing zhu university edinburgh zhanxing zhued ac uk benedict leimkuhler university edinburgh b leimkuhlered ac uk amos j storkey university edinburgh storkeyed ac uk abstract monte carlo sampling bayesian posterior inference common approach used machine learning markov chain monte carlo procedures used often discretetime analogues associated stochastic differential equations sdes sdes guaranteed leave invariant required posterior distribution area current research addresses computational benefits stochastic gradient methods setting existing techniques rely estimating variance covariance subsampling error typically assume constant variance article propose covariancecontrolled adaptive langevin thermostat effectively dissipate parameterdependent noise maintaining desired target distribution proposed method achieves substantial speedup popular alternative schemes largescale machine learning applications introduction machine learning applications direct sampling entire largescale dataset computationally infeasible instance standard markov chain monte carlo mcmc methods [ ] well typical hybrid monte carlo hmc methods [ ] require calculation acceptance probability creation informed proposals based whole dataset order improve computational efficiency number stochastic gradient methods [ ] proposed setting bayesian sampling based random much smaller subsets approximate likelihood whole dataset thus substantially reducing computational cost practice welling teh proposed socalled stochastic gradient langevin dynamics sgld [ ] combining ideas stochastic optimization [ ] traditional brownian dynamics sequence stepsizes decreasing zero fixed stepsize often adopted practice choice article vollmer et al [ ] modified sgld msgld also introduced designed reduce sampling bias sgld generates samples first order brownian dynamics thus fixed timestep one show unable dissipate excess noise gradient approximations maintaining desired invariant distribution [ ] stochastic gradient hamiltonian monte carlo sghmc method proposed chen et al [ ] relies second order langevin dynamics incorporates parameterdependent diffusion matrix intended effectively offset stochastic perturbation gradient however difficult accommodate additional diffusion term practice first second authors contributed equally listed author order decided lot moreover pointed [ ] poor estimation may significant adverse influence sampling target distribution example effective system temperature may altered thermostat idea widely used molecular dynamics [ ] recently adopted stochastic gradient nosehoover thermostat sgnht ding et al [ ] order adjust kinetic energy simulation way canonical ensemble preserved e prescribed constant temperature distribution maintained fact sgnht method essentially equivalent adaptive langevin adlangevin thermostat proposed earlier jones leimkuhler [ ] molecular dynamics setting see [ ] discussions despite substantial interest generated methods mathematical foundation stochastic gradient methods incomplete underlying dynamics sgnht method [ ] taken leimkuhler shang [ ] together design discretization schemes high effective order accuracy sgnht methods designed based assumption constant noise variance article propose covariancecontrolled adaptive langevin ccadl thermostat handle parameterdependent noise improving robustness reliability practice effectively speed convergence desired invariant distribution largescale machine learning applications rest article organized follows section describe setting bayesian sampling noisy gradients briefly review existing techniques section considers construction novel ccadl method effectively dissipate parameterdependent noise maintaining correct distribution various numerical experiments performed section verify usefulness ccadl wide range largescale machine learning applications finally summarize findings section bayesian sampling noisy gradients typical setting bayesian sampling [ ] one interested drawing states posterior distribution defined x x rnd parameter vector interest x denotes entire dataset x likelihood prior distributions respectively introduce potential energy function u defining x exp u positive parameter interpreted proportional reciprocal temperature associated physical system e kb kb boltzmann constant temperature practice often set unity notational simplicity taking logarithm yields u log x log assuming data independent identically distributed logarithm likelihood calculated n x log x log xi n size entire dataset however already mentioned computationally infeasible deal entire largescale dataset timestep would typically required mcmc hmc methods instead order improve efficiency random much smaller e n n subset preferred stochastic gradient methods likelihood dataset given parameters approximated n nx log x log xri n xri ni represents random subset x thus noisy potential energy written n nx u log xri log n negative gradient potential referred noisy force e f u goal correctly sample gibbs distribution exp u [ ] gradient noise assumed gaussian mean zero unknown variance case one may rewrite noisy force p f u r typically diagonal matrix represents covariance matrix noise p r vector standard normal random variables note r actually equivalent n typical setting numerical integration withassociated stepsize hone p p hf h u r hu h h r therefore assuming constant covariance matrix e identity matrix sgnht method ding et al [ ] following underlying dynamics written standard ito stochastic differential equation sde system [ ] pdt p dp u dt hm dwpdt dwa p pnd kb dt colloquially dw dwa represent vectors independent wiener increments p often informally denoted n dti [ ] coefficient represents strength artificial noise added system improve ergodicity termed effective friction positive parameter proportional variance noise auxiliary variable r governed nosehoover device [ ] via negative feedback mechanism e instantaneous temperature average kinetic energy per degree freedom calculated kb pt pnd target temperature dynamical friction would decrease allowing increase temperature would increase temperature target coupling parameter referred thermal mass molecular dynamics setting proposition see jones leimkuhler [ ] sgnht method preserves modified gibbs stationary distribution p z exp h p exp z normalizing constant h p pt pu hamiltonian ah proposition tells us sgnht method adaptively dissipate excess noise pumped system maintaining correct distribution variance gradient noise need known priori long constant auxiliary variable able automatically find mean value fly however parameterdependent covariance matrix sgnht method would produce required target distribution ding et al [ ] claimed reasonable assume covariance matrix constant size dataset n large case variance posterior small magnitude posterior variance actually relate constancy however general constant simply assuming nonconstancy significant impact performance method notably stability measured largest usable stepsize therefore essential approach handle parameterdependent noise following section propose covariancecontrolled thermostat effectively dissipate parameterdependent noise maintaining target stationary distribution covariancecontrolled adaptive langevin thermostat mentioned previous section sgnht method dissipate noise constant covariance matrix covariance matrix becomes parameterdependent general parameterdependent covariance matrix imply required thermal equilibrium e system cannot expected converge desired invariant distribution typically resulting poor estimation functions parameters interest fact case clear whether exists invariant distribution order construct stochasticdynamical system preserves canonical distribution suggest adding suitable damping viscous term effectively dissipate parameterdependent gradient noise end propose following covariancecontrolled adaptive langevin ccadl thermostat pdt p p dp u dt h dw h pdtpdt dwa pt pnd kb dt proposition ccadl thermostat preserves modified gibbs stationary distribution p z exp h p exp proof fokkerplanck equation corresponding l p u p h p mp h p p p p p mp pt pnd kb insert fokkerplanck operator l see vanishes incorporation parameterdependent covariance matrix intended offset covariance matrix coming gradient approximation however practice one know priori thus instead one must estimate simulation task addressed section procedure related method used sghmc method proposed chen et al [ ] uses dynamics following form pdt p p dp u dt h dwapdt aih dwa shown sghmc method preserves gibbs canonical distribution p z exp h p although ccadl sghmc preserve respective invariant distributions let us note several advantages former latter practice ccadl sghmc require estimation covariance matrix simulation costly high dimension numerical experiments found simply using diagonal covariance matrix significantly reduced computational cost works quite well ccadl contrast difficult find suitable value parameter sghmc since one make sure matrix aih positive semidefinite one may attempt use large value effective friction andor small stepsize h however toolarge friction would essentially reduce sghmc sgld desirable pointed [ ] extremely small stepsize would significantly impact computational efficiency ii estimation covariance matrix unavoidably introduces additional noise ccadl sghmc nonetheless ccadl still effectively control system temperature e maintaining correct distribution momenta due use stabilizing nosehoover control sghmc poor estimation covariance matrix may lead significant deviations system temperature well distribution momenta resulting poor sampling parameters interest covariance estimation noisy gradients assumption noise stochastic gradient follows normal distribution apply similar method [ ] estimate covariance matrix associated noisy gradient let g x log x assume size subset n large enough central limit theorem hold n x g xri n ex [ g x ] n n cov[ g x ] covariance gradient given noisy stochastic pn gradient based current subset u n g xri log clean n algorithm covariancecontrolled adaptive langevin ccadl thermostat input h tt initialize p pt h estimate using eq pt pt u h h nn pt ht pt h ahn ptt pt nd h end full gradient u thus pn g xi log ex [ u ] ex [ u ] n u u n n e n n assuming change dramatically time use moving average update estimate v n x v g xri g g xri g n empirical covariance gradient g represents mean gradient log likelihood computed subset proved [ ] estimator convergence order n already mentioned estimating full covariance matrix computationally infeasible high dimension however found employing diagonal approximation covariance matrix e estimating variance along dimension noisy gradient works quite well practice demonstrated section procedure ccadl method summarized algorithm simply used nd order consistent original implementation sgnht [ ] note simple first order terms stepsize algorithm recent article [ ] introduced higher order accuracy schemes improve accuracy interest direct comparison underlying machinery sghmc sgnht ccadl avoid modifications enhancements related timestepping stage following section compare newly established ccadl method sghmc sgnht various machine learning tasks demonstrate benefits ccadl bayesian sampling noisy gradient numerical experiments bayesian inference gaussian distribution first compare performance newly established ccadl method sghmc sgnht simple task using synthetic data e bayesian inference mean variance onedimensional normal distribution apply experimental setting [ ] generated n samples standard normal distribution n used likelihood function n xi assigned normalgamma distribution prior distribution e n gam corresponding posterior distribution another normalgamma distribution e x n n n gam n n n x n xi x n x n x n n n n n n n pn x xi n random subset size n selected timestep approximate full gradient resulting following stochastic gradients n n n x n n x u n x ri u xr n n seen variance stochastic gradient noise longer constant actually depends size subset n values iteration directly violates constant noise variance assumption sgnht [ ] ccadl adjusts varying noise variance marginal distributions obtained various methods different combinations h compared plotted figure table consisting corresponding root mean square error rmse distribution autocorrelation time samples cases sgnht ccadl easily outperform sghmc method possibly due presence nosehoover device sghmc showing superiority small value h large value neither desirable practice discussed section sgnht newly proposed ccadl method latter achieves better performance cases investigated highlighting importance covariance control parameterdependent noise true sghmc sgnht ccadl h b h true sghmc sgnht ccadl true sghmc sgnht ccadl density true sghmc sgnht ccadl density density true sghmc sgnht ccadl density true sghmc sgnht ccadl true sghmc sgnht ccadl density density density true sghmc sgnht ccadl density c h h figure comparisons marginal distribution density top row bottom row various values h indicated column peak region highlighted inset table comparisons rmse autocorrelation time various methods bayesian inference mean variance gaussian distribution methods h h h h sghmc sgnht ccadl largescale bayesian logistic regression consider bayesian logistic regression model trained benchmark mnist dataset binary classification digits using training data points test set size dimensional random projection original features used used likeliqn hood function xi yi n w exp yi w xi prior distribution w exp w w subset size n used timestep since dimensionality problem high full covariance estimation used ccadl investigate figure top row convergence speed method measuring test log likelihood using posterior mean number passes entire dataset ccadl displays significant improvements sghmc sgnht different values h ccadl converges much faster two also indicates faster mixing speed shorter burnin period ccadl shows robustness different values effective friction sghmc sgnht relying relative large value especially sghmc method intended dominate gradient noise compare sample quality obtained method figure bottom row plots twodimensional marginal posterior distribution randomly selected dimensions based samples method burnin period e start collect samples test log likelihood stabilizes true reference distribution obtained sufficiently long run standard hmc implemented runs standard hmc found variation runs guarantees qualification true reference distribution ccadl shows much better performance sghmc sgnht note contour sghmc even fit region plot fact shows significant deviation even estimation mean sghmc sghmc sgnht sgnht ccadl ccadl number passes true hmc sghmc sgnht ccadl w h number passes x true hmc sghmc sgnht ccadl w sghmc sghmc sgnht sgnht ccadl ccadl w w x true hmc sghmc sgnht ccadl number passes x sghmc sghmc sgnht sgnht ccadl ccadl w test log likelihood test log likelihood test log likelihood b h w c h figure comparisons bayesian logistic regression various methods mnist dataset digits various values h top row test log likelihood using posterior mean number passes entire dataset bottom row twodimensional marginal posterior distribution randomly selected dimensions fixed based samples method burnin period e start collect samples test log likelihood stabilizes magenta circle true reference posterior mean obtained standard hmc crosses represent sample means computed various methods ellipses represent isoprobability contours covering probability mass note contour sghmc well beyond scale plot especially large stepsize regime case include discriminative restricted boltzmann machine drbm drbm [ ] selfcontained nonlinear classifier gradient discriminative objective explicitly computed due limited space refer readers [ ] details trained drbm different largescale multiclass datasets libsvm dataset collection including connect letter sensit vehicle acoustic detailed information datasets presented table selected number hidden units using crossvalidation achieve best results since dimension parameters nd relatively high used diagonal covariance matrix estimation ccadl significantly reduce computational cost e estimating variance along dimension size subset chosen obtain reasonable variance estimation dataset chose first total number passes entire dataset burnin period collected remaining samples prediction table datasets used drbm corresponding parameter configurations datasets connect letter acoustic trainingtest set classes features hidden units total number parameters nd error rates computed various methods test set using posterior mean number passes entire dataset plotted figure observed sghmc sgnht work well large value effective friction corresponds strong random walk effect thus slows convergence contrary ccadl works httpwww csie ntu edu tw cjlinlibsvmtoolsdatasetsmulticlass html reliably much better two wide range importantly large stepsize regime speeds convergence rate relation computational work performed easily seen performance sghmc heavily relies using small value h large value significantly limits usefulness practice number passes test error sghmc sghmc sgnht sgnht ccadl ccadl number passes number passes acoustic h number passes sghmc sghmc sgnht sgnht ccadl ccadl number passes number passes b acoustic h number passes c letter h sghmc sghmc sgnht sgnht ccadl ccadl c connect h b letter h sghmc sghmc sgnht sgnht ccadl ccadl test error sghmc sghmc sgnht sgnht ccadl ccadl letter h number passes sghmc sghmc sgnht sgnht ccadl ccadl b connect h test error connect h test error test error sghmc sghmc sgnht sgnht ccadl ccadl test error test error test error sghmc sghmc sgnht sgnht ccadl ccadl test error sghmc sghmc sgnht sgnht ccadl ccadl number passes c acoustic h figure comparisons drbm datasets connect top row letter middle row acoustic bottom row various values h indicated test error rates various methods using posterior mean number passes entire dataset conclusions future work article proposed novel ccadl formulation effectively dissipate parameterdependent noise maintaining desired invariant distribution ccadl combines ideas sghmc sgnht literature achieves significant improvements methods practice additional error introduced covariance estimation expected small relative sense e substantially smaller error arising noisy gradient findings verified largescale machine learning applications particular consistently observed sghmc relies small stepsize h large friction significantly reduces usefulness practice discussed techniques presented article could use general settings largescale bayesian sampling optimization leave future work naive nonsymmetric splitting method applied ccadl fair comparison article however point optimal design splitting methods ergodic sde systems explored recently mathematics community [ ] moreover shown [ ] certain type symmetric splitting method adlangevinsgnht method clean full gradient inherits superconvergence property e fourth order convergence invariant distribution configurational quantities recently demonstrated setting langevin dynamics [ ] leave exploration direction context noisy gradients future work references [ ] abdulle g vilmart k c zygalakis long time accuracy lietrotter splitting methods langevin dynamics siam journal numerical analysis [ ] ahn korattikara welling bayesian posterior sampling via stochastic gradient fisher scoring proceedings th international conference machine learning pages [ ] brooks gelman g jones x l meng handbook markov chain monte carlo crc press [ ] chen e b fox c guestrin stochastic gradient hamiltonian monte carlo proceedings st international conference machine learning pages [ ] n ding fang r babbush c chen r skeel h neven bayesian sampling using stochastic gradient thermostats advances neural information processing systems pages [ ] duane kennedy b j pendleton roweth hybrid monte carlo physics letters b [ ] frenkel b smit understanding molecular simulation algorithms applications second edition academic press [ ] w g hoover computational statistical mechanics studies modern thermodynamics elsevier science [ ] horowitz generalized guided monte carlo algorithm physics letters b [ ] jones b leimkuhler adaptive stochastic methods sampling driven molecular systems journal chemical physics [ ] h larochelle bengio classification using discriminative restricted boltzmann machines proceedings th international conference machine learning pages [ ] b leimkuhler c matthews rational construction stochastic numerical methods molecular sampling applied mathematics research express [ ] b leimkuhler c matthews molecular dynamics deterministic stochastic numerical methods springer [ ] b leimkuhler c matthews g stoltz computation averages equilibrium nonequilibrium langevin molecular dynamics ima journal numerical analysis [ ] b leimkuhler x shang adaptive thermostats noisy gradient systems siam journal scientific computing [ ] n metropolis w rosenbluth n rosenbluth h teller e teller equation state calculations fast computing machines journal chemical physics [ ] nose unified formulation constant temperature molecular dynamics methods journal chemical physics [ ] h robbins monro stochastic approximation method annals mathematical statistics [ ] c robert g casella monte carlo statistical methods second edition springer [ ] j vollmer k c zygalakis w teh non asymptotic properties stochastic gradient langevin dynamics arxiv preprint arxiv [ ] welling w teh bayesian learning via stochastic gradient langevin dynamics proceedings th international conference machine learning pages')),\n",
       " ([(1, 0.33261463), (4, 0.02015491), (6, 0.1877847), (7, 0.45749432)],\n",
       "  (5714,\n",
       "   'robust portfolio optimization fang han department biostatistics johns hopkins university baltimore md fhanjhu edu huitong qiu department biostatistics johns hopkins university baltimore md hqiujhu edu han liu department operations research financial engineering princeton university princeton nj hanliuprinceton edu brian caffo department biostatistics johns hopkins university baltimore md bcaffojhsph edu abstract propose robust portfolio optimization approach based quantile statistics proposed method robust extreme events asset returns accommodates large portfolios limited historical data specifically show risk estimated portfolio converges oracle optimal risk parametric rate weakly dependent asset returns theory rely higher order moment assumptions thus allowing heavytailed asset returns moreover rate convergence quantifies size portfolio management allowed scale exponentially sample size historical data empirical effectiveness proposed method demonstrated synthetic real stock data work extends existing ones achieving robustness high dimensions allowing serial dependence introduction markowitzs meanvariance analysis sets basis modern portfolio optimization theory [ ] however meanvariance analysis criticized sensitive estimation errors mean covariance matrix asset returns [ ] compared covariance matrix mean asset returns influential harder estimate [ ] therefore many studies focus global minimum variance gmv formulation involves estimating covariance matrix asset returns estimating covariance matrix asset returns challenging due high dimensionality heavytailedness asset return data specifically number assets management usually much larger sample size exploitable historical data hand extreme events typical financial asset prices leading heavytailed asset returns overcome curse dimensionality structured covariance matrix estimators proposed asset return data [ ] considered estimators based factor models observable factors [ ] studied covariance matrix estimators based latent factor models [ ] proposed shrink sample covariance matrix towards highly structured covariance matrices including identity matrix order autoregressive covariance matrices onefactorbased covariance matrix estimators estimators commonly based sample covariance matrix sub gaussian tail assumptions required guarantee consistency heavytailed data robust estimators covariance matrices desired classic robust covariance matrix estimators include estimators minimum volume ellipsoid mve minimum covari ance determinant mcd estimators sestimators estimators based data outlyingness depth [ ] estimators specifically designed data low dimensions large sample sizes generalizing robust estimators high dimensions [ ] proposed orthogonalized gnanadesikankettenring ogk estimator extends [ ] estimator reestimating eigenvalues [ ] studied shrinkage estimators based tylers estimator however although ogk computationally tractable high dimensions consistency guaranteed fixed dimension shrunken tylors estimator involves iteratively inverting large matrices moreover consistency guaranteed dimension order sample size aforementioned robust estimators analyzed independent data points performance time series data questionable paper build quantilebased scatter matrix estimator propose robust portfolio optimization approach contributions three aspects first show proposed method accommodates high dimensional data allowing dimension scale exponentially sample size secondly verify consistency proposed method achieved without tail conditions thus allowing heavytailed asset return data thirdly consider weakly dependent time series demonstrate degree dependence affects consistency proposed method background section introduce notation system provide review grossexposure constrained portfolio optimization exploited paper notation let v v vd ddimensional real vector [ mjk ] rd matrix mjk j k entry q define ` q vector norm v pd kvkq j vj q ` vector norm v kvk maxdj vj let matrix qp ` max norm kmkmax maxjk mjk frobenius norm kmkf jk mjk let x x xd yd two random vectors write x x identically distributed use denote vectors every entry grossexposure constrained gmv formulation gmv formulation [ ] found imposing noshortsale constraint improves portfolio efficiency [ ] relaxed noshortsale constraint grossexposure constraint showed portfolio efficiency improved let x rd random vector asset returns portfolio characterized vector investment allocations w w wd among assets grossexposure constrained gmv portfolio optimization formulated min wt w w kwk c w covariance matrix x w budget constraint kwk c grossexposure constraint c called gross exposure constant controls percentage long short positions allowed portfolio [ ] optimization problem converted quadratic programming problem solved standard software [ ] method section introduce quantilebased portfolio optimization approach let z r random variable distribution function f zt tt sequence observations z constant q [ ] define qquantiles z zt tt q z q q f q inf z p z z q n b q z k k min q q z scatter matrix defined matrix proportional covariance matrix constant z z order statistics zt tt say q z q unique b q unique exists unique exists unique z p z z q say q z k z z zt z z following estimator qn [ ] define population sample quantilebased scales e b q z q z z bq zt tt q z zt stt q q ze independent copy z based b define robust scatter matrices asset returns detail let x x xd rd random vector representing returns assets xt tt sequence observations x xt xt xtd define population sample quantilebased scatter matrices qne bq bq rq [ rq jk ] r [ rjk ] b q given entries rq r q q b q bq xtj tt rjj xj r jj h q q rq x x x x j k j k jk h q q b q r b x x x x tj tk tj tk jk b q since bq computed using log time [ ] computational complexity r q b log since practice r computed almost efficiently sample covariance matrix complexity let w w wd vector investment allocations among assets matrix define risk function r rd rdd r r w wt mw x covariance matrix r w var wt x variance portfolio return wt x employed objected function gmv formulation however estimating difficult due heavy tails asset returns paper adopt r w rq robust alternative momentbased risk metric r w consider following oracle portfolio optimization problem wopt argmin r w rq w kwk c w kwk c grossexposure constraint introduced section practice rq b q onto cone unknown estimated convexity risk function project r positive definite matrices q b r e q argminr r r max r rdd mt min id max id e q optimization min max set lower upper bounds eigenvalues r problem solved projection contraction algorithm [ ] summarize e q formulate empirical robust portfolio algorithm supplementary material using r optimization e q w kwk c e opt argmin r w r w w remark robust portfolio optimization approach involves three parameters min max c empirically setting min max proves work well c typically provided investors controlling percentages short positions datadriven choice desired refer [ ] crossvalidationbased approach remark rationale behind positive definite projection lies two aspects first order portfolio optimization convex well conditioned positive definite matrix lower bounded eigenvalues needed guaranteed setting min secondly projection robust compared ogk estimate [ ] ogk induces positive definiteness reestimating eigenvalues using variances principal components robustness lost data possibly containing outliers projected onto principal directions estimating principal components remark adopt quantile definitions q bq achieve breakdown point however note methodology theory carries replaced absolute constant q theoretical properties section provide theoretical analysis proposed portfolio optimization approach b opt based estimate r rq next lemma shows error optimized portfolio w opt b rq r wopt rq essentially related estimation error r risks r w opt b lemma let w solution min r w r w kwk c w arbitrary matrix r b opt rq r wopt rq c kr rq kmax r w opt w solution oracle portfolio optimization problem c grossexposure constant e opt rq relates rate convergence next derive rate convergence r w q q e r kmax end first introduce dependence condition asset return series kr xt definition let xt tz stationary process denote f fn xt n fileds generated xt xt tn respectively mixing coefficient defined n sup p b p p b bf afn process xt tz mixing limn n condition xt rd tz stationary process j k xtj tz xtj xtk tz xtj xtk tz mixing processes satisfying n n n constant parameter determines rate decay n characterizes degree dependence xt tz next introduce identifiability condition distribution function asset returns f x e x ed independent copy x j k condition let x ej xj xk x ej x ek let fj fjk fjk distribution functions xj x e e xj xk xj xk assume exist constants inf f yq f dy f fj fjk fjk j k condition guarantees identifiability quantiles standard literature quantile statistics [ ] based conditions present rates convergence b q r e q r theorem let xt tz absolutely continuous stationary process satisfying conditions suppose log dt large enough probability smaller b q rq kmax rt kr rate convergence rt defined r n h c log log c rt max r q h c log log c io max q q q q p max max xj q xj xk xj xk j k c moreover r defined k k e q rq kmax rt kr implications theorem follows q p parameters max scale rate convergence reduces op log dt thus number assets management allowed scale exponentially sample size compared similar rates convergence obtained samplecovariancebased estimators [ ] require moment tail conditions thus accommodating heavytailed asset return data effect serial dependence p rate convergence characterized c specif ically approaches c k k increases towards infinity inflating rt allowed scale c log rate convergence rt inversely related lower bound marginal density functions around quantiles small distribution functions flat around quantiles making population quantiles harder estimate e opt rq combining lemma theorem obtain rate convergence r w theorem let xt tz absolutely continuous stationary process satisfying conditions suppose log dt rq large enough e opt rq r wopt rq c rt r w rt defined c grossexposure constant theorem shows risk estimated portfolio converges oracle optimal risk parametric rate rt number assets allowed scale exponentially sample size moreover rate convergence rely tail conditions distribution asset returns rest section build connection proposed robust portfolio optimization momentbased counterpart specifically show consistent elliptical model definition [ ] random vector x rd follows elliptical distribution location rd scatter rdd exist nonnegative random variable r matrix rdr rank r random vector u rr independent uniformly distributed rdimensional sphere sr x au aa rank r denote x ecd called generating variate commonly used elliptical distributions include gaussian distribution tdistribution elliptical distributions widely used modeling financial return data since naturally capture many stylized properties including heavy tails tail dependence [ ] next theorem relates rq r w rq momentbased counterparts r w elliptical model theorem let x x xd ecd absolutely continuous elliptical f x e x ed independent copy x random vector x rq mq q constant depending distribution x moreover e rq cq r w rq cq r w q cov x covariance matrix x c constant given n x x x n x x ej ej x ek j j k cq q q var xj var xj xk n x x x ej x ek j k q var xj xk last two inequalities hold var xj xk var xj xk theorem elliptical model minimizing robust risk metric r w rq equivalent minimizing standard momentbased risk metric r w thus robust portfolio optimization equivalent momentbased counterpart population level plugging leads following theorem theorem let xt tz absolutely continuous stationary process satisfying conditions suppose x ecd follows elliptical distribution covariance matrix log dt c e opt r wopt q rt r w c c grossexposure constant cq defined rt defined e opt obtained robust portfolio thus elliptical model optimal portfolio w optimization also leads parametric rate convergence standard momentbased risk experiments section investigate empirical performance proposed portfolio optimization approach section demonstrate robustness proposed approach using synthetic heavytailed data section simulate portfolio management using standard poors sp stock index data proposed portfolio optimization approach qne compared three competitors competitors constructed replacing covariance matrix commonly used covariancescatter matrix estimators ogk orthogonalized gnanadesikankettenring estimator constructs pilot scatter matrix estimate using robust estimator scale reestimates eigenvalues using variances principal components [ ] factor principal factor estimator iteratively solves specific variances factor loadings [ ] shrink shrinkage estimator shrinkages sample covariance matrix towards onefactor covariance estimator[ ] synthetic data following [ ] construct covariance matrix asset returns using threefactor model xj bj f bj f bj f j j xj return jth stock bjk loadings jth stock factor fk j idiosyncratic noise independent three factors model covariance matrix stock returns given bf bt diag b [ bjk ] matrix consisting factor loadings f covariance matrix three factors j variance noise adopt covariance simulations following [ ] generate factor loadings b trivariate normal distribution nd b b mean b covariance b specified table factor loadings generated fixed parameters throughout simulations covariance matrix f three factors also given table standard deviations idiosyncratic noises generated independently truncated gamma distribution shape scale restricting support [ standard deviations fixed parameters generated according [ ] parameters obtained fitting threefactor model using threeyear daily return data industry portfolios may aug covariance matrix fixed throughout simulations since interested risk optimization set mean asset returns dimension stocks consideration fixed given covariance matrix generate asset return data following three distributions multivariate gaussian distribution nd table parameters generating covariance matrix equation parameters factor loadings risk grossexposure constant c factor shrink grossexposure constant c gaussian qne ogk factor shrink matching rate qne ogk elliptical lognormal matching rate factor shrink grossexposure constant c multivariate gaussian qne ogk grossexposure constant c factor shrink oracle qne ogk risk factor shrink oracle qne ogk factor shrink oracle qne ogk risk f matching rate parameters factor returns b b grossexposure constant c multivariate grossexposure constant c elliptical lognormal figure portfolio risks selected number stocks matching rates oracle optimal portfolios multivariate distribution degree freedom covariance matrix elliptical distribution lognormal generating variate log n covariance matrix distribution generate asset return series half year estimate covariancescatter matrices using qne three competitors plug optimize portfolio allocations also solve true covariance matrix obtain oracle optimal portfolios benchmarks range grossexposure constraint c results based simulations b matching rates optimized portfolios figure shows portfolio risks r w oracle optimal portfolios matching rate defined follows two portfolios p p let corresponding sets selected assets e assets weights ws nonzero matching rate p p defined r p p denotes cardinality set note two observations figure four estimators leads comparable portfolio risks gaussian model however heavytailed distributions qne achieves lower portfolio risk ii matching rates qne stable across three models higher competing methods heavytailed distributions thus conclude qne robust heavy tails risk minimization asset selection real data section simulate portfolio management using sp stocks collect adjusted daily closing prices stocks stayed sp index january due ` regularization grossexposure constraint solution generally sparse adjusted closing prices accounts corporate actions including stock splits dividends rights offerings table annualized sharpe ratios returns risks competing approaches using sp index data sharpe ratio c c c c c c qne ogk factor shrink return c c c c c c risk c c c c c c december using closing prices obtain daily returns daily growth rates prices manage portfolio consisting stocks january december days optimize portfolio allocations using past months stock return data sample points hold portfolio one day evaluate portfolio return day way obtain portfolio returns repeat process four methods comparison range grossexposure constant c since true covariance matrix stock returns unknown adopt sharpe ratio evaluating performances portfolios table summarizes annualized sharpe ratios mean returns empirical risks e standard deviations portfolio returns observe qne achieves largest sharpe ratios values grossexposure constant indicating lowest risks returns equivalently highest returns risk discussion paper propose robust portfolio optimization framework building quantilebased scatter matrix obtain nonasymptotic rates convergence scatter matrix estimators risk estimated portfolio relations proposed framework momentbased counterpart well understood main contribution robust portfolio optimization approach lies robustness heavy tails high dimensions heavy tails present unique challenges high dimensions compared low dimensions example asymptotic theory estimators guarantees consistency rate p op dn even nongaussian data [ ] n statistical error diminishes rapidly increasing n however n statistical error may scale rapidly dimension thus stringent tail conditions subgaussian conditions required guarantee consistency momentbased estimators high dimensions [ ] paper based quantile statistics achieve consistency portfolio risk without assuming tail conditions allowing scale nearly exponentially n another contribution work lies theoretical analysis serial dependence may affect consistency estimation measure degree serial dependence using mixing coefficient n show effect serial dependence pon rate convergence summarized parameter c characterizes size n n drop data avoid financial crisis stock prices likely violate stationary assumption c imposes upper bound percentage short positions practice percentage short positions usually strictly controlled much lower references [ ] harry markowitz portfolio selection journal finance [ ] michael j best robert r grauer sensitivity meanvarianceefficient portfolios changes asset means analytical computational results review financial studies [ ] vijay kumar chopra william ziemba effect errors means variances covariances optimal portfolio choice journal portfolio management [ ] robert c merton estimating expected return market exploratory investigation journal financial economics [ ] jarl g kallberg william ziemba misspecifications portfolio selection problems risk capital pages springer [ ] jianqing fan yingying fan jinchi lv high dimensional covariance matrix estimation using factor model journal econometrics [ ] james h stock mark w watson forecasting using principal components large number predictors journal american statistical association [ ] jushan bai kunpeng li et al statistical analysis factor models high dimension annals statistics [ ] jianqing fan yuan liao martina mincheva large covariance estimation thresholding principal orthogonal complements journal royal statistical society series b statistical methodology [ ] olivier ledoit michael wolf improved estimation covariance matrix stock returns application portfolio selection journal empirical finance [ ] olivier ledoit michael wolf wellconditioned estimator largedimensional covariance matrices journal multivariate analysis [ ] olivier ledoit michael wolf honey shrunk sample covariance matrix journal portfolio management [ ] peter j huber robust statistics wiley [ ] ricardo maronna ruben h zamar robust estimates location dispersion highdimensional datasets technometrics [ ] ramanathan gnanadesikan john r kettenring robust estimates residuals outlier detection multiresponse data biometrics [ ] yilun chen ami wiesel alfred hero robust shrinkage estimation highdimensional covariance matrices ieee transactions signal processing [ ] romain couillet matthew r mckay large dimensional analysis optimization robust shrinkage covariance matrix estimators journal multivariate analysis [ ] ravi jagannathan risk reduction large portfolios imposing wrong constraints helps journal finance [ ] jianqing fan jingjin zhang ke yu vast portfolio selection grossexposure constraints journal american statistical association [ ] peter j rousseeuw christophe croux alternatives median absolute deviation journal american statistical association [ ] h xu h shao solving matrix nearness problem maximum norm applying projection contraction method advances operations research [ ] alexandre belloni victor chernozhukov ` penalized quantile regression highdimensional sparse models annals statistics [ ] lan wang yichao wu runze li quantile regression analyzing heterogeneity ultrahigh dimension journal american statistical association [ ] peter j bickel elizaveta levina covariance regularization thresholding annals statistics [ ] tony cai cunhui zhang harrison h zhou optimal rates convergence covariance matrix estimation annals statistics [ ] kaitai fang samuel kotz kai wang ng symmetric multivariate related distributions chapman hall [ ] harry joe multivariate models dependence concepts chapman hall [ ] rafael schmidt tail dependence elliptically contoured distributions mathematical methods operations research [ ] svetlozar todorov rachev handbook heavy tailed distributions finance elsevier [ ] svetlozar rachev christian menn frank j fabozzi fattailed skewed asset return distributions implications risk management portfolio selection option pricing wiley [ ] kevin dowd measuring market risk wiley [ ] torben gustav andersen handbook financial time series springer [ ] jushan bai shuzhong shi estimating high dimensional covariance matrices applications annals economics finance [ ] sara van de geer sa van de geer empirical processes estimation cambridge university press cambridge [ ] alastair r hall generalized method moments oxford university press oxford [ ] peter buhlmann sara van de geer statistics highdimensional data methods theory applications springer')),\n",
       " ([(4, 0.8490634), (5, 0.14296453)],\n",
       "  (5937,\n",
       "   'logarithmic time online multiclass prediction anna choromanska courant institute mathematical sciences new york ny usa achoromacims nyu edu john langford microsoft research new york ny usa jclmicrosoft com abstract study problem multiclass classification extremely large number classes k goal obtaining train test time complexity logarithmic number classes develop topdown tree construction approaches constructing logarithmic depth trees theoretical front formulate new objective function optimized node tree creates dynamic partitions data pure terms class labels balanced demonstrate favorable conditions construct logarithmic depth trees leaves low label entropy however objective function nodes challenging optimize computationally address empirical problem new online decision tree construction procedure experiments demonstrate online algorithm quickly achieves improvement test error compared common logarithmic training time approaches makes plausible method computationally constrained largek applications introduction central problem paper computational complexity setting number classes k multiclass prediction large problems occur natural language translation best search result best detection tasks almost machine learning algorithms exception decision trees running times multiclass classification k canonical example oneagainstall classifiers [ ] setting efficient possible accurate approach given information theory [ ] essence multiclass classification algorithm must uniquely specify bits labels predicts correctly consequently krafts inequality [ ] equation implies expected computational complexity predicting correctly h per example h shannon entropy label worst case distribution k classes implies log k computation required hence goal achieving log k computational time per example training testing effectively using online learning algorithms minimize passes data goal logarithmic k complexity naturally motivates approaches construct logarithmic depth hierarchy labels one label per leaf hierarchy sometimes available prior knowledge many scenarios needs learned well naturally leads partition problem arises node hierarchy partition problem finding classifier c x divides examples two subsets purer set labels original set definitions purity vary canonical examples number labels remaining subset softer notions average shannon entropy class labels despite resulting classifier problem fundamentally different standard binary classification see note replacing c x c x bad binary classification impact quality partition partition problem fundamentally nonconvex throughout paper logarithmic time mean logarithmic time per example problem bears parallels clustering regard symmetric classes since average c x c x c x c x poor partition always function places points side choice partition matters problem dependent ways example consider examples line label position threshold classifiers case trying partition class labels class label results poor performance accuracy partition problem typically solved decision tree learning via enumerateandtest approach amongst small set possible classifiers see e g [ ] multiclass setting desirable achieve substantial error reduction node tree motivates using richer set classifiers nodes minimize number nodes thereby decrease computational complexity main theoretical contribution work establish boosting algorithm learning trees k nodes log k depth thereby addressing goal logarithmic time train test complexity main theoretical result presented section generalizes binary boostingbydecisiontree theorem [ ] multiclass boosting boosting results performance critically dependent quality weak learner supporting intuition need sufficiently rich partitioners nodes approach uses new objective decision tree learning optimize node tree objective theoretical properties presented section complete system multiple partitions lomtree vs oneagainstall could constructed top boost oaa ing theorem bottom filter tree [ ] lomtree bottom partition process appears impossi ble representational constraints shown section supplementary material focus topdown tree creation whenever representational constraints partitions linear classifiers finding strong partition function requires efficient search set classifiers ef ficient searches large function classes routinely performed via gradient descent tech niques supervised learning seem number classes like natural candidate existing literature figure comparison oneagainst examples exist problem oaa logarithmic online multi indeed binary prespeciclass tree lomtree oneagainstall con fied hierarchy labels need strained use training time find partitioners aligned hierarchy lomtree dataset truncation lomtree con neither cases applieswe multistrained use representation complex ple labels want dynamically create ity oneagainstall number class choice partition rather assuming labels grows problem becomes harder one handed us exist purity criterion amenable gradient descent aplomtree becomes dominant proach precise objective studied theory fails test due discrete nature even natural approximations challenging tractably optimize computational constraints result use theoretical objective motivation construct new logarithmic online multiclass tree lomtree algorithm empirical evaluation creating tree online fashion creates new class problems node initially created eventually proves useless examples go best results wasteful solution practice starves parts tree need representational complexity deal design efficient process recycling orphan nodes locations needed prove number times node recycled logarithmic number examples algorithm described section analyzed section effective given inherent nonconvexity partition problem unavoidably empirical question answer range datasets varying k classes section find constrained training times approach quite effective compared baselines dominating log k train time approaches whats new best knowledge splitting criterion boosting statement lomtree algorithm swapping guarantee experimental results new prior work authors address logarithmic time training filter tree [ ] addresses consistent robust multiclass classification showing possible statistical limit filter tree address partition problem shown experimental section often helpful partition finding problem addressed conditional probability tree [ ] paper addresses conditional probability estimation conditional probability estimation converted multiclass prediction [ ] logarithmic time operation quite authors addressed logarithmic testing time allowing training time k worse approaches intractable larger scale problems describe context partition problem addressed recursively applying spectral clustering confusion graph [ ] clustering approaches include [ ] empirically approach found sometimes lead badly imbalanced splits [ ] context ranking another approach uses kmeans hierarchical clustering recover label sets given partition [ ] recent work [ ] multiclass classification problem addresses via sparse output coding tuning highcardinality multiclass categorization bitbybit decoding problem authors decouple learning processes coding matrix bit predictors use probabilistic decoding decode optimal class label authors however specify class similarity k compute see section [ ] hence approach different complexity class also born experimentally variant popular error correcting output code scheme solving multilabel prediction problems large output spaces assumption output sparsity also considered [ ] approach general requires k running time decode since essence fit label predictions must checked k labels another approach [ ] proposes iterative leastsquaresstyle algorithms multiclass multilabel prediction relatively large number examples data dimensions work [ ] focusing particular costsensitive multiclass classification approaches however k training time decision trees naturally structured allow logarithmic time prediction traditional decision trees often difficulties large number classes splitting criteria wellsuited large class setting however newer approaches [ ] addressed effectively significant scales context multilabel classification multilabel learning missing labels also addressed [ ] specifically first work [ ] performs brute force optimization multilabel variant gini index defined set positive labels node assumes label independence random forest construction method makes fast predictions however high training costs [ ] second work [ ] optimizes rank sensitive loss function discounted cumulative gain additionally wellknown problem hierarchical classification performance significantly deteriorates lower hierarchy [ ] authors solve biasing training distribution reduce error propagation simultaneously combining bottomup topdown approaches training [ ] reduction approach use optimizing partitions implicitly optimizes differential objective nonreductive approach tried previously [ ] objectives yielding good results different context framework theoretical analysis section describe essential elements approach outline theoretical properties resulting framework begin highlevel ideas setting employ hierarchical approach learning multiclass decision tree structure training structure topdown fashion assume receive examples x x rd labels k also assume access hypothesis class h h h binary classifier h x overall objective learn tree depth log k node tree consists classifier h classifiers trained way hn x hn denotes classifier node n tree means example x sent right subtree node n hn x sends x left subtree reach leaf predict according label highest frequency amongst examples reaching leaf paper skip index n whenever clear context consider fixed tree node interest computational complexity want encourage number examples going left right fairly balanced good statistical accuracy want send examples class almost exclusively either left right subtree thereby refining purity class distributions subsequent levels tree purity tree node therefore measure whether examples class reaching node mostly sent one child node pure split otherwise children impure split formal definitions balancedness purity introduced section objective expressing criteria resulting theoretical properties illustrated following sections key consideration picking objective want effectively optimize hypotheses h h streaming examples online fashion seems unsuitable standard decision tree objectives shannon gini entropy leads us design new objective time show section suitable assumptions optimizing objective also leads effective reduction average shannon entropy entire tree objective analysis resulting partitions define criterion measure quality hypothesis h h creating partitions fixed node n tree let denotes proportion label amongst examples reaching node let p h x p h x denote fraction examples reaching n h x marginally conditional class respectively define objective k x j h p h x p h x aim maximize objective j h obtain high quality partitions intuitively objective encourages fraction examples going right class substantially different background fraction class concrete simple scenario p h x hypothesis h objective prefers p h x close possible class leading pure partitions make intuitions formal definition purity hypothesis h h induces pure split k x min p h x p h x [ called purity factor particular partition called maximally pure meaning class sent exclusively left right define similar definition balancedness split definition balancedness hypothesis h h induces balanced split c p h x c z c ] called balancing factor partition called maximally balanced meaning equal number examples sent left right children partition balancing factor purity factor related shown lemma proofs lemma following lemma lemma deferred supplementary material lemma hypothesis h distribution examples x purity factor balancing factor satisfy min j h partition called maximally pure balanced satisfies see j h hypothesis h inducing maximally pure balanced partition captured next lemma course expect hypotheses producing maximally pure balanced splits practice lemma hypothesis h x objective j h satisfies j h [ ] furthermore h induces maximally pure balanced partition j h want objective achieve optimum simultaneously pure balanced split standard entropybased criteria shannon gini entropy well criterion propose posed equation satisfy requirement entropybased criteria see [ ] criterion see lemma algorithm could also implemented batch streaming case latter one example make one pass data per every tree level however massive datasets making multiple passes data computationally costly justifying need online approach proposed objective function exhibits similarities socalled carnaps measure [ ] used probability inductive logic quality entire tree section helps us understand quality individual split produced effectively maximizing j h next reason quality entire tree add nodes measure quality trees using average entropy leaves tree track decrease entropy function number nodes analysis extends theoretical analysis [ ] originally developed show boosting properties decision trees binary classification problems multiclass classification setting given tree consider entropy function gt measure quality tree k x x gt wl li ln li li probabilities randomly chosen data point x drawn p p fixed target distribution x label given x reaches node l l denotes set tree leaves denotes number internal tree nodes wl weight p leaf l defined probability randomly chosen x drawn p reaches leaf l note wl next state main theoretical result paper captured theorem adopt weak learning framework weak hypothesis assumption captured definition posits node tree hypothesis h hypothesis class h guarantees simultaneously weak purity weak balancedness split distribution p x assumption one use new decision tree approach drive error threshold definition weak hypothesis assumption let denote node tree let p hm x pmi p hm x furthermore let r min ] say weak hypothesis assumption satisfied distribution p x node tree exists hypothesis hm h pk j hm mi pmi theorem weak hypothesis assumption [ ] obtain gt suffices make ln k splits defer proof theorem supplementary material provide sketch analysis studies tree construction algorithm recursively find leaf node highest weight choose split two children let n heaviest leaf time consider splitting two children contribution node n tree entropy changes splits change entropy reduction corresponds gap jensens inequality applied concave function thus lowerbounded use fact shannon entropy strongly concave respect ` norm see e g example shalevshwartz [ ] obtained lowerbound turns depend proportionally j hn implies larger objective j hn time larger entropy reduction ends reinforces intuitions maximize j general might possible find hypothesis large enough objective j hn guarantee sufficient progress point appeal weak learning assumption assumption used lowerbound entropy reduction prove theorem lomtree algorithm objective function section another convenient form yields simple online algorithm tree construction training note equation written details shown section supplementary material j h ei [ ex [ h x ] ex [ h x ] ] maximizing objective discrete optimization problem relaxed follows j h ei [ ex [ h x ] ex [ h x ] ] ex [ h x ] expected score class next explain empirical approach maximizing relaxed objective empirical estimates expectations easily stored updated online every tree node decision whether send example reaching node left right child node based sign difference two expectations ex [ h x ] ex [ h x ] label data point e ex [ h x ] ex [ h x ] data point sent left else sent right procedure conveniently demonstrated toy example section supplement training algorithm assigns unique label node tree currently leaf label highest frequency amongst examples reaching leaf algorithm lomtree algorithm online tree training input regression algorithm r max number tree nonleaf nodes swap resistance rs subroutine setnode v mv mv sum scores class lv lv number points class reaching v nv nv number points class used train regressor v ev ev expected score class ev expected total score cv size smallest leaf subtree root v subroutine updatec v v r cparent v cv v parent v cv min cleft v cright v subroutine swap v find leaf cs cr spaparent sgpa grandpa ssibsibling spa left sgpa left sgpa ssib else right sgpa ssib updatec ssib setnode left v setnode spa right v spa create root r setnode r example x set j r lj mj lj nj ej lj j leaf lj least nonzero entries tt cj maxi lj rs cr tt setnode left j setnode right j else swap j cleft j bcj c cright j cj cleft j updatec left j j leaf ej ej c else c train hj example x c r x c pk mj nj mj hj x ej mj nj ej pi k nj set j child j corresponding hj else cj break testing test example pushed tree along path root leaf nonleaf node path regressor directs example either left right child node test example labeled label assigned leaf example descended training algorithm detailed algorithm tree node contains classifier use linear classifiers e hj regressor stored node j hj x value prediction hj example x stopping criterion expanding tree number nonleaf nodes reaches threshold swapping consider scenario current training example descends leaf j leaf split create two children examples reached past coming least two different smallest leaf one smallest total number data points reaching past parent v left v right v denote resp parent left right child node v grandpa v sibling v denote respectively grandparent node v sibling node v e node parent v implementation sums stored variables thus updating ev takes computations also refer prediction value score section r j r sgpa ssib j spa spa sgpa ssib figure illustration swapping procedure left swap right swap classes however number nonleaf nodes tree reaches threshold nodes expanded thus j cannot create children since tree construction done online nodes created early stages training may end useless examples reach later prevents potentially useful splits leaf j problem solved recycling orphan nodes subroutine swap algorithm general idea behind node recycling allow nodes split certain condition met particular node j splits following holds cj max k lj rs cr r denotes root entire tree cj size smallest leaf subtree root j smallest leaf one smallest total number data points reaching past lj kdimensional vector nonnegative integers ith element count number data points label reaching leaf j past finally rs swap resistance subtraction maxi k lj equation ensures pure node recycled condition inequality satisfied swap nodes performed orphan leaf reached smallest number examples past parent spa detached tree become children node j whereas old sibling ssib orphan node becomes direct child old grandparent sgpa swapping procedure shown figure condition captured inequality allows us prove number times given node recycled upperbounded logarithm number examples whenever swap resistance lemma lemma let swap resistance rs greater equal sequences examples number times algorithm recycles given node upperbounded logarithm base sequence length experiments address several hypotheses experimentally lomtree algorithm achieves true logarithmic time computation practice lomtree algorithm competitive better logarithmic traintest time algorithms multiclass classification lomtree algorithm statistical performance close common k approaches address hypotheses contable dataset sizes ducted experiments variety isolet sector aloi imnet odp benchmark multiclass datasets isosize mb mb mbgb gb let sector aloi imagenet im features k net odp details examples k datasets provided table datasets divided training classes k k testing furthermore training dataset used validation set baselines compared lomtree balanced random tree logarithmic depth rtree filter tree [ ] computationally feasible also compared oneagainstall classifier oaa representative k approach methods implemented vowpal wabbit [ ] learning system similar levels optimization regressors tree nodes lomtree rtree filter tree well oaa regressors trained online gradient descent explored step sizes chosen set used compressed details source dataset provided supplementary material linear regressors method investigated training passes data selected best setting parameters step size number passes one minimizing validation error additionally lomtree investigated different settings stopping criterion tree expansion k k k k k k k swap resistance rs table report respectively train time perexample test time best performer indicated bold training time later reported test error provided oaa imagenet odp due intractability petabyte scale computations table training time selected problems table perexample test time problems isolet sector aloi isolet sector aloi imnet odp lomtree lomtree ms ms ms ms ms oaa oaa ms ms ms log time ratio first hypothesis consistent experimental results timewise lomtree significantly outperforms oaa due building closeto logarithmic depth trees improvement training time increases number classes classification problem instance aloi training lomtree times faster oaa said test time perexample test time aloi imagenet odp respectively times faster oaa significant advantage lomtree oaa also captured figure next table best logarithmic time perlomtree vs oneagainstall former indicated bold report test error logarithmic traintest time algorithms also show binomial symmetrical confidence intervals results clearly sec ond hypothesis also consistent experimental results since rtree imposes random label partition resulting error ob tains generally worse error obtained competitor methods including lomtree learns label partitioning directly data time lomtree beats fil ter tree every dataset though imagenet log number classes figure logarithm ratio perexample odp high level noise advantage lomtree significant test times oaa lomtree problems table test error confidence interval problems lomtree rtree filter tree oaa isolet sector aloi imnet na odp na third hypothesis weakly consistent empirical results time advantage lomtree comes loss statistical accuracy respect oaa oaa tractable conclude lomtree significantly closes gap logarithmic time methods oaa making plausible approach computationally constrained largek applications conclusion lomtree algorithm reduces multiclass problem set binary problems organized tree structure partition every tree node done optimizing new partition criterion online criterion guarantees pure balanced splits leading logarithmic training testing time tree classifier provide theoretical justification approach via boosting statement empirically evaluate multiple multiclass datasets empirically find best available logarithmic time approach multiclass classification problems note however mechanics testing datastes much easier one simply test effectively untrained parameters examples measure test speed thus perexample test time oaa imagenet odp provided also best knowledge exist stateoftheart results oaa performance datasets published literature acknowledgments would like thank alekh agarwal dean foster robert schapire matus telgarsky valuable discussions references [ ] r rifkin klautau defense onevsall classification j mach learn res [ ] cover j thomas elements information theory john wiley sons inc [ ] l breiman j h friedman r olshen c j stone classification regression trees crc press llc boca raton florida [ ] kearns mansour boosting ability topdown decision tree learning algorithms journal computer systems sciences also stoc [ ] beygelzimer j langford p ravikumar errorcorrecting tournaments alt [ ] beygelzimer j langford lifshits g b sorkin l strehl conditional probability tree estimation analysis algorithms uai [ ] c bishop pattern recognition machine learning springer [ ] bengio j weston grangier label embedding trees large multiclass tasks nips [ ] g madzarov gjorgjevikj chorbev multiclass svm classifier utilizing binary decision tree informatica [ ] j deng satheesh c berg l feifei fast balanced efficient label tree learning large scale object recognition nips [ ] j weston makadia h yee label partitioning sublinear ranking icml [ ] b zhao e p xing sparse output coding largescale visual recognition cvpr [ ] hsu kakade j langford zhang multilabel prediction via compressed sensing nips [ ] agarwal kakade n karampatziakis l song g valiant least squares revisited scalable approaches multiclass prediction icml [ ] beijbom saberian kriegman n vasconcelos guessaverse loss functions costsensitive multiclass boosting icml [ ] r agarwal gupta prabhu varma multilabel learning millions labels recommending advertiser bid phrases web pages www [ ] prabhu varma fastxml fast accurate stable treeclassifier extreme multilabel learning acm sigkdd [ ] h f yu p jain p kar dhillon largescale multilabel learning missing labels icml [ ] liu yang h wan h j zeng z chen w support vector machines classification largescale taxonomy sigkdd explorations [ ] p n bennett n nguyen refined experts improving classification large taxonomies sigir [ ] montillo j tu j shotton j winn j e iglesias n metaxas criminisi entanglement differentiable information gain maximization decision forests computer vision medical image analysis [ ] k tentori v crupi n bonini osherson comparison confirmation measures cognition [ ] r carnap logical foundations probability nd ed chicago university chicago press par pp [ ] shalevshwartz online learning online convex optimization found trends mach learn [ ] j langford l li strehl httphunch net vw [ ] nesterov introductory lectures convex optimization basic course applied optimization kluwer academic publ [ ] j deng w dong r socher l j li k li l feifei imagenet largescale hierarchical image database cvpr')),\n",
       " ([(2, 0.652671), (5, 0.026048621), (6, 0.18106683), (9, 0.13853863)],\n",
       "  (5802,\n",
       "   'planar ultrametrics image segmentation charless c fowlkes department computer science university california irvine fowlkesics uci edu julian yarkony experian data lab san diego ca julian yarkonyexperian com abstract study problem hierarchical clustering planar graphs formulate terms finding closest ultrametric specified set distances solve using lp relaxation leverages minimum cost perfect matching subroutine efficiently explore space planar partitions apply algorithm problem hierarchical image segmentation introduction formulate hierarchical image segmentation perspective estimating ultrametric distance set image pixels agrees closely input set noisy pairwise distances ultrametric space replaces usual triangle inequality ultrametric inequality u v max u w v w captures transitive property clustering u w cluster v w cluster u v must also cluster thresholding ultrametric immediately yields partition sets whose diameter less given threshold varying distance threshold naturally produces hierarchical clustering clusters high thresholds composed clusters lower thresholds inspired approach [ ] method represents ultrametric explicitly hierarchical collection segmentations determining appropriate segmentation single distance threshold equivalent finding minimumweight multicut graph positive negative edge weights [ ] finding ultrametric imposes additional constraint multicuts hierarchically consistent across different thresholds focus case input distances specified planar graph arises naturally domain image segmentation elements pixels superpixels distances defined neighbors allows us exploit fast combinatorial algorithms partitioning planar graphs yield tighter lp relaxations local polytope relaxation often used graphical inference [ ] paper organized follows first introduce closest ultrametric problem relation multicuts ultrametrics describe lp relaxation uses delayed column generation approach exploits planarity efficiently find cuts via classic reduction minimumweight perfect matching [ ] apply algorithm task natural image segmentation demonstrate algorithm converges rapidly produces optimal nearoptimal solutions practice closest ultrametric multicuts let g v e weighted graph nonnegative edge weights indexed edges e u v e goal find ultrametric distance uv vertices graph p close sense distortion uv e k uv uv k minimized begin reformulating closest ultrametric problem terms finding set nested multicuts family weighted graphs specify partitioning multicut vertices graph g components using binary vector x e xe indicates edge e u v cut vertices u v associated edge separate components partition use mcut g denote set binary indicator vectors x represent valid multicuts graph g notational simplicity remainder paper frequently omit dependence g given fixed input necessary sufficient condition indicator vector x define valid multicut g every cycle edges one edge cycle cut least one edge cycle must also cut let c denote set cycles g cycle c c set edges c e set edges cycle c excluding edge e express mcut terms cycle inequalities x e xe xe c c e c mcut x ece hierarchical clustering graph described nested collection multicuts denote space valid hierarchical partitions l layers l represent set l edgeindicator vectors x x x x x l cut edge remains cut finer layers hierarchy l x x x l x l mcut x l x l l given valid hierarchical clustering x ultrametric specified vertices graph choosing sequence real values l indicate distance threshold associated level l hierarchical clustering ultrametric distance specified pair x assigns distance pair vertices uv based coarsest level clustering remain separate clusters pairs corresponding edge graph u v e e write explicitly terms multicut indicator vectors l x de max l xel l [ xel xel ] l l l xel xe pairs u v correspond assume convention edge original graph still assigned unique distance based coarsest level l lie different connected components cut specified x l compute quality ultrametric respect input set edge weights measure squared l difference edge weights ultrametric distance k dk write compactly terms multicut pm indicator vectors construct set weights edge layer denoted el l el ke k weights given explicitly telescoping series e ke k l use r e el ke l k ke l k denote vector containing el l e e fixed number levels l fixed set thresholds problem finding closest ultrametric written integer linear program ilp edge cut indicators l l x x xx l l l min [ xe xe ] min ke l k xel xel e x l x l ee ee l l l x x l l l l l ke k ke k xe ke k xe min ke k xe x l min x l ee l x x l ee l el xel min x l l x l x l l optimization corresponds solving collection minimumweight multicut problems multicuts constrained hierarchically consistent linear combination cut vectors b hierarchical cuts figure partitioning x represented linear superposition cuts z cut isolates connected component partition assigned weight [ ] introducing auxiliary slack variables able represent larger set valid indicator vectors x using fewer columns z b introducing additional slack variables layer hierarchical segmentation efficiently represent many hierarchical segmentations x x x consistent layer layer using small number cut indicators columns z computing minimumweight multicuts also known correlation clustering np hard even case planar graphs [ ] direct approach finding approximate solution eq relax integrality constraints x l instead optimize whole polytope defined set cycle inequalities use l denote corresponding relaxation l resulting polytope convex hull mcut integral vertices correspond exactly set valid multicuts [ ] practice found applying straightforward cuttingplane approach successively adds violated cycle inequalities relaxation eq requires far many constraints slow useful instead develop column generation approach tailored planar graphs allows efficient accurate approximate inference cut cone planar multicuts consider partition planar graph two disjoint sets nodes denote space indicator vectors corresponding twoway cuts cut cut may yield two connected components produce every possible multicut e g split triangle three nodes three separate components let z e cut indicator matrix column specifies valid twoway cut zek edge e cut twoway cut k indicator vector multicut planar graph generated suitable linear combination cuts columns z isolate individual components rest graph weight cut let r cut vector specifying positive weighted combination cuts set cut z conic hull cut cut cone since multicut expressed superposition cuts cut cone identical conic hull mcut equivalence suggests lp relaxation minimumcost multicut given min z z vector r e specifies edge weights case planar graphs solution lp relaxation satisfies cycle inequalities see supplement [ ] expanded multicut objective since matrix z contains exponential number cuts eq still intractable instead consider approximation using constraint set z subset columns z previous work [ ] showed since optimal multicut may longer lie span reduced cut matrix z useful allow values z exceed see figure example introduce slack vector tracks presence overcut edges prevents contributing objective corresponding edge weight negative let e min e denote nonpositive component e expanded multicut objective given min z z edge e e decrease objective overcutting amount e exactly compensated objective term e e z contains cuts e z z eq eq equivalent [ ] minimizer eq z contains subset columns edge indicator vector given x min z still satisfies cycle inequalities see supplement details expanded lp finding closest ultrametric develop lp relaxation closest ultrametric problem replace multicut problem layer l expanded multicut objective described eq let l l denote collection weights slacks levels hierarchy let el max el el min el denote positive negative components l enforce hierarchical consistency layers would like add constraint z l z l however constraint rigid z include possible cuts thus computationally useful introduce additional slack vector associated level l edge e denote l introduction el allows cuts represented z l violate hierarchical constraint modify objective violations original hierarchy constraint paid proportion el introduction allows us find valid ultrametrics using smaller number columns z used would otherwise required illustrated figure b call relaxed closest ultrametric problem including slack variable expanded closest ultrametric objective written min l l l x x x l z l l l l l l l z l l z l l l l z l l l l convention define l dropped constant l term eq given solution recover relaxed solution closest ultrametric problem eq l setting xel min maxml z e supplement demonstrate obeys constraints eq thresholding operation yields solution x lies l achieves lower objective value dual objective optimize dual objective eq using efficient column generation approach based perfect matching introduce two sets lagrange multipliers l l corresponding within layer constraints respectively algorithm dual closest ultrametric via cutting planes z l l residual residual solve eq given z residual l l z l arg minzcut l l l l z residual residual l l l l z l z z z isocuts z l z l z l z z z end end notational convenience let dual objective written max l x l l l l l l l l l l l l l z l dual lp interpreted finding small modification original edge weights l every possible twoway cut resulting graph level l nonnegative weight observe introduction two slack terms primal problem eq results bounds lagrange multipliers dual problem eq practice dual constraints turn essential efficient optimization constitute core contribution paper solving dual via cutting planes chief complexity dual lp contained constraints including z encodes nonnegativity exponential number cuts graph represented columns z circumvent difficulty explicitly enumerating columns z employ cutting plane method efficiently searches additional violated constraints columns z successively added let z denote current working set columns dual optimization algorithm iterates following three steps solve dual lp z find violated constraint form l l l l z layer l append column matrix z cut found terminate violated constraints exist computational budget exceeded finding violated constraints identifying columns add z carried layer l separately finding violated constraint full problem corresponds computing minimumweight cut graph edge weights l l l l cut nonnegative weight constraints satisfied otherwise add corresponding cut indicator vector additional column z generate new constraint layer l based current lagrange multipliers solve x z l arg min el le el el ze zcut ee subsequently add new constraints layers lp z [ z z z z l ] unlike multicut problem finding twoway cut planar graph solved exactly reduction minimumweight perfect matching classic result e g provides exact solution ground state lattice ising model without ferromagnetic field [ ] n log n time [ ] ub lb bound counts time sec objective ratio ucm um figure average convergence upper blue lowerbounds red function running time values plotted gap bound best lowerbound computed termination given problem instance relative gap averaged problem instances yet converged given time point indicate percentage problem instances yet terminate using black bars marking [ ] percent b histogram ratio closest ultrametric objective values algorithm um baseline clustering produced ucm ratios less showing instances um produce worse solution ucm computing lower bound given iteration prior adding newly generated set constraints p compute total residual constraint violation layers hierarchy l l l l l z l supplement demonstrate value dual objective plus lowerbound relaxed closest ultrametric problem eq thus costs minimumweight matchings approach zero objective reduced problem z approaches accurate lowerbound optimization l expanding generated cut constraints given cut z l produces two connected components found useful add constraint corresponding component following approach [ ] let number connected components z l denoted components add one column z corresponding cut isolates connected component rest allows flexibility representing final optimum multicut superpositions components addition also found useful practice maintain separate set constraints z l layer l maintaining independent constraints z z z l result smaller overall lp speeding convergence found adding explicit penalty term objective encourages small values speeds convergence dramatically loss solution quality experiments penalty scaled parameter chosen extremely small magnitude relative values influence forces acting given term primal decoding algorithm gives summary dual solver produces lowerbound well set cuts described constraint matrices z l subroutine isocuts z l computes set cuts isolate connected component z l generate hierarchical clustering solve primal eq using reduced set z order recover fractional solution xel min maxml z e use lp solver ibm cplex provides primal solution free solving dual alg round fractional primal solution x discrete hierarchical clustering thresholding xel [ xel ] repair uncut cut edges lie inside connected component implementation test discrete thresholds take threshold yields x lowest cost pass loop alg compute upperbounds retain optimum solution observed thus far precision maximum fmeasure ucm ucml um recall um ucml ucm time sec figure boundary detection performance closest ultrametric algorithm um baseline ultrametric contour maps algorithm ucm without ucml length weighting [ ] bsds black circles indicate thresholds used closest um optimization b anytime performance fmeasure bsds benchmark function runtime um ucm without length weighting achieve maximum fmeasure respectively experiments applied algorithm segmenting images berkeley segmentation data set bsds [ ] use superpixels generated performing oriented watershed transform output global probability boundary gpb edge detector [ ] construct planar graph whose vertices superpixels edges connecting neighbors image plane whose base distance derived gp b let gp local estimate boundary contrast given averaging gp b classifier output boundary pair neighboring superpixels truncate extreme values enforce gp gp [ ] set e log gp log additive offset assures e experiments use fixed set eleven distance threshold levels l chosen uniformly span useful range threshold values [ ] finally weighted edges proportionally length corresponding boundary image performed dual cutting plane iterations convergence seconds passed lowerbounds bsds segmentations order terminate total residual greater codes written matlab using blossom v implementation minimumweight perfect matching [ ] ibm ilog cplex lp solver default options baseline compare results hierarchical clusterings produced ultrametric contour map ucm [ ] ucm performs agglomerative clustering superpixels assigns lengthweighted averaged gp b value distance pair merged regions ucm explicitly designed find closest ultrametric provides strong baseline hierarchical clustering compute closest llevel ultrametric corresponding ucm clustering result solve minimization eq restricting multicut partition level ucm hierarchy convergence timing figure shows average behavior convergence function runtime found upperbound given cost decoded integer solution lowerbound estimated dual lp close integrality gap typically within lowerbound never convergence dual achieved quite rapidly instances require less iterations converge roughly linear growth size lp iteration cutting planes added fig display histogram computed test image problem instances cost ucm solutions relative produced closest ultrametric um estimated method ratio less indicates approach generated solution lower distortion ultrametric problem instance ucm outperform um algorithm um mc um mc figure proposed closest ultrametric um enforces consistency across levels performing independent multicut clustering mc threshold guarantee hierarchical segmentation c f first image columns second image hierarchical segmentation um better preserves semantic parts two birds correctly merging background regions segmentation quality figure shows segmentation benchmark accuracy closest ultrametric algorithm denoted um along baseline ultrametric contour maps algorithm ucm without length weighting [ ] terms segmentation accuracy um performs nearly identically state art ucm algorithm small gains highprecision regime worth noting bsds benchmark provide strong penalties small leaks two segments total number boundary pixels involved small algorithm may find strong application domains local boundary signal noisier e g biological imaging undersegmentation heavily penalized cuttingplane approach slower agglomerative clustering necessary wait convergence order produce high quality results found upper lower bounds decrease function time clustering performance measured precisionrecall often nearly optimal ten seconds remains stable figure shows plot fmeasure achieved um function time importance enforcing hierarchical constraints although independently finding multicuts different thresholds often produces hierarchical clusterings means guaranteed ran algorithm setting el allowing layer solved independently fig shows examples hierarchical constraints layers improves segmentation quality relative independent clustering threshold conclusion introduced new method approximating closest ultrametric planar graphs applicable hierarchical image segmentation contribution dual cutting plane approach exploits introduction novel slack terms allow representing much larger space solutions relatively cutting planes yields efficient algorithm provides rigorous bounds quality resulting solution empirically observe algorithm rapidly produces compelling image segmentations along lower upperbounds nearly tight benchmark bsds test data set acknowledgements jy acknowledges support experian cf acknowledges support nsf grants iis dbi references [ ] nir ailon moses charikar fitting tree metrics hierarchical clustering phylogeny foundations computer science pages [ ] bjoern andres joerg h kappes thorsten beier ullrich kothe fred hamprecht probabilistic image segmentation closedness constraints proc iccv pages [ ] bjoern andres thorben kroger kevin l briggman winfried denk natalya korogod graham knott ullrich kothe fred hamprecht globally optimal closedsurface segmentation connectomics proc eccv [ ] bjoern andres julian yarkony b manjunath stephen kirchhoff engin turetken charless fowlkes hanspeter pfister segmenting planar superpixel adjacency graphs w r nonplanar superpixel affinity graphs proc emmcvpr [ ] pablo arbelaez michael maire charless fowlkes jitendra malik contour detection hierarchical image segmentation ieee trans pattern anal mach intell may [ ] yoram bachrach pushmeet kohli vladimir kolmogorov morteza zadimoghaddam optimal coalition structure generation cooperative graph games proc aaai [ ] shai bagon meirav galun large scale correlation clustering corr abs [ ] f barahona computational complexity ising spin glass models journal physics mathematical nuclear general april [ ] f barahona cuts matchings planar graphs mathematical programming november [ ] f barahona mahjoub cut polytope mathematical programming september [ ] thorsten beier thorben kroeger jorg h kappes ullrich kothe fred hamprecht cut glue cut fast approximate solver multicut partitioning computer vision pattern recognition cvpr ieee conference pages [ ] michel deza monique laurent geometry cuts metrics volume springer science business media [ ] michael fisher dimer solution planar ising models journal mathematical physics [ ] sungwoong kim sebastian nowozin pushmeet kohli chang dong yoo higherorder correlation clustering image segmentation advances neural information processing systems pages [ ] vladimir kolmogorov blossom v new implementation minimum cost perfect matching algorithm mathematical programming computation [ ] david martin charless fowlkes doron tal jitendra malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics proc iccv pages [ ] david martin charless c fowlkes jitendra malik learning detect natural image boundaries using local brightness color texture cues ieee trans pattern anal mach intell may [ ] julian yarkony analyzing planarcc nips workshop [ ] julian yarkony thorsten beier pierre baldi fred hamprecht parallel multicut segmentation via dual decomposition new frontiers mining complex patterns [ ] julian yarkony alexander ihler charless fowlkes fast planar correlation clustering image segmentation proc eccv [ ] chong zhang julian yarkony fred hamprecht cell detection segmentation using correlation clustering miccai volume pages')),\n",
       " ([(2, 0.96110547), (8, 0.03858853)],\n",
       "  (5776,\n",
       "   'expressing image stream sequence natural sentences cesc chunseong park gunhee kim seoul national university seoul korea park chunseonggunhee snu ac kr httpsgithub comcescparkcrcn abstract propose approach retrieving sequence natural sentences image stream since general users often take series pictures special moments would better take consideration whole image stream produce natural language descriptions almost previous studies dealt relation single image single natural sentence work extends input output dimension sequence images sequence sentences end design multimodal architecture called coherence recurrent convolutional network crcn consists convolutional neural networks bidirectional recurrent neural networks entitybased local coherence model approach directly learns vast usergenerated resource blog posts textimage parallel training data demonstrate approach outperforms stateoftheart candidate methods using quantitative measures e g bleu topk recall user studies via amazon mechanical turk introduction recently hike interest automatically generating natural language descriptions images research computer vision natural language processing machine learning e g [ ] existing work aims discovering relation single image single natural sentence extend input output dimension sequence images sequence sentences may obvious next step toward joint understanding visual content images language descriptions albeit underaddressed current literature problem setup motivated general users often take series pictures memorable moments example many people visit new york city nyc would capture experiences large image streams thus would better take whole photo stream consideration translation natural language description figure intuition problem statement new york city example aim expressing image stream sequence natural sentences leverage natural blog posts learn relation image streams sentence sequences b propose coherence recurrent convolutional networks crcn integrate convolutional networks bidirectional recurrent networks entitybased coherence model fig illustrates intuition problem statement example visiting nyc objective given photo stream automatically produce sequence natural language sentences best describe essence input image set propose novel multimodal architecture named coherence recurrent convolutional networks crcn integrate convolutional neural networks image description [ ] bidirectional recurrent neural networks language model [ ] local coherence model [ ] smooth flow multiple sentences since problem deals learning semantic relations long streams images text challenging obtain appropriate textimage parallel corpus previous research single sentence generation idea issue directly leverage online natural blog posts textimage parallel training data usually blog consists sequence informative text multiple representative images carefully selected authors way storytelling see example fig evaluate approach blog datasets nyc disneyland consisting k blog posts k associated images although focus tourism topics experiments approach completely unsupervised thus applicable domain large set blog posts images demonstrate superior performance approach comparing stateoftheart alternatives including [ ] evaluate quantitative measures e g bleu topk recall user studies via amazon mechanical turk amt related work due recent surge volume literature subject generating natural language descriptions image data discuss representative selection ideas closely related work one popular approaches pose text generation retrieval problem learns ranking embedding caption test image transferred sentences similar training images [ ] approach partly involves text retrieval search candidate sentences image query sequence training database however create final paragraph considering compatibilities individual images text coherence captures text relatedness level sentencetosentence transitions also videosentence works e g [ ] key novelty explicitly include coherence model unlike videos consecutive images streams may show sharp changes visual content cause abrupt discontinuity consecutive sentences thus coherence model demanded make output passages fluent many recent works exploited multimodal networks combine deep convolutional neural networks cnn [ ] recurrent neural network rnn [ ] notable architectures category integrate cnn bidirectional rnns [ ] longterm recurrent convolutional nets [ ] longshort term memory nets [ ] deep boltzmann machines [ ] dependencytree rnn [ ] variants multimodal rnns [ ] although method partly take advantage recent progress multimodal neural networks major novelty integrate coherence model unified endtoend architecture retrieve fluent sequential multiple sentences following compare previous work bears particular resemblance among multimodal neural network models longterm recurrent convolutional net [ ] related objective framework explicitly models relations sequential inputs outputs however model applied video description task creating sentence given short video clip address generation multiple sequential sentences hence unlike mechanism coherence sentences work [ ] addresses retrieval image sequences query paragraph opposite direction problem propose latent structural svm framework learn semantic relevance relations text image sequences however model specialized image sequence retrieval thus applicable natural sentence generation contributions highlight main contributions paper follows best knowledge work first address problem expressing image streams sentence sequences extend input output elaborate forms respect whole body existing methods image streams instead individual images sentence sequences instead individual sentences develop multimodal architecture coherence recurrent convolutional networks crcn integrates convolutional networks image representation recurrent networks sentence modeling local coherence model fluent transitions sentences evaluate method large datasets unstructured blog posts consisting k blog posts k associated images quantitative evaluation user studies show approach successful stateoftheart alternatives verbalizing image stream textimage parallel dataset blog posts discuss transform blog posts training set b imagetext parallel data streams l l sequence imagesentence pairs b l il tl l tn l b training set size denoted l b fig shows summary preprocessing steps blog posts blog preprocessing assume blog authors augment text multiple images semantically meaningful manner order decompose blog sequence images associated text first perform text segmentation text summarization purpose text segmentation divide input blog text set text segments associated single image thus number segments identical number images blog objective text summarization reduce text segment single key sentence result l l two processes transform blog form b l il tl l tn l text segmentation first divide blog passage text blocks according paragraphs apply standard paragraph tokenizer nltk [ ] uses rulebased regular expressions detect paragraph divisions use heuristics based imagetotext block distances proposed [ ] simply assign text block image minimum index distance text block image counted single index distance blog text summarization summarize text segment single key sentence apply latent semantic analysis lsa based summarization method [ ] uses singular value decomposition obtain concept dimension sentences recursively finds representative sentences maximize intersentence similarity topic text segment data augmentation data augmentation wellknown technique convolutional neural networks improve image classification accuracies [ ] basic idea artificially increase number training examples applying transformations horizontal reflection adding noise training images empirically observe idea leads better performance problem l l well imagesentence sequence b l il tl l tn l augment l sentence tn multiple sentences training perform lsabased text summarization select top highest ranked summary sentences among topranked one becomes summary sentence associated image top ones used training model slight abuse notation let tnl denote single summary sentence augmented sentences choose thorough empirical tests text description represent text segment sentences extract paragraph vector [ ] represent content text paragraph vector neuralnetwork based unsupervised algorithm learns fixedlength feature representation variablelength pieces passage learn dimensional dense vector representation separately two classes blog dataset using gensim docvec code use pn denote paragraph vector representation text tn extract parsed tree tn identify coreferent entities grammatical roles words use stanford core nlp library [ ] parse trees used local coherence model discussed section architecture many existing sentence generation models e g [ ] combine words phrases training data generate sentence novel image approach one level higher use sentences training database author sequence sentences novel image stream although model easily extended use words phrases basic building blocks granularity makes sequences long train language model may cause several difficulties learning rnn models example vanishing gradient effect wellknown hardship backpropagate error signal longrange temporal interval therefore design approach retrieves individual candidate sentences query image training database crafts best sentence sequence considering fitness individual imagetosentence pairs coherence consecutive sentences figure illustration preprocessing steps blog posts b proposed crcn architecture fig b illustrates structure crcn consists three main components convolutional neural networks cnn [ ] image representation bidirectional recurrent neural networks brnn [ ] sentence sequence modeling local coherence model [ ] smooth flow multiple sentences data stream variablelength sequence denoted tn use n denote position sentenceimage sequence define cnn brnn model position separately coherence model whole data stream cnn component choice vggnet [ ] represents images dimensional vectors discuss details brnn coherence model section section respectively finally present combine output three components create single compatibility score section brnn model role brnn model represent content flow text sequences problem brnn suitable normal rnn brnn simultaneously model forward backward streams allow us consider previous next sentences sentence make content whole sequence interact one another shown fig b brnn five layers input layer forwardbackward layer output layer relu activation layer finally merged coherence model two fully connected layers note text represented dimensional paragraph vector pt discussed section exact form brnn follows see fig b together better understanding xft f wif pt bfi hft f xft wf hft xbt f wib pt bbi bf hbt f xbt wb hbt bb ot wo hft hbt bo brnn takes sequence text vectors pt input compute xft xbt activations input units forward backward units unlike brnn models separate input activation forward backward ones different sets parameters wif wib empirically leads better performance set activation function f rectified linear unit relu f x max x create two independent forward backward hidden units denoted hft hbt final activation brnn ot regarded description content sentence location also implicitly encodes flow sentence surrounding context sequence parameter sets learn include weights wif wib wf wb wo r biases bfi bbi bf bb bo r local coherence model brnn model capture flow text content lacks learning coherence passage reflects distributional syntactic referential information discourse entities thus explicitly include local coherence model based work [ ] focuses resolving patterns local transitions discourse entities e coreferent noun phrases whole text shown fig b first extract parse trees every summarized text denoted zt concatenate sequenced parse trees one large one make entity grid whole sequence entity grid table row corresponds discourse entity column represents sentence grammatical role expressed three categories one absent e referenced sentence subjects objects x subject object absent making entity grid enumerate transitions grammatical roles entities whole text set history parameter three means obtain transition descriptions e g oox computing ratio occurrence frequency transition finally create dimensional representation captures coherence sequence finally make descriptor dimensional vector zeropadding forward relu layer done brnn output combination cnn rnn coherence model relu activation layers rnn coherence model output e ot n q goes two fully connected fc layers whose role decide proper combination brnn language factors coherence factors drop bias terms fullyconnected layers dimensions variables wf r wf r ot q r st g r rn rn [ ] [ sn ] wf wf [ q ] [ g ] use shared parameters q output mixes well interaction content flows coherency tests joint learning outperforms learning two terms separate parameters note multiplication wf wf last two fc layers reduce single linear mapping thanks dropout assign dropout rates two layers empirically improves generalization performance much single fc layer dropout training crcn train crcn model first define compatibility score image stream paragraph sequence score function inspired karpathy et al [ ] two major differences first score function [ ] deals sentence fragments image fragments thus algorithm considers combinations find best matching hand define score ordered paired compatibility sentence sequence image sequence second also add term measures relevance relation coherency image sequence text sequence finally score skl sentence sequence k image stream l defined skl x skt vtl g k vtl n vtl denotes cnn feature vector tth image stream l define cost function train crcn model follows [ ] c xhx k max skl skk l x max slk skk l skk denotes score training pair corresponding image sentence sequence objective based maxmargin structured loss encourages aligned imagesentence sequence pairs higher score margin misaligned pairs positive training example randomly sample ne examples training set since contrastive example random length sampled dataset wide range content extremely unlikely negative examples length content order sentences positive examples optimization use backpropagation time bptt algorithm [ ] train model apply stochastic gradient descent sgd minibatches data streams among many sgd techniques select rmsprop optimizer [ ] leads best performance experiments initialize weights crcn model using method et al [ ] robust deep rectified models observe better simple gaussian random initialization although model extremely deep use dropout regularization layers except brnn dropout last fc layer remaining layers retrieval sentence sequences test time objective retrieve best sentence sequence given query image stream iq iqn first select knearest images query image training database using ` distance cnn vggnet fc features [ ] experiments k successful generate set sentence sequence candidates c concatenating sentences associated knearest images location finally use learned crcn model compute compatibility score query image stream sequence candidate according rank candidates however one major difficulty scenario exponentially many candidates e c k n resolve issue use approximate divideandconquer strategy recursively halve problem subproblems size subproblem manageable example halve search candidate length q times search space subproblem q becomes k n using beam search idea first find topm best sequence candidates subproblem lowest level recursively increase candidate lengths maximum candidate size limited set though approximate search experiments assure achieves almost optimal solutions plausible combinatorial search mainly local fluency coherence undoubtedly necessary global one order whole sentence sequence fluent coherent subparts must well experiments compare performance approach stateoftheart candidate methods via quantitative measures user studies using amazon mechanical turk amt please refer supplementary material results details implementation experimental setting experimental setting dataset collect blog datasets two topics nyc disneyland reuse blog data disneyland dataset [ ] newly collect data nyc using crawling method [ ] first crawl blog posts associated pictures two popular blog publishing sites blogspot wordpress changing query terms google search manually select travelogue posts describe stories events multiple images finally dataset includes unique blog posts images nyc blog posts images disneyland task quantitative evaluation randomly split dataset training set validation others test set test post use image sequence query iq sequence summarized sentences groundtruth tg algorithm retrieves best sequences training database query image sequence ideally retrieved sequences match well tg since training test data disjoint algorithm retrieve similar identical sentences best quantitative measures exploit two types metrics language similarity e bleu [ ] cider [ ] meteor [ ] scores retrieval accuracies e topk recall median rank popularly used text generation literature [ ] topk recall rk recall rate groundtruth retrieval given top k candidates median rank indicates median ranking value first retrieved groundtruth better performance indicated higher bleu cider meteor rk scores lower median rank values baselines since sentence sequence generation image streams addressed yet previous research instead extend several stateoftheart singlesentence models publicly available codes baselines including logbilinear multimodal models kiros et al [ ] recurrent convolutional models karpathy et al [ ] vinyals et al [ ] [ ] use three variants introduced paper standard logbilinear model lbl two multimodal extensions modalitybased lbl mlblb factored threeway lbl mlblf use neuraltalk package authored karpathy et al baseline [ ] denoted cnnrnn [ ] denoted cnnlstm simplest baseline also compare global matching glomatch [ ] baselines create final sentence sequences concatenating sentences generated image query stream b b cnnlstm [ ] cnnrnn [ ] mlblf [ ] mlblb [ ] lbl [ ] glomatch [ ] nn rcn crcn cnnlstm [ ] cnnrnn [ ] mlblf [ ] mlblb [ ] lbl [ ] glomatch [ ] nn rcn crcn language metrics retrieval metrics b b cider meteor r r r medrank new york city disneyland table evaluation sentence generation two datasets new york city disneyland language similarity metrics bleu retrieval metrics rk median rank better performance indicated higher bleu cider meteor rk scores lower median rank values also compare different variants method validate contributions key components method test knearest search nn without rnn part simplest variant image test query find k similar training images simply concatenate associated sentences second variant brnnonly method denoted rcn excludes entitybased coherence model approach complete method denoted crcn comparison quantifies improvement coherence model fair use vggnet fc feature [ ] algorithms quantitative results table shows quantitative results experiments using language retrieval metrics approach crcn rcn outperform large margins stateoftheart baselines generate passages without consideration sentencetosentence transitions unlike mlblf shows best performance among three models [ ] albeit small margin partly share word dictionary training among mrnnbased models cnnlstm significantly outperforms cnnrnn lstm units help learn models irregular lengthy data natural blogs robustly also observe crcn outperforms nn rcn especially retrieval metrics shows integration two key components brnn coherence model indeed contributes performance improvement crcn slightly better rcn language metrics significantly better retrieval metrics means rcn fine retrieving fairly good solutions good ranking correct solution high compared crcn small margins language metrics also attributed inherent limitation example bleu focuses counting matches ngram words thus good comparing sentences even worse paragraphs fully evaluating fluency coherency fig illustrates several examples sentence sequence retrieval set show query image stream text results created method baselines except fig show parts sequences rather long illustration qualitative examples demonstrate approach successful verbalize image sequences include variety content user studies via amazon mechanical turk perform user studies using amt observe general users preferences text sequences different algorithms since evaluation involves multiple images long passages text design amt task sufficiently simple general turkers background knowledge figure examples sentence sequence retrieval nyc top disneyland bottom set present part query image stream corresponding text output method baseline baselines glomatch cnnlstm mlblb rcn rcn n nyc disneyland table results amt pairwise preference tests present percentages responses turkers vote crcn baselines length query streams except last column first randomly sample test streams two datasets first set maximum number images per query query longer uniformly sample amt test show query image stream iq pair passages generated method crcn one baseline random order ask turkers choose agreed text sequence iq design test pairwise comparison instead multiplechoice question make answering analysis easier questions look similar examples fig obtain answers three different turkers query compare four baselines choose mlblb among three variants [ ] cnnlstm among mrnnbased methods also select glomatch rcn variants method table shows results amt tests validate amt annotators prefer results baselines glomatch worst uses weak image representation e gist tiny images differences crcn rcn e th column table significant previous quantitative measures mainly query image stream sampled relatively short coherence becomes critical passage longer justify argument run another set amt tests use images per query shown last column table performance margins crcn rcn become larger lengths query image streams increase result assures passages longer coherence becomes important thus crcn output preferred turkers conclusion proposed approach retrieving sentence sequences image stream developed coherence recurrent convolutional network crcn consists convolutional networks bidirectional recurrent networks entitybased local coherence model quantitative evaluation users studies using amt large collections blog posts demonstrated crcn approach outperformed stateoftheart candidate methods acknowledgements research partially supported hancom basic science research program national research foundation korea rcaa references [ ] r barzilay lapata modeling local coherence entitybased approach acl [ ] bird e loper e klein natural language processing python oreilly media inc [ ] x chen c l zitnick minds eye recurrent visual representation image caption generation cvpr [ ] f choi p wiemerhastings j moore latent semantic analysis text segmentation emnlp [ ] j donahue l hendricks guadarrama rohrbach venugopalan k saenko darrell longterm recurrent convolutional networks visual recognition description cvpr [ ] gong l wang hodosh j hockenmaier lazebnik improving imagesentence embeddings using large weakly annotated photo collections eccv [ ] k x zhang ren j sun delving deep rectifiers surpassing humanlevel performance imagenet classification arxiv [ ] hodosh p young j hockenmaier framing image description ranking task data models evaluation metrics jair [ ] karpathy l feifei deep visualsemantic alignments generating image descriptions cvpr [ ] g kim moon l sigal joint photo stream blog post summarization exploration cvpr [ ] g kim moon l sigal ranking retrieval image sequences multiple paragraph queries cvpr [ ] r kiros r salakhutdinov r zemel multimodal neural language models icml [ ] krizhevsky sutskever g e hinton imagenet classification deep convolutional neural networks nips [ ] g kulkarni v premraj dhar li choi c berg l berg baby talk understanding generating image descriptions cvpr [ ] p kuznetsova v ordonez l berg choi treetalk composition compression trees image descriptions tacl [ ] b lavie meteor automatic metric mt evaluation improved correlation human judgments acl [ ] q le mikolov distributed representations sentences documents icml [ ] c manning surdeanu j bauer j finkel j bethard mcclosky stanford corenlp natural language processing toolkit acl [ ] j mao w xu yang j wang z huang l yuille deep captioning multimodal recurrent neural networks mrnn iclr [ ] mikolov statistical language models based neural networks ph thesis brno university technology [ ] v ordonez g kulkarni l berg imtext describing images using million captioned photographs nips [ ] k papineni roukos ward w j zhu bleu method automatic evaluation machine translation acl [ ] rohrbach w qiu titov thater pinkal b schiele translating video content natural language descriptions iccv [ ] schuster k k paliwal bidirectional recurrent neural networks ieee tsp [ ] k simonyan zisserman deep convolutional networks largescale image recognition iclr [ ] r socher karpathy q v le c manning ng grounded compositional semantics finding describing images sentences tacl [ ] n srivastava r salakhutdinov multimodal learning deep boltzmann machines nips [ ] tieleman g e hinton lecture rmsprop coursera [ ] r vedantam c l zitnick parikh cider consensusbased image description evaluation arxiv [ ] vinyals toshev bengio erhan show tell neural image caption generator cvpr [ ] p j werbos generalization backpropagation application recurrent gas market model neural networks [ ] r xu c xiong w chen j j corso jointly modeling deep video compositional text bridge vision language unified framework aaai')),\n",
       " ([(2, 0.991918)],\n",
       "  (5814,\n",
       "   'parallel correlation clustering big graphs xinghao pan dimitris papailiopoulos samet oymak benjamin recht kannan ramchandran michael jordan amplab eecs uc berkeley statistics uc berkeley abstract given similarity graph items correlation clustering cc groups similar items together dissimilar ones apart one popular cc algorithms kwikcluster algorithm serially clusters neighborhoods vertices obtains approximation ratio unfortunately practice kwikcluster requires large number clustering rounds potential bottleneck large graphs present c clusterwild two algorithms parallel correlation clustering run polylogarithmic number rounds provably achieve nearly linear speedups c uses concurrency control enforce serializability parallel clustering process guarantees approximation ratio clusterwild coordination free algorithm abandons consistency benefit better scaling leads provably small loss approximation ratio demonstrate experimentally algorithms outperform state art terms clustering accuracy running time show algorithms cluster billionedge graphs seconds cores achieving speedup introduction clustering items according notion similarity major primitive machine learning correlation clustering serves basic means achieve goal given similarity measure items goal group similar items together dissimilar items apart contrast clustering approaches number clusters determined priori good solutions aim balance tension grouping items together versus isolating simplest cc variant described complete signed graph input graph g n vertices weights edges similar items edges dissimilar ones goal generate partition vertices disjoint sets minimizes number disagreeing edges equals number edges cut clusters plus number edges inside clusters metric commonly called number disagreements figure give toy example cc instance cluster cluster cost edges inside clusters edges across clusters figure graph solid edges denote similarity dashed dissimilarity number disagreeing edges clustering clustering color bad edges red entity deduplication archetypal motivating example correlation clustering applications chat disentanglement coreference resolution spam detection [ ] input set entities say results keyword search pairwise classifier indicates errorsimilarities entities two results keyword search might refer item might look different come different sources building similarity graph entities applying cc hope cluster duplicate entities group context keyword search implies meaningful compact list results cc applied finding communities signed networks classifying missing edges opinion trust networks [ ] gene clustering [ ] consensus clustering [ ] kwikcluster simplest cc algorithm achieves provable approximation ratio [ ] works following way pick vertex v random cluster center create cluster v positive neighborhood n v e vertices connected v positive edges peel vertices associated edges graph repeat vertices clustered beyond theoretical guarantees experimentally kwikcluster performs well combined local heuristics [ ] kwikcluster seems like inherently sequential algorithm cases interest requires many peeling rounds happens small number vertices clustered per round bottleneck large graphs recently efforts develop scalable variants kwikcluster [ ] [ ] distributed peeling algorithm presented context mapreduce using elegant analysis authors establish approximation polylogarithmic number rounds algorithm employs simple step rejects vertices executed parallel conflicting however see experiments seemingly minor coordination step hinders scaleups parallel core setting [ ] sketch distributed algorithm presented algorithm achieves approximation kwikcluster logarithmic number rounds expectation however performs significant redundant work per iteration effort detect parallel vertices become cluster centers contributions present c clusterwild two parallel cc algorithms provable performance guarantees practice outperform state art terms running time clustering accuracy c parallel version kwikcluster uses concurrency control establish approximation ratio clusterwild simple implement coordinationfree algorithm abandons consistency benefit better scaling provably small loss approximation ratio c achieves approximation ratio polylogarithmic number rounds enforcing consistency concurrently running peeling threads consistency enforced using concurrency control notion extensively studied databases transactions recently used parallelize inherently sequential machine learning algorithms [ ] clusterwild coordinationfree parallel cc algorithm waives consistency favor speed cost pay arbitrarily small loss clusterwild accuracy show clusterwild achieves opt n log n approximation polylogarithmic number rounds provable nearly linear speedups main theoretical innovation clusterwild analyzing coordinationfree algorithm serial variant kwikcluster runs noisy graph experimental evaluation demonstrate algorithms gracefully scale graphs billions edges large graphs algorithms output valid clustering less seconds threads order magnitude faster kwikcluster observe unexpectedly clusterwild faster c quite surprisingly abandoning coordination parallel setting amounts relative loss clustering accuracy furthermore compare state art parallel cc algorithms showing consistently outperform algorithms terms running time clustering accuracy notation g denotes graph n vertices edges g complete edges denote dv positive degree vertex e number vertices connected v positive edges denotes positive maximum degree g n v denotes positive neighborhood v moreover let cv v n v two vertices u v termed friends u n v vice versa denote permutation n two parallel algorithms correlation clustering formal definition correlation clustering given correlation clustering given graph g n vertices partition vertices arbitrary number k disjoint subsets c ck sum negative edges within subsets plus sum positive edges across subsets minimized opt min kn min ci \\\\cj ij [ k ci n k x e ci ci k k x x ji e ci cj e e sets positive negative edges g kwikcluster remarkably simple algorithm approximately solves combinatorial problem operates follows random vertex v picked cluster cv created v positive neighborhood vertices cv peeled graph process repeated vertices clustered kwikcluster equivalently executed noted [ ] substitute random choice vertex per peeling round random order preassigned vertices see alg select random permutation vertices peel vertex indexed friends remove vertices cv repeat process order among vertices makes discussion parallel algorithms convenient c parallel cc using concurency control algorithm kwikcluster suppose wish run parallel version kwikcluster say two threads one thread random permutation n picks vertex v indexed v thread picks u indexed concurrently select vertex v indexed cv v n v vertices cluster centers remove clustered vertices g iff friends g v u con end nected positive edge vertex smallest order wins concurency rule assume v u friends g v u become cluster centers moreover assume v u common unclustered friend say w w clustered v u need follow would happen kwikcluster alg w go vertex smallest permutation number case v concurency rule following simple rules develop c serializable parallel cc algorithm since c constructs clusters kwikcluster given ordering inherits approximation idea identifying cluster centers rounds first used [ ] obtain parallel algorithm maximal independent set mis c shown alg starts assigning random permutation vertices samples active set n unclustered vertices sample taken prefix sampling p threads picks vertex smallest order checks vertex become cluster center first enforce concurrency rule adjacent vertices cannot cluster centers time c enforces making thread check friends vertex say v picked thread check attemptcluster whether vertex v preceding friends cluster centers none go ahead label v cluster center proceed creating cluster preceding friend v cluster center v labeled cluster center preceding friend v call u yet received label e u currently processed yet labeled cluster center thread processing v wait u receive label major technical detail showing wait time bounded show log n threads conflict time using new subgraph sampling lemma [ ] since c serializable respect concurrency rule vertex u adjacency two cluster centers gets assigned one smaller permutation order accomplished createcluster processing vertices threads synchronized bulk clustered vertices removed new active set sampled process repeated everything clustered following section present theoretical guarantees c algorithm c clusterwild createcluster v clusterid v v u v \\\\ clusterid u min clusterid u v end input g clusterid clusterid n random permutation n v attemptcluster v maximum vertex degree g v clusterid u iscenter v first n vertices v [ ] createcluster v parallel end v first element v iscenter v c concurrency control u v check friends order attemptcluster v u v precede wait else clusterwild coordination free wait clusterid u till clustered createcluster v iscenter u end return friend center cant end end remove clustered vertices v end end end output clusterid clusterid n return earlier friends centers clusterwild coordinationfree correlation clustering clusterwild speeds computation ignoring first concurrency rule uniformly samples unclustered vertices builds clusters around without respecting rule cluster centers cannot friends g clusterwild threads bypass attemptcluster routine eliminates waiting part c clusterwild samples set vertices prefix thread picks first ordered vertex remaining using vertex cluster center creates cluster around peels away clustered vertices repeats process next remaining vertex end processing vertices threads synchronized bulk clustered vertices removed new active set sampled parallel clustering repeated careful analysis along lines [ ] shows number rounds e bulk synchronization steps polylogarithmic quite unsurprisingly clusterwild faster c interestingly abandoning consistency incur much loss approximation ratio show error introduced accuracy solution bounded characterize error theoretically show practice translates relative loss objective main intuition clusterwild introduce much error chance two randomly selected vertices friends small hence concurrency rules infrequently broken theoretical guarantees section bound number rounds required algorithms establish theoretical speedup one obtain p parallel threads proceed present approximation guarantees would like remind reader thatas relevant literaturewe consider graphs complete signed unweighted omitted proofs found appendix number rounds running time analysis follows [ ] [ ] main idea track fast maximum degree decreases remaining graph end round lemma c clusterwild terminate log n log rounds w h p analyze running time algorithms simplified bsp model main idea running time super step e round determined straggling thread e one gets assigned amount work plus time needed synchronization end round assumption assume threads operate asynchronously within round synchronize end round memory cell writtenread concurrently multiple threads time spent per round algorithm proportional time slowest thread cost thread synchronization end batch takes time p p number threads total computation cost proportional sum time spent rounds plus time spent bulk synchronization step simplified model show algorithms obtain nearly linear speedup clusterwild faster c precisely due lack coordination main tool analyzing c recent graphtheoretic result [ ] theorem guarantees one samples n subset vertices graph sampled subgraph connected component size log n combining appendix show following result theorem theoretical running time c p cores upper bounded mn log n p log n log long number cores p smaller mini nii p nii size batch ith round algorithm running time clusterwild p cores upper bounded mn p log n log p approximation ratio proceed establishing approximation ratios c clusterwild c serializable straightforward c obtains precisely approximation ratio kwikcluster one simply show permutation kwikcluster c output clustering indeed true two simple concurrency rules mentioned previous section sufficient c equivalent kwikcluster theorem c achieves approximation ratio expectation clusterwild serial procedure noisy graph analyzing clusterwild bit involved guarantees based fact clusterwild treated one running peeling algorithm noisy graph since adjacent active vertices still become cluster centers clusterwild one view edges deleted somewhat unconventional adversary analyze new noisy graph establish theoretical result theorem clusterwild achieves opto nlog n approximation expectation provide sketch proof delegate details appendix since clusterwild ignores edges among active vertices treat edges deleted main result quantify loss clustering accuracy caused ignoring edges proceed define bad triangles combinatorial structure used measure clustering quality peeling algorithm definition bad triangle g set three vertices two pairs joined positive edge one pair joined negative edge let tb denote set bad triangles g quantify cost clusterwild make observation lemma cost greedy algorithm picks vertex v irrespective sampling order creates cv peels away repeats equal number bad triangles adjacent cluster center v lemma let g denote random graph induced deleting edges active vertices per round given run clusterwild let new denote number additional bad triangles thatp g compared g expected cost clusterwild upper bounded e ttb pt new pt event triangle end points j k bad least one end points becomes active still part original unclustered graph proof begin bounding second term e new considering number new bad triangles newi created round e newi x uv e p u v ai n u [ n v x uv e e n using result clusterwild terminates log n log rounds get e new n log n p p left bound e ttb pt ttb pt use following lemma p p pt lemma pt satisfies e tettb ttb pt op proof let b one many sets thatpattribute thep cost optimal p possibly p p edges pt pt pt algorithm opt eb eb tettb ttb b \\\\ ttb z [ ] p simply boundsthe expectation bad triangles adjacent edge u v uv ttb pt let suv uv ttb union sets nodes bad triangles contain vertices u v observe w s\\\\ u v becomes active u v cost e cost bad triangle u v w incurred hand either u v selected pivots round cuv high e equal bad triangles containing edge u v let auv u v activated vertices suv c e [ cuv ] e [ cuv auv ] p auv e cuv ac uv p auv p u v \\\\ \\\\ p v \\\\ \\\\ last inequality obtained union bound u v bound following probability p v \\\\ p v p \\\\ v p v p \\\\ p \\\\ p v p \\\\ observe p v hence need upper bound p \\\\ probability per round positive neighbors become activated upper bounded np np np n p p p p n n n n e p hence probability upper bounded p v \\\\ \\\\ e know also n hence p e cuv exp ttb pt new overall expectation bounded e e opt n ln n log approximation ratio clusterwild opt n log n establishes bsp algorithms proxy asynchronous algorithms would like note analysis bsp model useful proxy performance completely asynchronous variants algorithms specifically see alg remove synchronization barriers difference asynchronous execution alg compared alg complete lack bulk synchronization end processing active set although analysis bsp variants algorithms tractable unfortunately analyzing precisely speedup asynchronous c approximation guarantees asynchronous clusterwild challenging however experimental section test completely asynchronous algorithms bsp algorithms previous section observe perform quite similarly terms accuracy clustering running times skip constants simplify presentation however smaller related work correlation clustering formally introduced bansal et al [ ] general case minimizing disagreements nphard hard approximate within arbitrarily small constant apxhard [ ] two variations problem cc complete graphs edges present weights ii cc general graphs arbitrary edge weights problems hard however general graph setup seems fundamentally harder best known approximation ratio latter log n reduction minimum multicut problem indicates improvement requires fundamental breakthroughs theoretical algorithms [ ] algorithm c clusterwild asynchronous execution input g clusterid clusterid n random permutation n v v first element v v v v c concurrency control attemptcluster v else clusterwild coordination free createcluster v end remove clustered vertices v end output clusterid clusterid n case complete unweighted graphs long series results establishes approximation via rounded linear program lp [ ] recent result establishes approximation using elegant rounding lp relaxation [ ] avoiding expensive lp using rounding procedure [ ] basis greedy algorithm yields kwikcluster approximation cc complete unweighted graphs variations cost metric cc change algorithmic landscape maximizing agreements dual measure disagreements [ ] maximizing difference number agreements disagreements [ ] come different hardness approximation results also several variants chromatic cc [ ] overlapping cc [ ] small number clusters cc added constraints suitable biology applications [ ] way c finds cluster centers seen variation mis algorithm [ ] main difference case passively detect mis locking memory variables waiting preceding ordered threads means vertex pushes cluster id status cluster centerclusteredunclustered friends versus pulling asking friends cluster status saves substantial amount computational effort experiments parallel algorithms implemented scalawe defer full discussion implementation details appendix c ran experiments amazon ecs r xlarge vcpus gb memory instances using threads real graphs listed table graph dblp enwiki uk webbase vertices edges description dblp coauthorship network [ ] link graph english part wikipedia [ ] crawl uk domain [ ] crawl domain [ ] crawl webbase crawler [ ] table graphs used evaluation parallel algorithms tested different random orderings measured runtimes speedups ratio runtime thread runtime p threads objective values obtained parallel algorithms comparison also implemented algorithm presented [ ] denote cdk short values used c bsp clusterwild bsp cdk interest space present representative plots results full results given appendix code available httpsgithub compxinghaoparallelcorrelationclustering cdk tested smaller graphs dblp enwiki cdk prohibitively slow often orders magnitude slower c clusterwild even kwikcluster mean runtime uk mean runtime ms mean runtime ms mean runtime serial c c bsp cw cw bsp mean speedup webbase serial c c bsp cw cw bsp ideal c c bsp cw cw bsp speedup number threads mean runtimes uk blocked vertices number rounds mean number synchronization rounds bsp algorithms number threads number threads c mean speedup webbase objective value relative serial dblp c bsp min c bsp mean c bsp max c bsp min c bsp mean c bsp max blocked vertices enwiki ccw bsp uk ccw bsp ccw bsp webbase ccw bsp dblp cdk dblp ccw bsp enwiki cdk enwiki number threads b mean runtimes mean number rounds algo obj value serial obj value e percent blocked vertices c enwiki bsp run cw bsp mean cw bsp median cw mean cw median cdk mean cdk median number threads f median objective values dblp cw bsp cdk run figure figures cw short clusterwild bsp short bulksynchronous variants parallel algorithms short asynchronous variants runtimes speedups c clusterwild initially slower serial due overheads required atomic operations parallel setting however parallel algorithms outperform kwikcluster threads threads added asychronous variants become faster bsp counterparts synchronization barrriers difference bsp asychronous variants greater smaller clusterwild also always faster c since coordination overheads asynchronous algorithms able achieve speedup x threads bsp algorithms poorer speedup ratio nevertheless achieve x speedup synchronization rounds main overhead bsp algorithms lies need synchronization rounds increases amount synchronization decreases algorithms less synchronization rounds small considering size graphs multicore setting blocked vertices additionally c incurs overhead number vertices blocked waiting earlier vertices complete note overhead extremely small practice graphs less vertices blocked larger sparser graphs drops less e vertices objective value design c algorithms also return output thus objective value kwikcluster find clusterwild bsp worse serial across graphs values behavior asynchronous clusterwild worsens threads added reaching worse serial one graphs finally smaller graphs able test cdk cdk returns worse median objective value clusterwild variants conclusions future directions paper presented two parallel algorithms correlation clustering nearly linear speedups provable approximation ratios overall two approaches support otherwhen c relatively fast relative clusterwild may prefer c guarantees accuracy clusterwild accurate relative c may prefer clusterwild speed future intend implement algorithms distributed environment synchronization communication often account highest cost c clusterwild wellsuited distributed environment since polylogarithmic number rounds references [ ] ahmed k elmagarmid panagiotis g ipeirotis vassilios verykios duplicate record detection survey knowledge data engineering ieee transactions [ ] arvind arasu christopher dan suciu largescale deduplication constraints using dedupalog data engineering icde ieee th international conference pages ieee [ ] micha elsner warren schudy bounding comparing methods correlation clustering beyond ilp proceedings workshop integer linear programming natural langauge processing pages association computational linguistics [ ] bilal hussain oktie hassanzadeh fei chiang hyun chul lee renee j miller evaluation clustering algorithms duplicate detection technical report [ ] francesco bonchi david garciasoriano edo liberty correlation clustering theory practice proceedings th acm sigkdd international conference knowledge discovery data mining pages acm [ ] flavio chierichetti nilesh dalvi ravi kumar correlation clustering mapreduce proceedings th acm sigkdd international conference knowledge discovery data mining pages acm [ ] bo yang william k cheung jiming liu community mining signed social networks knowledge data engineering ieee transactions [ ] n cesabianchi c gentile f vitale g zappella et al correlation clustering approach link classification signed networks annual conference learning theory pages microtome [ ] amir bendor ron shamir zohar yakhini clustering gene expression patterns journal computational biology [ ] nir ailon moses charikar alantha newman aggregating inconsistent information ranking clustering journal acm jacm [ ] xinghao pan joseph e gonzalez stefanie jegelka tamara broderick michael jordan optimistic concurrency control distributed unsupervised learning advances neural information processing systems pages [ ] guy e blelloch jeremy fineman julian shun greedy sequential maximal independent set matching parallel average proceedings twentyfourth annual acm symposium parallelism algorithms architectures pages acm [ ] michael krivelevich phase transition site percolation pseudorandom graphs arxiv preprint arxiv [ ] nikhil bansal avrim blum shuchi chawla correlation clustering ieee th annual symposium foundations computer science pages ieee computer society [ ] moses charikar venkatesan guruswami anthony wirth clustering qualitative information foundations computer science proceedings th annual ieee symposium pages ieee [ ] erik demaine dotan emanuel amos fiat nicole immorlica correlation clustering general weighted graphs theoretical computer science [ ] shuchi chawla konstantin makarychev tselil schramm grigory yaroslavtsev near optimal lp rounding algorithm correlation clustering complete complete kpartite graphs proceedings fortyseventh annual acm symposium theory computing stoc pages [ ] chaitanya swamy correlation clustering maximizing agreements via semidefinite programming proceedings fifteenth annual acmsiam symposium discrete algorithms pages society industrial applied mathematics [ ] ioannis giotis venkatesan guruswami correlation clustering fixed number clusters proceedings seventeenth annual acmsiam symposium discrete algorithm pages acm [ ] moses charikar anthony wirth maximizing quadratic programs extending grothendiecks inequality foundations computer science proceedings th annual ieee symposium pages ieee [ ] noga alon konstantin makarychev yury makarychev assaf naor quadratic forms graphs inventiones mathematicae [ ] francesco bonchi aristides gionis francesco gullo antti ukkonen chromatic correlation clustering proceedings th acm sigkdd international conference knowledge discovery data mining pages acm [ ] francesco bonchi aristides gionis antti ukkonen overlapping correlation clustering data mining icdm ieee th international conference pages ieee [ ] gregory j puleo olgica milenkovic correlation clustering constrained cluster sizes extended weights bounds arxiv preprint arxiv [ ] p boldi vigna webgraph framework compression techniques www [ ] p boldi rosa santini vigna layered label propagation multiresolution coordinatefree ordering compressing social networks www acm press [ ] p boldi b codenotti santini vigna ubicrawler scalable fully distributed web crawler software practice experience'))]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, j) for i,j in zip(lda_corpus,documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-3aa5687ce7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcluster1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcluster2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcluster3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-3aa5687ce7b5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcluster1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcluster2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcluster3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print\n",
    "\n",
    "cluster1 = [j[1][0] for i,j in zip(lda_corpus,documents) if i[0][1] > threshold]\n",
    "cluster2 = [j[1][0] for i,j in zip(lda_corpus,documents) if i[1][1] > threshold]\n",
    "cluster3 = [j[1][0] for i,j in zip(lda_corpus,documents) if i[2][1] > threshold]\n",
    "\n",
    "print (cluster1)\n",
    "print (cluster2)\n",
    "print (cluster3)\n",
    "\n",
    "lda.show_topic(1, topn = 5)\n",
    "\n",
    "lda.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.172*\"user\" + 0.171*\"response\" + 0.170*\"time\" + 0.104*\"system\" + 0.101*\"computer\" + 0.101*\"survey\" + 0.037*\"trees\" + 0.031*\"graph\" + 0.030*\"interface\" + 0.029*\"minors\"')\n",
      "(1, '0.260*\"system\" + 0.184*\"eps\" + 0.109*\"user\" + 0.108*\"human\" + 0.108*\"interface\" + 0.039*\"trees\" + 0.038*\"graph\" + 0.032*\"minors\" + 0.032*\"time\" + 0.030*\"survey\"')\n",
      "(2, '0.202*\"graph\" + 0.196*\"trees\" + 0.144*\"minors\" + 0.084*\"human\" + 0.084*\"survey\" + 0.083*\"computer\" + 0.083*\"interface\" + 0.028*\"system\" + 0.025*\"user\" + 0.024*\"eps\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "\n",
    "\"\"\" DEMO \"\"\"\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "texts = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "# Create Dictionary.\n",
    "id2word = corpora.Dictionary(texts)\n",
    "# Creates the Bag of Word corpus.\n",
    "mm = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Trains the LDA models.\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=3, \\\n",
    "                               update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "# Prints the topics.\n",
    "for top in lda.print_topics():\n",
    "  print(top)\n",
    "print\n",
    "\n",
    "# Assigns the topics to the documents in corpus\n",
    "lda_corpus = lda[mm]\n",
    "\n",
    "# Find the threshold, let's set the threshold to be 1/#clusters,\n",
    "# To prove that the threshold is sane, we average the sum of all probabilities:\n",
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "threshold = sum(scores)/len(scores)\n",
    "#print threshold\n",
    "print\n",
    "\n",
    "cluster1 = [j for i,j in zip(lda_corpus,documents) if i[0][1] > threshold]\n",
    "cluster2 = [j for i,j in zip(lda_corpus,documents) if i[1][1] > threshold]\n",
    "cluster3 = [j for i,j in zip(lda_corpus,documents) if i[2][1] > threshold]\n",
    "\n",
    "#print cluster1\n",
    "#print cluster2\n",
    "#print cluster3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.090562195), (1, 0.10387861), (2, 0.8055592)],\n",
       " [(0, 0.8963971), (1, 0.0537468), (2, 0.049856074)],\n",
       " [(0, 0.073913366), (1, 0.85719573), (2, 0.06889087)],\n",
       " [(0, 0.06846908), (1, 0.8627082), (2, 0.068822704)],\n",
       " [(0, 0.8304014), (1, 0.08592879), (2, 0.08366977)],\n",
       " [(0, 0.16822203), (1, 0.1682373), (2, 0.6635407)],\n",
       " [(0, 0.11188998), (1, 0.1120651), (2, 0.7760449)],\n",
       " [(0, 0.083892055), (1, 0.08400609), (2, 0.8321019)],\n",
       " [(0, 0.08963511), (1, 0.08413829), (2, 0.8262266)]]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human machine interface for lab abc computer applications',\n",
       " 'A survey of user opinion of computer system response time',\n",
       " 'The EPS user interface management system',\n",
       " 'System and human system engineering testing of EPS',\n",
       " 'Relation of user perceived response time to error measurement',\n",
       " 'The generation of random binary unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV Widths of trees and well quasi ordering',\n",
       " 'Graph minors A survey']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for i,j in zip(lda_corpus,documents) if i[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
