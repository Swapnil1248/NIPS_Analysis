,Id,Title,EventType,PdfName,Abstract,PaperText
0,5677,Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing,Poster,5677-double-or-nothing-multiplicative-incentive-mechanisms-for-crowdsourcing.pdf,"Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible  mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers.  Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit.  In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.","Double or Nothing: Multiplicative
Incentive Mechanisms for Crowdsourcing
Nihar B. Shah
University of California, Berkeley
nihar@eecs.berkeley.edu

Dengyong Zhou
Microsoft Research
dengyong.zhou@microsoft.com

Abstract
Crowdsourcing has gained immense popularity in machine learning applications
for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but
suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize
workers to answer only the questions that they are sure of and skip the rest. We
show that surprisingly, under a mild and natural “no-free-lunch” requirement, this
mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms
(that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a
“multiplicative” form. The simplicity of the mechanism is an added benefit. In
preliminary experiments involving over several hundred workers, we observe a
significant reduction in the error rates under our unique mechanism for the same
or lower monetary expenditure.

1

Introduction

Complex machine learning tools such as deep learning are gaining increasing popularity and are
being applied to a wide variety of problems. These tools, however, require large amounts of labeled
data [HDY+ 12, RYZ+ 10, DDS+ 09, CBW+ 10]. These large labeling tasks are being performed by
coordinating crowds of semi-skilled workers through the Internet. This is known as crowdsourcing.
Crowdsourcing as a means of collecting labeled training data has now become indispensable to the
engineering of intelligent systems.
Most workers in crowdsourcing are not experts. As a consequence, labels obtained from crowdsourcing typically have a significant amount of error [KKKMF11, VdVE11, WLC+ 10]. Recent
efforts have focused on developing statistical techniques to post-process the noisy labels in order
to improve its quality (e.g., [RYZ+ 10, ZLP+ 15, KOS11, IPSW14]). However, when the inputs to
these algorithms are erroneous, it is difficult to guarantee that the processed labels will be reliable
enough for subsequent use by machine learning or other applications. In order to avoid “garbage in,
garbage out”, we take a complementary approach to this problem: cleaning the data at the time of
collection.
We consider crowdsourcing settings where the workers are paid for their services, such as in the
popular crowdsourcing platforms of Amazon Mechanical Turk and others. These commercial platforms have gained substantial popularity due to their support for a diverse range of tasks for machine
learning labeling, varying from image annotation and text recognition to speech captioning and machine translation. We consider problems that are objective in nature, that is, have a definite answer.
Figure 1a depicts an example of such a question where the worker is shown a set of images, and for
each image, the worker is required to identify if the image depicts the Golden Gate Bridge.
1

Is this the Golden Gate Bridge?

Is this the Golden Gate Bridge?

Yes!

Yes!

No!

No

I’m not sure

(b)!

(a)!

Figure 1: Different interfaces in a crowdsourcing setup: (a) the conventional interface, and (b) with
an option to skip.
Our approach builds on the simple insight that in typical crowdsourcing setups, workers are simply
paid in proportion to the amount of tasks they complete. As a result, workers attempt to answer
questions that they are not sure of, thereby increasing the error rate of the labels. For the questions
that a worker is not sure of, her answers could be very unreliable [WLC+ 10, KKKMF11, VdVE11,
JSV14]. To ensure acquisition of only high-quality labels, we wish to encourage the worker to
skip the questions about which she is unsure, for instance, by providing an explicit “I’m not sure”
option for every question (see Figure 1b). Our goal is to develop payment mechanisms to encourage
the worker to select this option when she is unsure. We will term any payment mechanism that
incentivizes the worker to do so as “incentive compatible”.
In addition to incentive compatibility, preventing spammers is another desirable requirement from
incentive mechanisms in crowdsourcing. Spammers are workers who answer randomly without
regard to the question being asked, in the hope of earning some free money, and are known to exist
in large numbers on crowdsourcing platforms [WLC+ 10, Boh11, KKKMF11, VdVE11]. It is thus
of interest to deter spammers by paying them as low as possible. An intuitive objective, to this end,
is to ensure a zero expenditure on spammers who answer randomly. In this paper, however, we
impose a strictly and significantly weaker condition, and then show that there is one and only one
incentive-compatible mechanism that can satisfy this weak condition. Our requirement, referred to
as the “no-free-lunch” axiom, says that if all the questions attempted by the worker are answered
incorrectly, then the payment must be zero.
We propose a payment mechanism for the aforementioned setting (“incentive compatibility” plus
“no-free-lunch”), and show that surprisingly, this is the only possible mechanism. We also show that
additionally, our mechanism makes the smallest possible payment to spammers among all possible
incentive compatible mechanisms that may or may not satisfy the no-free-lunch axiom. Our payment
mechanism takes a multiplicative form: the evaluation of the worker’s response to each question is
a certain score, and the final payment is a product of these scores. This mechanism has additional
appealing features in that it is simple to compute, and is also simple to explain to the workers. Our
mechanism is applicable to any type of objective questions, including multiple choice annotation
questions, transcription tasks, etc.
In order to test whether our mechanism is practical, and to assess the quality of the final labels
obtained, we conducted experiments on the Amazon Mechanical Turk crowdsourcing platform. In
our preliminary experiments that involved over several hundred workers, we found that the quality
of data improved by two-fold under our unique mechanism, with the total monetary expenditure
being the same or lower as compared to the conventional baseline.

2

Problem Setting

In the crowdsourcing setting that we consider, one or more workers perform a task, where a task
consists of multiple questions. The questions are objective, by which we mean, each question has
precisely one correct answer. Examples of objective questions include multiple-choice classification
questions such as Figure 1, questions on transcribing text from audio or images, etc.
For any possible answer to any question, we define the worker’s confidence about an answer as the
probability, according to her belief, of this answer being correct. In other words, one can assume
that the worker has (in her mind) a probability distribution over all possible answers to a question,
and the confidence for an answer is the probability of that answer being correct. As a shorthand, we
also define the confidence about a question as the confidence for the answer that the worker is most
2

confident about for that question. We assume that the worker’s confidences for different questions
are independent. Our goal is that for every question, the worker should be incentivized to:
1. skip if the confidence is below a certain pre-defined threshold, otherwise:
2. select the answer that she thinks is most confident about.
More formally, let T 2 (0, 1) be a predefined value. The goal is to design payment mechanisms that
incentivize the worker to skip the questions for which her confidence is lower than T , and attempt
those for which her confidence is higher than T . 1 Moreover, for the questions that she attempts to
answer, she must be incentivized to select the answer that she believes is most likely to be correct.
The threshold T may be chosen based on various factors of the problem at hand, for example, on
the downstream machine learning algorithms using the crowdsourced data, or the knowledge of the
statistics of worker abilities, etc. In this paper we assume that the threshold T is given to us.
Let N denote the total number of questions in the task. Among these, we assume the existence of
some “gold standard” questions, that is, a set of questions whose answers are known to the requester.
Let G (1  G  N ) denote the number of gold standard questions. The G gold standard questions
are assumed to be distributed uniformly at random in the pool of N questions (of course, the worker
does not know which G of the N questions form the gold standard). The payment to a worker for
a task is computed after receiving her responses to all the questions in the task. The payment is
based on the worker’s performance on the gold standard questions. Since the payment is based on
known answers, the payments to different workers do not depend on each other, thereby allowing us
to consider the presence of only one worker without any loss in generality.
We will employ the following standard notation. For any positive integer K, the set {1, . . . , K} is
denoted by [K]. The indicator function is denoted by 1, i.e., 1{z} = 1 if z is true, and 0 otherwise.
The notation R+ denotes the set of all non-negative real numbers.
Let x1 , . . . , xG 2 { 1, 0, +1} denote the evaluations of the answers that the worker gives to the G
gold standard questions. Here, “0” denotes that the worker skipped the question, “ 1” denotes that
the worker attempted to answer the question and that answer was incorrect, and “+1” denotes that
the worker attempted to answer the question and that answer was correct. Let f : { 1, 0, +1}G !
R+ denote the payment function, namely, a function that determines the payment to the worker
based on these evaluations x1 , . . . , xG . Note that the crowdsourcing platforms of today mandate the
payments to be non-negative. We will let µ (> 0) denote the budget, i.e., the maximum amount that
can be paid to any individual worker for this task:
max f (x1 , . . . , xG ) = µ.
x1 ,...,xG

The amount µ is thus the amount of compensation paid to a perfect agent for her work. We will
assume this budget condition of µ throughout the rest of the paper.
We assume that the worker attempts to maximize her overall expected payment. In what follows, the
expression ‘the worker’s expected payment’ will refer to the expected payment from the worker’s
point of view, and the expectation will be taken with respect to the worker’s confidences about her
answers and the uniformly random choice of the G gold standard questions among the N questions
in the task. For any question i 2 [N ], let yi = 1 if the worker attempts question i, and set yi = 0
otherwise. Further, for every question i 2 [N ] such that yi 6= 0, let pi be the confidence of the
worker for the answer she has selected for question i, and for every question i 2 [N ] such that
yi = 0, let pi 2 (0, 1) be any arbitrary value. Let E = (✏1 , . . . , ✏G ) 2 { 1, 1}G . Then from the
worker’s perspective, the expected payment for the selected answers and confidence-levels is
!
G
X
X
Y
1+✏i
1 ✏i
1
f (✏1 yj1 , . . . , ✏G yjG ) (pji ) 2 (1 pji ) 2
.
N
G

i=1

(j1 ,...,jG ) E2{ 1,1}G
✓{1,...,N }

In the expression above, the outermost summation corresponds to the expectation with respect to the
randomness arising from the unknown choice of the gold standard questions. The inner summation
corresponds to the expectation with respect to the worker’s beliefs about the correctness of her
responses.
1
In the event that the confidence about a question is exactly equal to T , the worker may be equally incentivized to answer or skip.

3

We will call any payment function f as an incentive-compatible mechanism if the expected payment
of the worker under this payment function is strictly maximized when the worker responds in the
manner desired.2

3

Main results: Incentive-compatible mechanism and guarantees

In this section, we present the main results of the paper, namely, the design of incentive-compatible
mechanisms with practically useful properties. To this end, we impose the following natural requirement on the payment function f that is motivated by the practical considerations of budget
constraints and discouraging spammers and miscreants [Boh11, KKKMF11, VdVE11, WLC+ 10].
We term this requirement as the “no-free-lunch axiom”:
Axiom 1 (No-free-lunch axiom). If all the answers attempted by the worker in the gold standard are
wrong, then the payment is zero. More formally, for every set of evaluations (x1 , . . . , xG ) that satisfy
PG
PG
0 < i=1 1{xi 6= 0} = i=1 1{xi = 1}, we require the payment to satisfy f (x1 , . . . , xG ) = 0.

Observe that no-free-lunch is an extremely mild requirement. In fact, it is significantly weaker than
imposing a zero payment on workers who answer randomly. For instance, if the questions are of
binary-choice format, then randomly choosing among the two options for each question would result
in 50% of the answers being correct in expectation, while the no-free-lunch axiom is applicable only
when none of them turns out to be correct.
3.1

Proposed “Multiplicative” Mechanism

We now present our proposed payment mechanism in Algorithm 1.
Algorithm 1 “Multiplicative” incentive-compatible mechanism
• Inputs: Threshold T , Budget µ, Evaluations (x1 , . . . , xG ) 2 { 1, 0, +1}G of the worker’s answers to the G gold standard questions
PG
PG
• Let C = i=1 1{xi = 1} and W = i=1 1{xi = 1}
• The payment is

f (x1 , . . . , xG ) = µT G

C

1{W = 0}.

The proposed mechanism has a multiplicative form: each answer in the gold standard is given a
score based on whether it was correct (score = T1 ), incorrect (score = 0) or skipped (score = 1),
and the final payment is simply a product of these scores (scaled by µ). The mechanism is easy to
describe to workers: For instance, if T = 12 , G = 3 and µ = 80 cents, then the description reads:
“The reward starts at 10 cents. For every correct answer in the 3 gold standard questions,
the reward will double. However, if any of these questions are answered incorrectly, then
the reward will become zero. So please use the ‘I’m not sure’ option wisely.”
Observe how this payment rule is similar to the popular ‘double or nothing’ paradigm [Dou14].
The algorithm makes a zero payment if one or more attempted answers in the gold standard are
wrong. Note that this property is significantly stronger than the property of no-free-lunch which
we originally required, where we wanted a zero payment only when all attempted answers were
wrong. Surprisingly, as we prove shortly, Algorithm 1 is the only incentive-compatible mechanism
that satisfies no-free-lunch.
The following theorem shows that the proposed payment mechanism indeed incentivizes a worker
to skip the questions for which her confidence is below T , while answering those for which her
confidence is greater than T . In the latter case, the worker is incentivized to select the answer which
she thinks is most likely to be correct.
Theorem 1. The payment mechanism of Algorithm 1 is incentive-compatible and satisfies the nofree-lunch condition.
2
Such a payment function that is based on gold standard questions is also called a “strictly proper scoring
rule” [GR07].

4

The proof of Theorem 1 is presented in Appendix A. It is easy to see that the mechanism satisfies nofree-lunch. The proof of incentive compatibility is also not hard: We consider any arbitrary worker
(with arbitrary belief distributions), and compute the expected payment for that worker for the case
when her choices in the task follow the requirements. We then show that any other choice leads to a
strictly smaller expected payment.
While we started out with a very weak condition of no-free-lunch of making a zero payment when
all attempted answers are wrong, the mechanism proposed in Algorithm 1 is significantly more
strict and makes a zero payment when any of the attempted answers is wrong. A natural question
that arises is: can we design an alternative mechanism satisfying incentive compatibility and nofree-lunch that operates somewhere in between?
3.2

Uniqueness of the Mechanism

In the previous section we showed that our proposed multiplicative mechanism is incentive compatible and satisfies the intuitive requirement of no-free-lunch. It turns out, perhaps surprisingly, that
this mechanism is unique in this respect.
Theorem 2. The payment mechanism of Algorithm 1 is the only incentive-compatible mechanism
that satisfies the no-free-lunch condition.
Theorem 2 gives a strong result despite imposing very weak requirements. To see this, recall our earlier discussion on deterring spammers, that is, incurring a low expenditure on workers who answer
randomly. For instance, when the task comprises binary-choice questions, one may wish to design
mechanisms which make a zero payment when the responses to 50% or more of the questions in the
gold standard are incorrect. The no-free-lunch axiom is a much weaker requirement, and the only
mechanism that can satisfy this requirement is the mechanism of Algorithm 1.
The proof of Theorem 2 is available in Appendix B. The proof relies on the following key lemma
that establishes a condition that any incentive-compatible mechanism must necessarily satisfy. The
lemma applies to any incentive-compatible mechanism and not just to those satisfying no-free-lunch.
Lemma. Any incentive-compatible payment mechanism f must satisfy, for every i 2 {1, . . . , G}
and every (y1 , . . . , yi 1 , yi+1 , . . . , yG ) 2 { 1, 0, 1}G 1 ,
T f (y1 , . . . , yi

1 , 1, yi+1 , . . . , yG )

+ (1

T )f (y1 , . . . , yi 1 , 1, yi+1 , . . . , yG )
= f (y1 , . . . , yi 1 , 0, yi+1 , . . . , yG ).

The proof of this lemma is provided in Appendix C. Given this lemma, the proof of Theorem 2 is
then completed via an induction on the number of skipped questions.
3.3

Optimality against Spamming Behavior

As discussed earlier, crowdsouring tasks, especially those with multiple choice questions, often
encounter spammers who answer randomly without heed to the question being asked. For instance,
under a binary-choice setup, a spammer will choose one of the two options uniformly at random for
every question. A highly desirable objective in crowdsourcing settings is to deter spammers. To this
end, one may wish to impose a condition of zero payment when the responses to 50% or more of
the attempted questions in the gold standard are incorrect. A second desirable metric could be to
minimize the expenditure on a worker who simply skips all questions. While the aforementioned
requirements were deterministic functions of the worker’s responses, one may alternatively wish to
impose requirements that depend on the distribution of the worker’s answering process. For instance,
a third desirable feature would be to minimize the expected payment to a worker who answers all
questions uniformly at random. We now show that interestingly, our unique multiplicative payment
mechanism simultaneously satisfies all these requirements. The result is stated assuming a multiplechoice setup, but extends trivially to non-multiple-choice settings.
Theorem 3.A (Distributional). Consider any value A 2 {0, . . . , G}. Among all incentivecompatible mechanisms (that may or may not satisfy no-free-lunch), Algorithm 1 strictly minimizes
the expenditure on a worker who skips some A of the questions in the the gold standard, and chooses
answers to the remaining (G A) questions uniformly at random.
5

Theorem 3.B (Deterministic). Consider any value B 2 (0, 1]. Among all incentive-compatible
mechanisms (that may or may not satisfy no-free-lunch), Algorithm 1 strictly minimizes the expenditure on a worker who gives incorrect answers to a fraction B or more of the questions attempted
in the gold standard.
The proof of Theorem 3 is presented in Appendix D. We see from this result that the multiplicative
payment mechanism of Algorithm 1 thus possesses very useful properties geared to deter spammers,
while ensuring that a good worker will be paid a high enough amount.
To illustrate this point, let us compare the mechanism of Algorithm 1 with the popular additive class
of payment mechanisms.
Example 1. Consider the popular class of “additive” mechanisms, where the payments to a worker
are added across the gold standard questions. This additive payment mechanism offers a reward of
µ
µT
G for every correct answer in the gold standard, G for every question skipped, and 0 for every
incorrect answer. Importantly, the final payment to the worker is the sum of the rewards across the
G gold standard questions. One can verify that this additive mechanism is incentive compatible.
One can also see that that as guaranteed by our theory, this additive payment mechanism does not
satisfy the no-free-lunch axiom.
Suppose each question involves choosing from two options. Let us compute the expenditure that
these two mechanisms make under a spamming behavior of choosing the answer randomly to each
question. Given the 50% likelihood of each question being correct, on can compute that the additive
mechanism makes a payment of µ2 in expectation. On the other hand, our mechanism pays an
expected amount of only µ2 G . The payment to spammers thus reduces exponentially with the
number of gold standard questions under our mechanism, whereas it does not reduce at all in the
additive mechanism.
Now, consider a different means of exploiting the mechanism(s) where the worker simply skips all
questions. To this end, observe that if a worker skips all the questions then the additive payment
mechanism will incur an expenditure of µT . On the other hand, the proposed payment mechanism
of Algorithm 1 pays an exponentially smaller amount of µT G (recall that T < 1).

4

Simulations and Experiments

In this section, we present synthetic simulations and real-world experiments to evaluate the effects
of our setting and our mechanism on the final label quality.
4.1

Synthetic Simulations

We employ synthetic simulations to understand the effects of various kinds of labeling errors in
crowdsourcing. We consider binary-choice questions in this set of simulations. Whenever a worker
answers a question, her confidence for the correct answer is drawn from a distribution P independent
of all else. We investigate the effects of the following five choices of the distribution P:
•
•
•
•
•

The uniform distribution on the support [0.5, 1].
A triangular distribution with lower end-point 0.2, upper end-point 1 and a mode of 0.6.
A beta distribution with parameter values ↵ = 5 and = 1.
The hammer-spammer distribution [KOS11], that is, uniform on the discrete set {0.5, 1}.
A truncated Gaussian distribution: a truncation of N (0.75, 0.5) to the interval [0, 1].

When a worker has a confidence p (drawn from the distribution P) and attempts the question, the
probability of making an error equals (1 p).
We compare (a) the setting where workers attempt every question, with (b) the setting where workers
skip questions for which their confidence is below a certain threshold T . In this set of simulations,
we set T = 0.75. In either setting, we aggregate the labels obtained from the workers for each
question via a majority vote on the two classes. Ties are broken by choosing one of the two options
uniformly at random.
6

Figure 2: Error under different interfaces for synthetic simulations of five distributions of the workers’ error probabilities.

Figure 2 depicts the results from these simulations. Each bar represents the fraction of questions that
are labeled incorrectly, and is an average across 50,000 trials. (The standard error of the mean is too
small to be visible.) We see that the skip-based setting consistently outperforms the conventional
setting, and the gains obtained are moderate to high depending on the underlying distribution of the
workers’ errors. In particular, the gains are quite striking under the hammer-spammer model: this
result is not surprising since the mechanism (ideally) screens the spammers out and leaves only the
hammers who answer perfectly.

4.2

Experiments on Amazon Mechanical Turk

We conducted preliminary experiments on the Amazon Mechanical Turk commercial crowdsourcing
platform (mturk.com) to evaluate our proposed scheme in real-world scenarios. The complete
data, including the interface presented to the workers in each of the tasks, the results obtained from
the workers, and the ground truth solutions, are available on the website of the first author.
Goal. Before delving into details, we first note certain caveats relating to such a study of mechanism design on crowdsourcing platforms. When a worker encounters a mechanism for only a
small amount of time (a handful of tasks in typical research experiments) and for a small amount of
money (at most a few dollars in typical crowdsourcing tasks), we cannot expect the worker to completely understand the mechanism and act precisely as required. For instance, we wouldn’t expect
our experimental results to change significantly even upon moderate modifications in the promised
amounts, and furthermore, we do expect the outcomes to be noisy. Incentive compatibility kicks
in when the worker encounters a mechanism across a longer term, for example, when a proposed
mechanism is adopted as a standard for a platform, or when higher amounts are involved. This is
when we would expect workers or others (e.g., bloggers or researchers) to design strategies that can
game the mechanism. The theoretical guarantee of incentive compatibility or strict properness then
prevents such gaming in the long run.
We thus regard these experiments as preliminary. Our intentions towards this experimental exercise
were (a) to evaluate the potential of our algorithms to work in practice, and (b) to investigate the
effect of the proposed algorithms on the net error in the collected labelled data.
Experimental setup. We conducted the five following experiments (“tasks”) on Amazon Mechanical Turk: (a) identifying the golden gate bridge from pictures, (b) identifying the breeds of dogs
from pictures, (c) identifying heads of countries, (d) identifying continents to which flags belong,
and (e) identifying the textures in displayed images. Each of these tasks comprised 20 to 126 multi7

Figure 3: Error under different interfaces and mechanisms for five experiments conducted on Mechanical Turk.
ple choice questions.3 For each experiment, we compared (i) a baseline setting (Figure 1a) with an
additive payment mechanism that pays a fixed amount per correct answer, and (ii) our skip-based
setting (Figure 1b) with the multiplicative mechanism of Algorithm 1. For each experiment, and for
each of the two settings, we had 35 workers independently perform the task.
Upon completion of the tasks on Amazon Mechanical Turk, we aggregated the data in the following
manner. For each mechanism in each experiment, we subsampled 3, 5, 7, 9 and 11 workers, and
took a majority vote of their responses. We averaged the accuracy across all questions and across
1, 000 iterations of this subsample-and-aggregate procedure.
Results. Figure 3 reports the error in the aggregate data in the five experiments. We see that in
most cases, our skip-based setting results in a higher quality data, and in many of the instances, the
reduction is two-fold or higher. All in all, in the experiments, we observed a substantial reduction in
the amount of error in the labelled data while expending the same or lower amounts and receiving
no negative comments from the workers. These observations suggest that our proposed skip-based
setting coupled with our multiplicative payment mechanisms have potential to work in practice; the
underlying fundamental theory ensures that the system cannot be gamed in the long run.

5

Discussion and Conclusions

In an extended version of this paper [SZ14], we generalize the “skip-based” setting considered here
to one where we also elicit the workers’ confidence about their answers. Moreover, in a companion
paper [SZP15], we construct mechanisms to elicit the support of worker’s beliefs.
Our mechanism offers some additional benefits. The pattern of skips of the workers provide a reasonable estimate of the difficulty of each question. In practice, the questions that are estimated to
be more difficult may now be delegated to an expert or to additional non-expert workers. Secondly,
the theoretical guarantees of our mechanism may allow for better post-processing of the data, incorporating the confidence information and improving the overall accuracy. Developing statistical
aggregation algorithms or augmenting existing ones (e.g., [RYZ+ 10, KOS11, LPI12, ZLP+ 15]) for
this purpose is a useful direction of research. Thirdly, the simplicity of our mechanisms may facilitate an easier adoption among the workers. In conclusion, given the uniqueness and optimality
in theory, simplicity, and good performance observed in practice, we envisage our multiplicative
payment mechanisms to be of interest to practitioners as well as researchers who employ crowdsourcing.

3
See the extended version of this paper [SZ14] for additional experiments involving free-form responses,
such as text transcription.

8

References
[Boh11]

John Bohannon. Social science for pennies. Science, 334(6054):307–307, 2011.

[CBW 10]

Andrew Carlson, Justin Betteridge, Richard C Wang, Estevam R Hruschka Jr, and
Tom M Mitchell. Coupled semi-supervised learning for information extraction. In
ACM WSDM, pages 101–110, 2010.

[DDS+ 09]

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A
large-scale hierarchical image database. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 248–255, 2009.

[Dou14]

Double or Nothing. http://wikipedia.org/wiki/Double_or_nothing,
2014. Last accessed: July 31, 2014.

[GR07]

Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and
estimation. Journal of the American Statistical Association, 102(477):359–378, 2007.

[HDY+ 12]

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath,
et al. Deep neural networks for acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.
Panagiotis G Ipeirotis, Foster Provost, Victor S Sheng, and Jing Wang. Repeated
labeling using multiple noisy labelers. Data Mining and Knowledge Discovery,
28(2):402–441, 2014.

+

[IPSW14]
[JSV14]

Srikanth Jagabathula, Lakshminarayanan Subramanian, and Ashwin Venkataraman.
Reputation-based worker filtering in crowdsourcing. In Advances in Neural Information Processing Systems 27, pages 2492–2500, 2014.

[KKKMF11] Gabriella Kazai, Jaap Kamps, Marijn Koolen, and Natasa Milic-Frayling. Crowdsourcing for book search evaluation: impact of HIT design on comparative system
ranking. In ACM SIGIR, pages 205–214, 2011.
[KOS11]

David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable
crowdsourcing systems. In Advances in neural information processing systems, pages
1953–1961, 2011.

[LPI12]

Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing.
In NIPS, pages 701–709, 2012.

[RYZ+ 10]

Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles
Florin, Luca Bogoni, and Linda Moy. Learning from crowds. The Journal of Machine
Learning Research, 11:1297–1322, 2010.

[SZ14]

Nihar B Shah and Dengyong Zhou. Double or nothing: Multiplicative incentive
mechanisms for crowdsourcing. arXiv:1408.1387, 2014.

[SZP15]

Nihar B Shah, Dengyong Zhou, and Yuval Peres. Approval voting and incentives in
crowdsourcing. In International Conference on Machine Learning (ICML), 2015.

[VdVE11]

Jeroen Vuurens, Arjen P de Vries, and Carsten Eickhoff. How much spam can you
take? An analysis of crowdsourcing results to increase accuracy. In ACM SIGIR
Workshop on Crowdsourcing for Information Retrieval, pages 21–26, 2011.

[WLC+ 10]

Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards building a highquality workforce with Mechanical Turk. NIPS workshop on computational social
science and the wisdom of crowds, 2010.

[ZLP+ 15]

Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar B
Shah. Regularized minimax conditional entropy for crowdsourcing. arXiv preprint
arXiv:1503.07240, 2015.

9

"
1,5941,Learning with Symmetric Label Noise: The Importance of Being Unhinged,Spotlight,5941-learning-with-symmetric-label-noise-the-importance-of-being-unhinged.pdf,"Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.","Learning with Symmetric Label Noise: The
Importance of Being Unhinged

Brendan van Rooyen∗,†
∗

Aditya Krishna Menon†,∗

The Australian National University

†

Robert C. Williamson∗,†

National ICT Australia

{ brendan.vanrooyen, aditya.menon, bob.williamson }@nicta.com.au

Abstract
Convex potential minimisation is the de facto approach to binary classification.
However, Long and Servedio [2010] proved that under symmetric label noise
(SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly
shows that convex losses are not SLN-robust. In this paper, we propose a convex,
classification-calibrated loss and prove that it is SLN-robust. The loss avoids the
Long and Servedio [2010] result by virtue of being negatively unbounded. The
loss is a modification of the hinge loss, where one does not clamp at zero; hence,
we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any
convex potential; this implies that strong `2 regularisation makes most standard
learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness is
borne out in practice. So, with apologies to Wilde [1895], while the truth is rarely
pure, it can be simple.

1

Learning with symmetric label noise

Binary classification is the canonical supervised learning problem. Given an instance space X, and
samples from some distribution D over X × {±1}, the goal is to learn a scorer s : X → R with low
misclassification error on future samples drawn from D. Our interest is in the more realistic scenario
where the learner observes samples from some corruption D of D, where labels have some constant
probability of being flipped, and the goal is still to perform well with respect to D. This problem is
known as learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988].
Long and Servedio [2010] showed that there exist linearly separable D where, when the learner
observes some corruption D with symmetric label noise of any nonzero rate, minimisation of any
convex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not “SLN-robust” and
motivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010,
Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013].
In this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the result
of Long and Servedio [2010] by virtue of being negatively unbounded. The loss is a modification of the hinge loss where one does not clamp at zero; thus, we call it the unhinged loss. This
loss has several appealing properties, such as being the unique convex loss satisfying a notion of
“strong” SLN-robustness (Proposition 5), being classification-calibrated (Proposition 6), consistent
when minimised on D (Proposition 7), and having an simple optimal solution that is the difference
of two kernel means (Equation 8). Finally, we show that this optimal solution is equivalent to that of
a strongly regularised SVM (Proposition 8), and any twice-differentiable convex potential (Proposition 9), implying that strong `2 regularisation endows most standard learners with SLN-robustness.
1

The classifier resulting from minimising the unhinged loss is not new [Devroye et al., 1996, Chapter 10], [Schölkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section
5.1]. However, establishing this classifier’s (strong) SLN-robustness, uniqueness thereof, and its
equivalence to a highly regularised SVM solution, to our knowledge is novel.

2

Background and problem setup

Fix an instance space X. We denote by D a distribution over X × {±1}, with random variables
(X, Y) ∼ D. Any D may be expressed via the class-conditionals (P, Q) = (P(X | Y = 1), P(X |
Y = −1)) and base rate π = P(Y = 1), or via the marginal M = P(X) and class-probability
function η : x 7→ P(Y = 1 | X = x). We interchangeably write D as DP,Q,π or DM,η .
2.1

Classifiers, scorers, and risks

A scorer is any function s : X → R. A loss is any function ` : {±1} × R → R. We use `−1 , `1 to
refer to `(−1, ·) and `(1, ·). The `-conditional risk L` : [0, 1] × R → R is defined as L` : (η, v) 7→
η · `1 (v) + (1 − η) · `−1 (v). Given a distribution D, the `-risk of a scorer s is defined as
.

LD
` (s) =

[`(Y, s(X))] ,

E

(X,Y)∼D

(1)

D
so that LD
` (s) = E [L` (η(X), s(X))]. For a set S, L` (S) is the set of `-risks for all scorers in S.
X∼M

A function class is any F ⊆ RX . Given some F, the set of restricted Bayes-optimal scorers for a
loss ` are those scorers in F that minimise the `-risk:
.

SD,F,∗
= Argmin LD
` (s).
`
s∈F

The set of (unrestricted) Bayes-optimal scorers is SD,∗
= SD,F,∗
for F = RX . The restricted
`
`
`-regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer:
.

D
regretD,F
(s) = LD
` (s) − inf L` (t).
`
t∈F

Binary classification is concerned with the zero-one loss, `01 : (y, v) 7→ Jyv < 0K + 21 Jv = 0K.
A loss ` is classification-calibrated if all its Bayes-optimal scorers are also optimal for zero-one
loss: (∀D) SD,∗
⊆ SD,∗
01 . A convex potential is any loss ` : (y, v) 7→ φ(yv), where φ : R → R+ is
`
convex, non-increasing, differentiable with φ0 (0) < 0, and φ(+∞) = 0 [Long and Servedio, 2010,
Definition 1]. All convex potentials are classification-calibrated [Bartlett et al., 2006, Theorem 2.1].
2.2

Learning with symmetric label noise (SLN learning)

The problem of learning with symmetric label noise (SLN learning) is the following [Angluin and
Laird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013]. For some notional
“clean” distribution D, which we would like to observe, we instead observe samples from some
corrupted distribution SLN(D, ρ), for some ρ ∈ [0, 1/2). The distribution SLN(D, ρ) is such that
the marginal distribution of instances is unchanged, but each label is independently flipped with
probability ρ. The goal is to learn a scorer from these corrupted samples such that LD
01 (s) is small.
For any quantity in D, we denote its corrupted counterparts in SLN(D, ρ) with a bar, e.g. M for
the corrupted marginal distribution, and η for the corrupted class-probability function; additionally,
when ρ is clear from context, we will occasionally refer to SLN(D, ρ) by D. It is easy to check that
the corrupted marginal distribution M = M , and [Natarajan et al., 2013, Lemma 7]
(∀x ∈ X) η(x) = (1 − 2ρ) · η(x) + ρ.

3

(2)

SLN-robustness: formalisation

We consider learners (`, F) for a loss ` and a function class F, with learning being the search for
some s ∈ F that minimises the `-risk. Informally, (`, F) is “robust” to symmetric label noise (SLNrobust) if minimising ` over F gives the same classifier on both the clean distribution D, which
2

the learner would like to observe, and SLN(D, ρ) for any ρ ∈ [0, 1/2), which the learner actually
observes. We now formalise this notion, and review what is known about SLN-robust learners.
3.1

SLN-robust learners: a formal definition

For some fixed instance space X, let ∆ denote the set of distributions on X × {±1}. Given a notional
“clean” distribution D, Nsln : ∆ → 2∆ returns the set of possible corrupted versions of D the learner
may observe, where labels are flipped with unknown probability ρ:



1
Nsln : D 7→ SLN(D, ρ) | ρ ∈ 0,
.
2
Equipped with this, we define our notion of SLN-robustness.
Definition 1 (SLN-robustness). We say that a learner (`, F) is SLN-robust if
D,F,∗
D,F,∗
(∀D ∈ ∆) (∀D ∈ Nsln (D)) LD
) = LD
).
01 (S`
01 (S`

(3)

That is, SLN-robustness requires that for any level of label noise in the observed distribution D, the
classification performance (wrt D) of the learner is the same as if the learner directly observes D.
Unfortunately, a widely adopted class of learners is not SLN-robust, as we will now see.
3.2

Convex potentials with linear function classes are not SLN-robust

Fix X = Rd , and consider learners with a convex potential `, and a function class of linear scorers
Flin = {x 7→ hw, xi | w ∈ Rd }.
This captures e.g. the linear SVM and logistic regression, which are widely studied in theory and
applied in practice. Disappointingly, these learners are not SLN-robust: Long and Servedio [2010,
Theorem 2] give an example where, when learning under symmetric label noise, for any convex
potential `, the corrupted `-risk minimiser over Flin has classification performance equivalent to
random guessing on D. This implies that (`, Flin ) is not SLN-robust1 as per Definition 1.
Proposition 1 (Long and Servedio [2010, Theorem 2]). Let X = Rd for any d ≥ 2. Pick any convex
potential `. Then, (`, Flin ) is not SLN-robust.
3.3

The fallout: what learners are SLN-robust?

In light of Proposition 1, there are two ways to proceed in order to obtain SLN-robust learners: either
we change the class of losses `, or we change the function class F.
The first approach has been pursued in a large body of work that embraces non-convex losses
[Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010,
Denchev et al., 2012, Manwani and Sastry, 2013]. While such losses avoid the conditions of Proposition 1, this does not automatically imply that they are SLN-robust when used with Flin . In Appendix
B, we present evidence that some of these losses are in fact not SLN-robust when used with Flin .
The second approach is to consider suitably rich F that contains the Bayes-optimal scorer for D,
e.g. by employing a universal kernel. With this choice, one can still use a convex potential loss, and
in fact, owing to Equation 2, any classification-calibrated loss.
Proposition 2. Pick any classification-calibrated `. Then, (`, RX ) is SLN-robust.
Both approaches have drawbacks. The first approach has a computational penalty, as it requires
optimising a non-convex loss. The second approach has a statistical penalty, as estimation rates
with a rich F will require a larger sample size. Thus, it appears that SLN-robustness involves a
computational-statistical tradeoff. However, there is a variant of the first option: pick a loss that is
convex, but not a convex potential. Such a loss would afford the computational and statistical advantages of minimising convex risks with linear scorers. Manwani and Sastry [2013] demonstrated
that square loss, `(y, v) = (1 − yv)2 , is one such loss. We will show that there is a simpler loss that
is convex and SLN-robust, but is not in the class of convex potentials by virtue of being negatively
unbounded. To derive this loss, we first re-interpret robustness via a noise-correction procedure.
1
Even if we were content with a difference of  ∈ [0, 1/2] between the clean and corrupted minimisers’
performance, Long and Servedio [2010, Theorem 2] implies that in the worst case  = 1/2.

3

4

A noise-corrected loss perspective on SLN-robustness

We now re-express SLN-robustness to reason about optimal scorers on the same distribution, but
with two different losses. This will help characterise a set of “strongly SLN-robust” losses.
4.1

Reformulating SLN-robustness via noise-corrected losses

Given any ρ ∈ [0, 1/2), Natarajan et al. [2013, Lemma 1] showed how to associate with a loss ` a
D
noise-corrected counterpart ` such that LD
` (s) = L` (s). The loss ` is defined as follows.
Definition 2 (Noise-corrected loss). Given any loss ` and ρ ∈ [0, 1/2), the noise-corrected loss ` is
(∀y ∈ {±1}) (∀v ∈ R) `(y, v) =

(1 − ρ) · `(y, v) − ρ · `(−y, v)
.
1 − 2ρ

(4)

Since ` depends on the unknown parameter ρ, it is not directly usable to design an SLN-robust
learner. Nonetheless, it is a useful theoretical device, since, by construction, for any F, SD,F,∗
=
`
SD,F,∗
= SD,F,∗
. This means that a sufficient condition for (`, F) to be SLN-robust is for SD,F,∗
.
`
`
`
Ghosh et al. [2015, Theorem 1] proved a sufficient condition on ` such that this holds, namely,
(∃C ∈ R)(∀v ∈ R) `1 (v) + `−1 (v) = C.
(5)
Interestingly, Equation 5 is necessary for a stronger notion of robustness, which we now explore.
4.2

Characterising a stronger notion of SLN-robustness

As the first step towards a stronger notion of robustness, we rewrite (with a slight abuse of notation)
LD
` (s) =

E

(X,Y)∼D

[`(Y, s(X))] =

E

(Y,S)∼R(D,s)

.

[`(Y, S)] = L` (R(D, s)),

where R(D, s) is a distribution over labels and scores. Standard SLN-robustness requires that label
noise does not change the `-risk minimisers, i.e. that if s is such that L` (R(D, s)) ≤ L` (R(D, s0 ))
for all s0 , the same relation holds with D in place of D. Strong SLN-robustness strengthens this
notion by requiring that label noise does not affect the ordering of all pairs of joint distributions over
labels and scores. (This of course trivially implies SLN-robustness.) As with the definition of D,
given a distribution R over labels and scores, let R be the corresponding distribution where labels
are flipped with probability ρ. Strong SLN-robustness can then be made precise as follows.
Definition 3 (Strong SLN-robustness). Call a loss ` strongly SLN-robust if for every ρ ∈ [0, 1/2),
(∀R, R0 ) L` (R) ≤ L` (R0 ) ⇐⇒ L` (R) ≤ L` (R0 ).
We now re-express strong SLN-robustness using a notion of order equivalence of loss pairs, which
simply requires that two losses order all distributions over labels and scores identically.
˜ order equivalent if
Definition 4 (Order equivalent loss pairs). Call a pair of losses (`, `)
(∀R, R0 ) L` (R) ≤ L` (R0 ) ⇐⇒ L`˜(R) ≤ L`˜(R0 ).
Clearly, order equivalence of (`, `) implies SD,F,∗
= SD,F,∗
, which in turn implies SLN-robustness.
`
`
It is thus not surprising that we can relate order equivalence to strong SLN-robustness of `.
Proposition 3. A loss ` is strongly SLN-robust iff for every ρ ∈ [0, 1/2), (`, `) are order equivalent.
This connection now lets us exploit a classical result in decision theory about order equivalent losses
being affine transformations of each other. Combined with the definition of `, this lets us conclude
that the sufficient condition of Equation 5 is also necessary for strong SLN-robustness of `.
Proposition 4. A loss ` is strongly SLN-robust if and only if it satisfies Equation 5.
We now return to our original goal, which was to find a convex ` that is SLN-robust for Flin (and
ideally more general function classes). The above suggests that to do so, it is reasonable to consider
those losses that satisfy Equation 5. Unfortunately, it is evident that if ` is convex, non-constant, and
bounded below by zero, then it cannot possibly be admissible in this sense. But we now show that
removing the boundedness restriction allows for the existence of a convex admissible loss.
4

5

The unhinged loss: a convex, strongly SLN-robust loss

Consider the following simple, but non-standard convex loss:
unh
`unh
1 (v) = 1 − v and `−1 (v) = 1 + v.

Compared to the hinge loss, the loss does not clamp at zero, i.e. it does not have a hinge. (Thus, peculiarly, it is negatively unbounded, an issue we discuss in §5.3.) Thus, we call this the unhinged loss2 .
The loss has a number of attractive properties, the most immediate being is its SLN-robustness.
5.1

The unhinged loss is strongly SLN-robust

unh
unh
Since `unh
is strongly SLN-robust, and thus that
1 (v) + `−1 (v) = 2, Proposition 4 implies that `
unh
(` , F) is SLN-robust for any F. Further, the following uniqueness property is not hard to show.
Proposition 5. Pick any convex loss `. Then,

(∃C ∈ R) `1 (v) + `−1 (v) = C ⇐⇒ (∃A, B, D ∈ R) `1 (v) = −A · v + B, `−1 (v) = A · v + D.
That is, up to scaling and translation, `unh is the only convex loss that is strongly SLN-robust.
Returning to the case of linear scorers, the above implies that (`unh , Flin ) is SLN-robust. This does
not contradict Proposition 1, since `unh is not a convex potential as it is negatively unbounded. Intuitively, this property allows the loss to offset the penalty incurred by instances that are misclassified
with high margin by awarding a “gain” for instances that correctly classified with high margin.
5.2

The unhinged loss is classification calibrated

SLN-robustness is by itself insufficient for a learner to be useful. For example, a loss that is uniformly zero is strongly SLN-robust, but is useless as it is not classification-calibrated. Fortunately,
the unhinged loss is classification-calibrated, as we now establish. For technical reasons (see §5.3),
we operate with FB = [−B, +B]X , the set of scorers with range bounded by B ∈ [0, ∞).
Proposition 6. Fix ` = `unh . For any DM,η , B ∈ [0, ∞), S`D,FB ,∗ = {x 7→ B · sign(2η(x) − 1)}.
Thus, for every B ∈ [0, ∞), the restricted Bayes-optimal scorer over FB has the same sign as the
Bayes-optimal classifier for 0-1 loss. In the limiting case where F = RX , the optimal scorer is
attainable if we operate over the extended reals R ∪ {±∞}, so that `unh is classification-calibrated.
5.3

Enforcing boundedness of the loss

While the classification-calibration of `unh is encouraging, Proposition 6 implies that its (unrestricted) Bayes-risk is −∞. Thus, the regret of every non-optimal scorer s is identically +∞, which
hampers analysis of consistency. In orthodox decision theory, analogous theoretical issues arise
when attempting to establish basic theorems with unbounded losses [Ferguson, 1967, pg. 78].
We can side-step this issue by restricting attention to bounded scorers, so that `unh is effectively
bounded. By Proposition 6, this does not affect the classification-calibration of the loss. In the context of linear scorers, boundedness of scorers can be achieved by regularisation:
instead of work√
ing with Flin , one can instead use Flin,λ = {x 7→ hw, xi | ||w||2 ≤ 1/ λ}, where λ > 0, so
that Flin,λ ⊆ FR/√λ for R = supx∈X ||x||2 . Observe that as (`unh , F) is SLN-robust for any F,
(`unh , Flin,λ ) is SLN-robust for any λ > 0. As we shall see in §6.3, working with Flin,λ also lets us
establish SLN-robustness of the hinge loss when λ is large.
5.4

Unhinged loss minimisation on corrupted distribution is consistent

Using bounded scorers makes it possible to establish a surrogate regret bound for the unhinged loss.
This shows classification consistency of unhinged loss minimisation on the corrupted distribution.
2
This loss has been considered in Sriperumbudur et al. [2009], Reid and Williamson [2011] in the context
of maximum mean discrepancy; see the Appendix. The analysis of its SLN-robustness is to our knowledge
novel.

5

Proposition 7. Fix ` = `unh . Then, for any D, ρ ∈ [0, 1/2), B ∈ [1, ∞), and scorer s ∈ FB ,
1
D,FB
regretD
(s) =
· regret`D,FB (s).
01 (s) ≤ regret`
1 − 2ρ
Standard rates of convergence via generalisation bounds are also trivial to derive; see the Appendix.

6

Learning with the unhinged loss and kernels

We now show that the optimal solution for the unhinged loss when employing regularisation and
kernelised scorers has a simple form. This sheds further light on SLN-robustness and regularisation.
6.1

The centroid classifier optimises the unhinged loss

Consider minimising the unhinged
risk over the class of kernelised scorers FH,λ = {s : x 7→
√
hw, Φ(x)iH | ||w||H ≤ 1/ λ} for some λ > 0, where Φ : X → H is a feature mapping into a
reproducing kernel Hilbert space H with kernel k. Equivalently, given a distribution3 D, we want
λ
∗
wunh,λ
= argmin E [1 − Y · hw, Φ(X)i] + hw, wiH .
(6)
2
(X,Y)∼D
w∈H
The first-order optimality condition implies that
1
∗
(7)
wunh,λ
= · E [Y · Φ(X)] ,
λ (X,Y)∼D
which is the kernel mean map of D [Smola et al., 2007], and thus the optimal unhinged scorer is


1
1
s∗unh,λ : x 7→ · E [Y · k(X, x)] = x 7→ · π · E [k(X, x)] − (1 − π) · E [k(X, x)] .
X∼P
X∼Q
λ (X,Y)∼D
λ
(8)
From Equation 8, the unhinged solution is equivalent to a nearest centroid classifier [Manning et al.,
2008, pg. 181] [Tibshirani et al., 2002] [Shawe-Taylor and Cristianini, 2004, Section 5.1]. Equation
8 gives a simple way to understand the SLN-robustness of (`unh , FH,λ ), as the optimal scorers on
the clean and corrupted distributions only differ by a scaling (see the Appendix):


1
· E
Y · k(X, x) .
(9)
(∀x ∈ X) E [Y · k(X, x)] =
1 − 2ρ (X,Y)∼D
(X,Y)∼D
Interestingly, Servedio [1999, Theorem 4] established that a nearest centroid classifier (which they
termed “AVERAGE ”) is robust to a general class of label noise, but required the assumption that
M is uniform over the unit sphere. Our result establishes that SLN robustness of the classifier
holds without any assumptions on M . In fact, Ghosh et al. [2015, Theorem 1] lets one quantify the
unhinged loss’ performance under a more general noise model; see the Appendix for discussion.
6.2

Practical considerations

We note several points relating to practical usage of the unhinged loss with kernelised scorers. First,
cross-validation is not required to select λ, since changing λ only changes the magnitude of scores,
not their sign. Thus, for the purposes of classification, one can simply use λ = 1.
Second, we can easily extend the scorers to use a bias regularised with strength 0 < λb 6= λ. Tuning
λb is equivalent to computing s∗unh,λ as per Equation 8, and tuning a threshold on a holdout set.
∗
Third, when H = Rd for d small, we can store wunh,λ
explicitly, and use this to make predictions.
For high (or infinite) dimensional H, we can either make predictions directly via Equation 8, or
use random Fourier features [Rahimi and Recht, 2007] to (approximately) embed H into some low∗
dimensional Rd , and then store wunh,λ
as usual. (The latter requires a translation-invariant kernel.)
∗
We now show that under some assumptions, wunh,λ
coincides with the solution of two established
methods; the Appendix discusses some further relationships, e.g. to the maximum mean discrepancy.
3

Given a training sample S ∼ Dn , we can use plugin estimates as appropriate.

6

6.3

Equivalence to a highly regularised SVM and other convex potentials

There is an interesting equivalence between the unhinged solution and that of a highly regularised
SVM. This has been noted in e.g. Hastie et al. [2004, Section 6], which showed how SVMs approach
a nearest centroid classifier, which is of course the optimal unhinged solution.
Proposition 8. Pick any D and Φ : X → H with R = supx∈X ||Φ(x)||H < ∞. For any λ > 0, let
∗
whinge,λ
= argmin
w∈H

E

(X,Y)∼D

[max(0, 1 − Y · hw, Φ(x)iH )] +

λ
hw, wiH
2

∗
∗
be the soft-margin SVM solution. Then, if λ ≥ R2 , whinge,λ
= wunh,λ
.

Since (`unh , FH,λ ) is SLN-robust, it follows that for `hinge : (y, v) 7→ max(0, 1−yv), (`hinge , FH,λ )
is similarly SLN-robust provided λ is sufficiently large. That is, strong `2 regularisation (and a
bounded feature map) endows the hinge loss with SLN-robustness4 . Proposition 8 can be generalised
∗
to show that wunh,λ
is the limiting solution of any twice differentiable convex potential. This shows
that strong `2 regularisation endows most learners with SLN-robustness. Intuitively, with strong
regularisation, one only considers the behaviour of a loss near zero; since a convex potential φ has
φ0 (0) < 0, it will behave similarly to its linear approximation around zero, viz. the unhinged loss.
Proposition 9. Pick any D, bounded feature mapping Φ : X → H, and twice differentiable convex
∗
potential φ with φ00 ([−1, 1]) bounded. Let wφ,λ
be the minimiser of the regularised φ risk. Then,

2
∗
 w ∗

wunh,λ


φ,λ
lim  ∗
−
 = 0.
∗
λ→∞  ||wφ,λ ||H
||wunh,λ
||H 
H

6.4

Equivalence to Fisher Linear Discriminant with whitened data

For binary classification on DM,η , the Fisher Linear Discriminant (FLD) finds a weight vector proportional to the minimiser of square loss `sq : (y, v) 7→ (1 − yv)2 [Bishop, 2006, Section 4.1.5],
∗
wsq,λ
= (EX∼M [XXT ] + λI)−1 · E(X,Y)∼D [Y · X].

(10)

∗
wsq,λ

is only changed by a scaling
By Equation 9, and the fact that the corrupted marginal M = M ,
factor under label noise. This provides an alternate proof of the fact that (`sq , Flin ) is SLN-robust5
∗
[Manwani and Sastry, 2013, Theorem 2]. Clearly, the unhinged loss solution wunh,λ
is equivalent to
 T
∗
the FLD and square loss solution wsq,λ when the input data is whitened i.e. E XX = I. With
X∼M

a well-specified F, e.g. with a universal kernel, both the unhinged and square loss asymptotically
recover the optimal classifier, but the unhinged loss does not require a matrix inversion. With a
misspecified F, one cannot in general argue for the superiority of the unhinged loss over square loss,
or vice-versa, as there is no universally good surrogate to the 0-1 loss [Reid and Williamson, 2010,
Appendix A]; the Appendix illustrate examples where both losses may underperform.

7

SLN-robustness of unhinged loss: empirical illustration

We now illustrate that the unhinged loss’ SLN-robustness is empirically manifest. We reiterate
that with high regularisation, the unhinged solution is equivalent to an SVM (and in the limit any
classification-calibrated loss) solution. Thus, we do not aim to assert that the unhinged loss is
“better” than other losses, but rather, to demonstrate that its SLN-robustness is not purely theoretical.
We first show that the unhinged risk minimiser performs well on the example of Long
and Servedio [2010] (henceforth LS10). Figure 1 shows the distribution D, where X =
{(1, 0), (γ, 5γ), (γ, −γ)} ⊂ R2 , with marginal distribution M = { 14 , 14 , 12 } and all three instances
are deterministically positive. We pick γ = 1/2. The unhinged minimiser perfectly classifies all
three points, regardless of the level of label noise (Figure 1). The hinge minimiser is perfect when
there is no noise, but with even a small amount of noise, achieves a 50% error rate.
4
5

Long and Servedio [2010, Section 6] show that `1 regularisation does not endow SLN-robustness.
Square loss escapes the result of Long and Servedio [2010] since it is not monotone decreasing.

7

1

Unhinged
Hinge 0% noise
Hinge 1% noise

0.5

0.5

ρ
ρ
ρ
ρ
ρ
ρ

1

−0.5

=
=
=
=
=
=

0
0.1
0.2
0.3
0.4
0.49

Hinge

t-logistic

Unhinged

0.00 ± 0.00
0.15 ± 0.27
0.21 ± 0.30
0.38 ± 0.37
0.42 ± 0.36
0.47 ± 0.38

0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.22 ± 0.08
0.22 ± 0.08
0.39 ± 0.23

0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.34 ± 0.48

Table 1: Mean and standard deviation of the 01 error over 125 trials on LS10. Grayed cells
denote the best performer at that noise rate.

−1

Figure 1: LS10 dataset.

We next consider empirical risk minimisers from a random training sample: we construct a training
set of 800 instances, injected with varying levels of label noise, and evaluate classification performance on a test set of 1000 instances. We compare the hinge, t-logistic (for t = 2) [Ding and
Vishwanathan, 2010] and unhinged minimisers using a linear scorer without a bias term, and regularisation strength λ = 10−16 . From Table 1, even at 40% label noise, the unhinged classifier is able
to find a perfect solution. By contrast, both other losses suffer at even moderate noise rates.
We next report results on some UCI datasets, where we additionally tune a threshold so as to ensure
the best training set 0-1 accuracy. Table 2 summarises results on a sample of four datasets. (The
Appendix contains results with more datasets, performance metrics, and losses.) Even at noise close
to 50%, the unhinged loss is often able to learn a classifier with some discriminative power.

ρ
ρ
ρ
ρ
ρ
ρ

=
=
=
=
=
=

0
0.1
0.2
0.3
0.4
0.49

Hinge

t-Logistic

Unhinged

0.00 ± 0.00
0.01 ± 0.03
0.06 ± 0.12
0.17 ± 0.20
0.35 ± 0.24
0.60 ± 0.20

0.00 ± 0.00
0.01 ± 0.03
0.04 ± 0.05
0.09 ± 0.11
0.24 ± 0.16
0.49 ± 0.20

0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.01
0.02 ± 0.07
0.13 ± 0.22
0.45 ± 0.33

ρ
ρ
ρ
ρ
ρ
ρ

=
=
=
=
=
=

0
0.1
0.2
0.3
0.4
0.49

Hinge

t-Logistic

Unhinged

0.05 ± 0.00
0.06 ± 0.01
0.06 ± 0.01
0.08 ± 0.04
0.14 ± 0.10
0.45 ± 0.26

0.05 ± 0.00
0.07 ± 0.02
0.08 ± 0.03
0.11 ± 0.05
0.24 ± 0.13
0.49 ± 0.16

0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.01
0.09 ± 0.10
0.46 ± 0.30

(a) iris.

ρ
ρ
ρ
ρ
ρ
ρ

=
=
=
=
=
=

0
0.1
0.2
0.3
0.4
0.49

(b) housing.

Hinge

t-Logistic

Unhinged

0.00 ± 0.00
0.10 ± 0.08
0.19 ± 0.11
0.31 ± 0.13
0.39 ± 0.13
0.50 ± 0.16

0.00 ± 0.00
0.11 ± 0.02
0.15 ± 0.02
0.22 ± 0.03
0.33 ± 0.04
0.48 ± 0.04

0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.01 ± 0.00
0.02 ± 0.02
0.34 ± 0.21

ρ
ρ
ρ
ρ
ρ
ρ

(c) usps0v7.

=
=
=
=
=
=

0
0.1
0.2
0.3
0.4
0.49

Hinge

t-Logistic

Unhinged

0.05 ± 0.00
0.15 ± 0.03
0.21 ± 0.03
0.25 ± 0.03
0.31 ± 0.05
0.48 ± 0.09

0.04 ± 0.00
0.24 ± 0.00
0.24 ± 0.00
0.24 ± 0.00
0.24 ± 0.00
0.40 ± 0.24

0.19 ± 0.00
0.19 ± 0.01
0.19 ± 0.01
0.19 ± 0.03
0.22 ± 0.05
0.45 ± 0.08

(d) splice.

Table 2: Mean and standard deviation of the 0-1 error over 125 trials on UCI datasets.

8

Conclusion and future work

We proposed a convex, classification-calibrated loss, proved that is robust to symmetric label noise
(SLN-robust), showed it is the unique loss that satisfies a notion of strong SLN-robustness, established that it is optimised by the nearest centroid classifier, and showed that most convex potentials,
such as the SVM, are also SLN-robust when highly regularised. So, with apologies to Wilde [1895]:
While the truth is rarely pure, it can be simple.
Acknowledgments
NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program. The authors thank
Cheng Soon Ong for valuable comments on a draft of this paper.
8

References
Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343–370, 1988.
Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. Journal
of the American Statistical Association, 101(473):138 – 156, 2006.
Christopher M Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Conference on
Computational Learning Theory (COLT), pages 92–100, 1998.
Vasil Denchev, Nan Ding, Hartmut Neven, and S.V.N. Vishwanathan. Robust classification with adiabatic
quantum optimization. In International Conference on Machine Learning (ICML), pages 863–870, 2012.
Luc Devroye, László Györfi, and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996.
Nan Ding and S.V.N. Vishwanathan. t-logistic regression. In Advances in Neural Information Processing
Systems (NIPS), pages 514–522. Curran Associates, Inc., 2010.
Thomas S. Ferguson. Mathematical Statistics: A Decision Theoretic Approach. Academic Press, 1967.
Aritra Ghosh, Naresh Manwani, and P. S. Sastry. Making risk minimization tolerant to label noise. Neurocomputing, 160:93 – 107, 2015.
Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The entire regularization path for the support
vector machine. Journal of Machine Learning Research, 5:1391–1415, December 2004. ISSN 1532-4435.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 5(6):392–401,
November 1998.
Philip M. Long and Rocco A. Servedio. Random classification noise defeats all convex potential boosters.
Machine Learning, 78(3):287–304, 2010. ISSN 0885-6125.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA, 2008. ISBN 0521865719, 9780521865715.
Naresh Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE Transactions on Cybernetics,
43(3):1146–1151, June 2013.
Hamed Masnadi-Shirazi, Vijay Mahadevan, and Nuno Vasconcelos. On the design of robust classifiers for
computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep D. Ravikumar, and Ambuj Tewari. Learning with noisy
labels. In Advances in Neural Information Processing Systems (NIPS), pages 1196–1204, 2013.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural
Information Processing Systems (NIPS), pages 1177–1184, 2007.
Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning Research,
11:2387–2422, December 2010.
Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments. Journal of
Machine Learning Research, 12:731–817, Mar 2011.
Bernhard Schölkopf and Alexander J Smola. Learning with kernels, volume 129. MIT Press, 2002.
Rocco A. Servedio. On PAC learning using Winnow, Perceptron, and a Perceptron-like algorithm. In Conference on Computational Learning Theory (COLT), 1999.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni. Press, 2004.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Schölkopf. A Hilbert space embedding for distributions.
In Algorithmic Learning Theory (ALT), 2007.
Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Gert R. G. Lanckriet, and Bernhard Schölkopf.
Kernel choice and classifiability for RKHS embeddings of probability distributions. In Advances in Neural
Information Processing Systems (NIPS), 2009.
Guillaume Stempfel and Liva Ralaivola. Learning SVMs from sloppily labeled data. In Artificial Neural
Networks (ICANN), volume 5768, pages 884–893. Springer Berlin Heidelberg, 2009.
Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. Diagnosis of multiple cancer
types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences, 99(10):
6567–6572, 2002.
Oscar Wilde. The Importance of Being Earnest, 1895.

9

"
2,6019,Algorithmic Stability and Uniform Generalization,Poster,6019-algorithmic-stability-and-uniform-generalization.pdf,"One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result.  For instance,  a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.","Algorithmic Stability and Uniform Generalization

Ibrahim Alabdulmohsin
King Abdullah University of Science and Technology
Thuwal 23955, Saudi Arabia
ibrahim.alabdulmohsin@kaust.edu.sa

Abstract
One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set
to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss
functions. We provide various interpretations of this result. For instance, a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by
augmenting training examples with artificial noise prior to learning. In addition,
we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction
methods. Finally, we connect algorithmic stability to the size of the hypothesis
space, which recovers the classical PAC result that the size (complexity) of the
hypothesis space should be controlled in order to improve algorithmic stability
and improve generalization.

1

Introduction

One fundamental goal of any learning algorithm is to strike a right balance between underfitting
and overfitting. In mathematical terms, this is often translated into two separate objectives. First,
we would like the learning algorithm to produce a hypothesis that is reasonably consistent with the
empirical evidence (i.e. to have a small empirical risk). Second, we would like to guarantee that the
empirical risk (training error) is a valid estimate of the true unknown risk (test error). The former
condition protects against underfitting while the latter condition protects against overfitting.
The rationale behind these two objectives can be understood if we define the generalization
risk

. 
Rgen by the absolute difference between the empirical and true risks: Rgen = Remp − Rtrue .
Then, it is elementary to observe that the true risk Rtrue is bounded from above by the sum
Remp + Rgen . Hence, by minimizing both the empirical risk (underfitting) and the generalization
risk (overfitting), one obtains an inference procedure whose true risk is minimal.
Minimizing the empirical risk alone can be carried out using the empirical risk minimization (ERM)
procedure [1] or some approximations to it. However, the generalization risk is often impossible to
deal with directly. Instead, it is a common practice to bound it analyticaly so that we can establish
conditions under which it is guaranteed to be small. By establishing conditions for generalization,
one hopes to design better learning algorithms that both perform well empirically and generalize
well to novel observations in the future. A prominent example of such an approach is the Support
Vector Machines (SVM) algorithm for binary classification [2].
However, bounding the generalization risk is quite intricate because it can be approached from
various angles. In fact, several methods have been proposed in the past to prove generalization bounds including uniform convergence, algorithmic stability, Rademacher and Gaussian complexities, generic chaining bounds, the PAC-Bayesian framework, and robustness-based analysis
1

[1, 3, 4, 5, 6, 7, 8, 9]. Concentration of measure inequalities form the building blocks of these rich
theories.
The proliferation of generalization bounds can be understood if we look into the general setting of
learning introduced by Vapnik [1]. In this setting, we have an observation space Z and a hypothesis
m
space H. A learning algorithm, henceforth denoted L : ∪∞
→ H, uses a finite set of
m=1 Z
observations to infer a hypothesis H ∈ H. In the general setting, the inference process end-to-end
is influenced by three key factors: (1) the nature of the observation space Z, (2) the nature of the
hypothesis space H, and (3) the details of the learning algorithm L. By imposing constraints on
any of these three components, one may be able to derive new generalization bounds. For example,
the Vapnik-Chervonenkis (VC) theory derives generalization bounds by assuming constraints on H,
while stability bounds, e.g. [6, 10, 11, 12], are derived by assuming constraints on L.
Given that different generalization bounds can be established by imposing constraints on any of
Z, H, or L, it is intriguing to ask if there exists a single view for generalization that ties all of these
different components together. In this paper, we answer this question in the affirmative by establishing that algorithmic stability alone is equivalent to uniform generalization. Informally speaking, an
inference process is said to generalize uniformly if the generalization risk vanishes uniformly across
all bounded parametric loss functions at the limit of large training sets. A more precise definition
will be presented in the sequel. We will show why constraints that are imposed on either H, Z, or
L to improve uniform generalization can be interpreted as methods of improving the stability of the
learning algorithm L. This is similar in spirit to a result by Kearns and Ron, who showed that having a finite VC dimension in the hypothesis space H implies a certain notion of algorithmic stability
in the inference process [13]. Our statement, however, is more general as it applies to all learning
algorithms that fall under Vapnik’s general setting of learning, well beyond uniform convergence.
The rest of the paper is as follows. First, we review the current literature on algorithmic stability,
generalization, and learnability. Then, we introduce key definitions that will be repeatedly used
throughout the paper. Next, we prove the central theorem, which reveals that algorithmic stability is
equivalent to uniform generalization, and provide various interpretations of this result afterward.

2

Related Work

Perhaps, the two most fundamental concepts in statistical learning theory are those of learnability
and generalization [12, 14]. The two concepts are distinct from each other. As will be discussed
in more details next, whereas learnability is concerned with measuring the excess risk within a
hypothesis space, generalization is concerned with estimating the true risk.
In order to define learnability and generalization, suppose we have an observation space Z, a probability distribution of observations P(z), and a bounded stochastic loss function L(·; H) : Z →
[0, 1], where H ∈ H is an inferred hypothesis. Note that L is implicitly a function of (parameterized by) H as well. We define the true risk of a hypothesis H ∈ H by the risk functional:


Rtrue (H) = EZ∼P(z) L(Z; H)
(1)
Then, a learning algorithm is called consistent if the true risk of its inferred hypothesis H converges
to the optimal true risk within the hypothesis space H at the limit of large training sets m → ∞.
A problem is called learnable if it admits a consistent learning algorithm [14]. It has been known
that learnability for supervised classification and regression problems is equivalent to uniform convergence [3, 14]. However, Shalev-Shwartz et al. recently showed that uniform convergence is not
necessary in Vapnik’s general setting of learning and proposed algorithmic stability as an alternative
key condition for learnability [14].
Unlike learnability, the question of generalization is concerned primarily with how representative
the empirical risk Remp is to the true risk Rtrue . To elaborate, suppose we have a finite training set
Sm = {Zi }i=1,..,m , which comprises of m i.i.d. observations Zi ∼ P(z). We define the empirical
risk of a hypothesis H with respect to Sm by:
1 X
Remp (H; Sm ) =
L(Zi ; H)
(2)
m
Zi ∈Sm

We also let Rtrue (H) be the true risk as defined in Eq. (1). Then, a learning algorithm L is said to
generalize if the empirical risk of its inferred hypothesis converges to its true risk as m → ∞.
2

Similar to learnability, uniform convergence is, by definition, sufficient for generalization [1], but
it is not necessary because the learning algorithm can always restrict its search space to a smaller
subset of H (artificially so to speak). By contrast, it is not known whether algorithmic stability is
necessary for generalization. It has been shown that various notions of algorithmic stability can be
defined that are sufficient for generalization [6, 10, 11, 12, 15, 16]. However, it is not known whether
an appropriate notion of algorithmic stability can be defined that is both necessary and sufficient for
generalization in Vapnik’s general setting of learning. In this paper, we answer this question by
showing that stability in the inference process is not only sufficient for generalization, but it is, in
fact, equivalent to uniform generalization, which is a notion of generalization that is stronger than
the one traditionally considered in the literature.

3

Preliminaries

To simplify the discussion, we will always assume that all sets are countable, including the observation space Z and the hypothesis space H. This is similar to the assumptions used in some previous
works such as [6]. However, the main results, which are presented in Section 4, can be readily
generalized. In addition, we assume that all learning algorithms are invariant to permutations of the
training set. Hence, the order of training examples is irrelevant.
Moreover, if X ∼ P(x) is a random variable
drawn from the alphabet X and f (X) is a function of
P
X, we write EX∼P(x) f (X) to mean x∈X P(x) f (x). Often, we will simply write EX f (X) to
mean EX∼P(x) f (X) if the distribution of X is clear from the context. If X takes its values from
a finite set S uniformly at random, we write X ∼ S to denote this distribution of X. If X is a
boolean random variable, then I{X} = 1 if and only if X is true, otherwise I{X} = 0. In general,
random variables are denoted with capital letters, instances of random variables are denoted with
small letters, and alphabets are denoted with calligraphic typeface. Also, given two probability mass
functions P and Q defined on the same alphabet A, we will write hP,
Qi to denote the overlapping
. P
coefficient, i.e. intersection, between P and Q. That is, hP, Qi = a∈A min{P (a), Q(a)}. Note
that hP, Qi = 1− ||P , Q||T , where ||P , Q||T is the total variation distance. Last, we will write
B(k; φ, n) = nk φk (1 − φ)n−k to denote the binomial distribution.
In this paper, we consider the general setting of learning introduced by Vapnik [1]. To reiterate, we
have an observation space Z and a hypothesis space H. Our learning algorithm L receives a set of
m observations Sm = {Zi }i=1,..,m ∈ Z m generated i.i.d. from a fixed unknown distribution P(z),
m
and picks a hypothesis H ∈ H with probability PL (H = h|Sm ). Formally, L : ∪∞
→ H is a
m=1 Z
stochastic map. In this paper, we allow the hypothesis H to be any summary statistic of the training
set. It can be a measure of central tendency, as in unsupervised learning, or it can be a mapping from
an input space to an output space, as in supervised learning. In fact, we even allow H to be a subset
of the training set itself. In formal terms, L is a stochastic map between the two random variables
H ∈ H and Sm ∈ Z m , where the exact interpretation of those random variables is irrelevant.
In any learning task, we assume a non-negative bounded loss function L(Z; H) : Z → [0, 1] is
used to measure the quality of the inferred hypothesis H ∈ H on the observation Z ∈ Z. Most
importantly, we assume that L(·; H) : Z → [0, 1] is parametric:
Definition 1 (Parametric Loss Functions). A loss function L(·; H) : Z → [0, 1] is called parametric if it is independent of the training set Sm given the inferred hypothesis H. That is, a parametric
loss function satisfies the Markov chain: Sm → H → L(·; H).
For any fixed hypothesis H ∈ H, we define its true risk Rtrue (H) by Eq. (1), and define its
empirical risk on a training set Sm , denoted Remp (H; Sm ), by Eq. (2). We also define the true and
empirical risks of the learning algorithm L by the expected risk of its inferred hypothesis:
R̂true (L) = ESm EH ∼PL (h|Sm ) Rtrue (H)
= ESm EH|Sm Rtrue (H)
(3)
R̂emp (L) = ESm EH ∼PL (h|Sm ) Remp (H; Sm )

= ESm EH|Sm Remp (H; Sm )

(4)

To simplify notation, we will write R̂true and R̂emp instead of R̂true (L) and R̂emp (L). We will
consider the following definition of generalization:
m
Definition 2 (Generalization). A learning algorithm L : ∪∞
→ H with a parametric
m=1 Z
loss function L(·; H) :  Z → [0, 1] generalizes if for any distribution P(z) on Z, we have
limm→∞ |R̂emp − R̂true  = 0, where R̂true and R̂emp are given in Eq. (3) and Eq. (4) respectively.
3

In other words, a learning algorithm L generalizes according to Definition 2 if its empirical performance (training loss) becomes an unbiased estimator to the true risk as m → ∞. Next, we define
uniform generalization:
m
Definition 3 (Uniform Generalization). A learning algorithm L : ∪∞
→ H generalizes
m=1 Z
uniformly if for any  > 0, there exists m0 () > 0 such that for all distributions P(z) on Z, all
parametric loss functions, and all sample sizes m > m0 (), we have |R̂emp (L) − R̂true (L) ≤ .
Uniform generalization is stronger than the original notion of generalization in Definition 2. In
particular, if a learning algorithm generalizes uniformly, then it generalizes according to Definition
2 as well. The converse, however, is not true. Even though uniform generalization appears to be
quite a strong condition, at first sight, a key contribution of this paper is to show that it is not a strong
condition because it is equivalent to a simple condition, namely algorithmic stability.

4

Main Results

Before we prove that algorithmic stability is equivalent to uniform generalization, we introduce a
probabilistic notion of mutual stability between two random variables. In order to abstract away any
labeling information the random variables might possess, e.g. the observation space may or may not
be a metric space, we define stability by the impact of observations on probability distributions:
Definition 4 (Mutual Stability). Let X ∈ X and Y ∈ Y be two random variables. Then, the mutual
stability between X and Y is defined by:
.
S(X; Y ) = hP(X) P(Y ), P(X, Y )i = EX hP(Y ), P(Y |X)i = EY hP(X), P(X|Y )i
If we recall that 0 ≤ hP, Qi ≤ 1 is the overlapping coefficient between the two probability distributions P and Q, we see that S(X; Y ) given by Definition 4 is indeed a probabilistic measure
of mutual stability. It measures how stable the distribution of Y is before and after observing an
instance of X, and vice versa. A small value of S(X; Y ) means that the probability distribution of
X or Y is heavily perturbed by a single observation of the other random variable. Perfect mutual
stability is achieved when the two random variables are independent of each other.
With this probabilistic notion of mutual stability in mind, we define the stability of a learning algorithm L by the mutual stability between its inferred hypothesis and a random training example.
m
→ H be a learning algorithm that receives
Definition 5 (Algorithmic Stability). Let L : ∪∞
m=1 Z
a finite set of training examples Sm = {Zi }i=1,..,m ∈ Z m drawn i.i.d. from a fixed distribution
P(z). Let H ∼ PL (h|Sm ) be the hypothesis inferred by L, and let Ztrn ∼ Sm be a single random training example. We define the stability of L by: S(L) = inf P(z) S(H; Ztrn ), where the
infimum is taken over all possible distributions of observations P(z). A learning algorithm is called
algorithmically stable if limm→∞ S(L) = 1.
Note that the above definition of algorithmic stability is rather weak; it only requires that the contribution of any single training example on the overall inference process to be more and more negligible
as the sample size increases. In addition, it is well-defined even if the learning algorithm is deterministic because the hypothesis H, if it is a deterministic function of an entire training set of m
observations, remains a stochastic function of any individual observation. We illustrate this concept
with the following example:
Example 1. Suppose that observations Zi ∈ {0, 1} are i.i.d. Bernoulli P
trials with P(Zi = 1) = φ,
m
1
and that
the
hypothesis
produced
by
L
is
the
empirical
average
H
=
i=1 Zi . Because P(H =
m


k/m  Ztrn = 1) = B(k − 1; φ, m − 1) and P(H = k/m  Ztrn = 0) = B(k; φ, m − 1), it can be
shown using Stirling’s approximation [17] that the algorithmic stability of this learning algorithm
is asymptotically given by S(L) ∼ 1 − √21π m , which is achieved when φ = 1/2. A more general
statement will be proved later in Section 5.
Next, we show that the notion of algorithmic stability in Definition 5 is equivalent to the notion of
uniform generalization in Definition 3. Before we do that, we first state the following lemma.
Lemma 1 (Data Processing Inequality). Let A, B, and C be three random variables that satisfy the
Markov chain A → B → C. Then: S(A; B) ≤ S(A; C).
4

Proof. The proof consists of two steps 1 . First, we note that because the Markov chain implies that
P(C|B, A) = P(C|B), we have S(A; (B, C)) = S(A; B) by direct substitution into Definition
5. Second, similar to the information-cannot-hurt inequality in information theory [18], it can be
shown that S(A; (B, C)) ≤ S(A; C) for any random variables A, B and C. This is proved using
some algebraic manipulationand
minimum of the sums is always larger than the
	 theP
P the fact
P that
α
,
β
sum of minimums, i.e. min
≥
i i
i i
i min{αi , βi }. Combining both results yields
S(A; B) = S(A; (B, C)) ≤ S(A; C), which is the desired result.
Now, we are ready to state the main result of this paper.
m
Theorem 1. For any learning algorithm L : ∪∞
→ H, algorithmic stability as given in Defm=1 Z
inition
5
is
both
necessary
and
sufficient
for
uniform
generalization
(see Definition 3). In addition,


R̂true − R̂emp  ≤ 1 − S(H; Ztrn ) ≤ 1 − S(L), where Rtrue and Remp are the true and empirical
risks of the learning algorithm defined in Eq. (3) and (4) respectively.
Proof. Here is an outline of the proof. First, because a parametric loss function L(·; H) : Z → [0, 1]
is itself a random variable that satisfies the Markov chain Sm → H → L(·; H), it is not independent
of Ztrn ∼ Sm . Hence, the empirical risk is given by R̂emp = EL(·;H) EZtrn |L(·;H) L(Ztrn ; H). By
contrast, the true risk is given by R̂true = EL(·;H) EZtrn ∼P(z) L(Ztrn ; H). The difference is:


R̂true − R̂emp = EL(·;H) EZtrn L(Ztrn ; H) − EZtrn |L(·;H) L(Ztrn ; H)
To sandwich the right-hand side between an upper and a lower bound, we note that if P1 (z) and
P2 (z) are two distributions
defined on the same alphabet
Z and F (·) : Z → [0, 1] is a bounded loss




function, then EZ∼P1 (z) F (Z) − EZ∼P2 (z) F (Z) ≤ ||P1 (z) , P2 (z)||T , where ||P , Q||T is the
total variation distance. The proof to this result can be immediately deduced by considering the two
regions {z ∈ Z : P1 (z) > P2 (z)} and {z ∈ Z : P1 (z) < P2 (z)} separately. This is, then, used to
deduce the inequalities:


R̂true − R̂emp  ≤ 1 − S(L(·; H); Ztrn ) ≤ 1 − S(H; Ztrn ) ≤ 1 − S(L),
where the second inequality follows by the data processing inequality in Lemma 1, whereas the
last inequality follows by definition of algorithmic stability (see Definition
5). This
 proves that

if L is algorithmically stable, i.e. S(L) → 1 as m → ∞, then R̂true − R̂emp  converges to
zero uniformly across all parametric loss functions. Therefore, algorithmic stability is sufficient for
uniform generalization. The converse is proved by showing that for any
a bounded
 δ > 0, there exists

parametric loss and a distribution Pδ (z) such that 1 − S(L) − δ ≤ R̂true − R̂emp  ≤ 1 − S(L).
Therefore, algorithmic stability is also necessary for uniform generalization.

5

Interpreting Algorithmic Stability and Uniform Generalization

In this section, we provide several interpretations of algorithmic stability and uniform generalization.
In addition, we show how Theorem 1 recovers some classical results in learning theory.
5.1

Algorithmic Stability and Data Processing

The relationship between algorithmic stability and data processing is presented in Lemma 1. Given
the random variables A, B, and C and the Markov chain A → B → C, we always have S(A; B) ≤
S(A; C). This presents us with qualitative insights into the design of machine learning algorithms.
First, suppose we have two different hypotheses H1 and H2 . We will say that H2 contains less
informative than H1 if the Markov chain Sm → H1 → H2 holds. For example, if observations
Zi ∈ {0, 1} are Bernoulli trials, then H1 ∈ R can be the empirical average as given in Example 1
while H2 ∈ {0, 1} can be the label that occurs most often in the training set. Because H2 = I{H1 ≥
m/2}, the hypothesis H2 contains strictly less information about the original training set than H1 .
Formally, we have Sm → H1 → H2 . In this case, H2 enjoys a better uniform generalization bound
than H1 because of data-processing. Intuitively, we know that such a result should hold because H2
is less tied to the original training set than H1 . This brings us to the following remark.
1

Detailed proofs are available in the supplementary file.

5

Remark 1. We can improve the uniform generalization bound (or equivalently algorithmic stability)
of a learning algorithm by post-processing its inferred hypothesis H in a manner that is conditionally independent of the original training set given H.
Example 2. Post-processing hypotheses is a common technique used in machine learning. This
includes sparsifying the coefficient vector w ∈ Rd in linear methods, where wj is set to zero if it has
a small absolute magnitude. It also includes methods that have been proposed to reduce the number
of support vectors in SVM by exploiting linear dependence [19]. By the data processing inequality,
such methods improve algorithmic stability and uniform generalization.
Needless to mention, better generalization does not immediately translate into a smaller true risk.
This is because the empirical risk itself may increase when the inferred hypothesis is post-processed
independently of the original training set.
Second, if the Markov chain A → B → C holds, we also obtain S(A; C) ≥ S(B; C) by applying
the data processing inequality to the reverse Markov chain C → B → A. As a result, we can improve algorithmic stability by contaminating training examples with artificial noise prior to learning.
This is because if Ŝm is a perturbed version of a training set Sm , then Sm → Ŝm → H implies that
S(Ztrn ; H) ≥ S(Ẑtrn ; H), when Ztrn ∼ Sm and Ẑtrn ∼ Ŝm are random training examples drawn
uniformly at random from each training set respectively. This brings us to the following remark:
Remark 2. We can improve the algorithmic stability of a learning algorithm by introducing artificial
noise to training examples, and applying the learning algorithm on the perturbed training set.
Example 3. Corrupting training examples with artificial noise, such as the recent dropout method,
are popular techniques in neural networks to improve generalization [20]. By the data processing
inequality, such methods indeed improve algorithmic stability and uniform generalization.
5.2

Algorithmic Stability and the Size of the Observation Space

Next, we look into how the size of the observation space Z influences algorithmic stability. First,
we start with the following definition:
Definition 6 (Lazy Learning). A learning algorithm L is called lazy if its hypothesis H ∈ H is
mapped one-to-one with the training set Sm , i.e. the mapping H → Sm is injective.
A lazy learner is called lazy if its hypothesis is equivalent to the original training set in its information content. Hence, no learning actually takes place. One example is instance-based learning
when H = Sm . Despite their simple nature, lazy learners are useful in practice. They are useful
theoretical tools as well. In particular, because of the equivalence H ≡ Sm and the data processing
inequality, the algorithmic stability of a lazy learner provides a lower bound to the stability of any
possible learning algorithm. Therefore, we can relate algorithmic stability (uniform generalization)
to the size of the observation space by quantifying the algorithmic stability of lazy learners. Because
the size of Z is usually infinite, however, we introduce the following definition of effective set size.
Definition 7. In a countable space Z endowed with a probability mass function P(z), the effective
p
2
P
.
size of Z w.r.t. P(z) is defined by: Ess [Z; P(z)] = 1 +
P(z) (1 − P(z)) .
z∈Z
At one extreme, if P(z) is uniform over a finite alphabet Z, then Ess [Z; P(z)] = |Z|. At the
other extreme, if P(z) is a Kronecker delta distribution, then Ess [Z; P(z)] = 1. As proved next,
this notion of effective set size determines the rate of convergence of an empirical probability mass
function to its true distribution when the distance is measured in the total variation sense. As a result,
it allows us to relate algorithmic stability to a property of the observation space Z.
Theorem 2. Let Z be a countable space endowed with a probability mass function P(z). Let Sm
be a set of m i.i.d. samples Zi ∼ P(z). Define PSm (z) to be the empirical probability mass function
q induced by drawing samples uniformly at random from Sm . Then: ESm ||P(z), PSm (z)||T =
√
Ess [Z; P(z)]−1
+ o(1/ m), where 1 ≤ Ess [Z; P(z)] ≤ |Z| is the effective size of Z (see Def2πm
∞
m
inition
q 7). In addition, for any learning algorithm L : ∪m=1 Z → H, we have S(H; Ztrn ) ≥
√
P(z)]−1
1 − Ess [Z;
− o(1/ m), where the bound is achieved by lazy learners (see Definition 6)2 .
2πm
2

A special case of Theorem 2 was proved by de Moivre in the 1730s, who showed that the
pempirical mean of
i.i.d. Bernoulli trials with a probability of success φ converges to the true mean at a rate of 2φ(1 − φ)/(πm)

6

 m1 m2
m
Proof. Here is an outline of the proof. First, we know that P(Sm ) = m1 , m
p1 p2 · · · , where
,
...
2

·
1
· is the multinomial coefficient. Using the relation ||P, Q||T = 2 ||P − Q||1 , the multinomial
series, and De Moivre’s formula for the mean deviation of the binomial random variable [22], it can
be shown with some algebraic manipulations that:
1 X
m!
k
ESm ||P(z), PSm (z)||T =
(1 − pk )(1−pk )m p1+mp
k
m
(pk m)! ((1 − pk )m − 1)!
k=1,2,...

Using Stirling’s approximation to the factorial [17], we obtain the simple asymptotic expression:
r
r
X
1
2pk (1 − pk )
Ess [Z; P(z)] − 1
ESm ||P(z), PSm (z)||T ∼
=1−
,
2
πm
2πm
k=1,2,3,...

which is tight due to the tightness of the Stirling approximation. The rest of the theorem follows
from the Markov chain Sm → Sm → H, the data processing inequality, and Definition 6.
Corollary 1. Given the conditions of Theorem 2,q
if Z is in addition finite (i.e. |Z| < ∞), then for
√
any learning algorithm L, we have: S(L) ≥ 1 − |Z|−1
2πm − o(1/ m)
Proof. Because in a finite observation space Z, the maximum effective set size (see Definition 7) is
|Z|, which is attained at the uniform distribution P(z) = 1/|Z|.
Intuitively speaking, Theorem 2 and its corollary state that in order to guarantee good uniform
generalization for all possible learning algorithms, the number of observations must be sufficiently
large to cover the entire effective size of the observation space Z. Needless to mention, this is
difficult to achieve in practice so the algorithmic stability of machine learning algorithms must be
controlled in order to guarantee a good generalization from a few empirical observations. Similarly,
the uniform generalization bound can be improved by reducing the effective size of the observation
space, such as by using dimensionality reduction methods.
5.3

Algorithmic Stability and the Complexity of the Hypothesis Space

Finally, we look into the hypothesis space and how it influences algorithmic stability. First, we look
into the role of the size of the hypothesis space. This is formalized in the following theorem.
m
Theorem 3. Denote by H ∈ H the hypothesis inferred by a learning algorithm L : ∪∞
→
m=1 Z
H. Then, the following bound on algorithmic stability always holds:
r
r
H(H)
log |H|
≥1−
,
S(L) ≥ 1 −
2m
2m
where H is the Shannon entropy measured in nats (i.e. using natural logarithms).
Proof. The proof is information-theoretic. If we let I(X; Y ) be the mutual information between the
r.v.’s X and Y and let Sm = {Z1 , Z2 , . . . , Zm } be a random choice of a training set, we have:
m
hX
i h
i
I(Sm ; H) = H(Sm ) − H(Sm | H) =
H(Zi ) − H(Z1 |H) + H(Z2 |Z1 , H) + · · ·
i=1

Because conditioning reduces entropy, i.e. H(A|B) ≤ H(A) for any r.v.’s A and B, we have:
I(Sm ; H) ≥

m
X

[H(Zi ) − H(Zi | H)] = m [H(Ztrn ) − H(Ztrn | H)]

i=1

Therefore:
I(Ztrn ; H) ≤

I(Sm ; H)
m

(5)

on average. This is believed to be the first appearance of the square-root law in statistical inference in the
literature [21]. Because the effective set size of the Bernoulli distribution, according to Definition 7, is given
by 1 + 4φ(1 − φ), Theorem 2 agrees with, in fact generalizes, de Moivre’s result.

7

Next, we use Pinsker’s
q inequality [18], which states that for any probability distributions P and
D(P || Q)
Q: ||P , Q||T ≤
, where ||P , Q||T is total variation distance and D(P || Q) is
2
the Kullback-Leibler divergence measured in nats (i.e. using natural logarithms). If we recall
that S(Sm ; H) = 1 − ||P(Sm ) P(H) , P(Sm , H)||T while mutual information is I(Sm ; H) =
D(P(Sm , H) || P(Sm ) P(H)), we deduce from Pinsker’s inequality and Eq. (5):
S(Ztrn ; H) = 1 − ||P(Ztrn ) P(H) , P(Ztrn , H)||T
r
r
r
r
I(Ztrn ; H)
I(Sm ; H)
H(H)
log |H|
≥1−
≥1−
≥1−
≥1−
2
2m
2m
2m
In the last line, we used the fact that I(X; Y ) ≤ H(X) for any random variables X and Y .
Theorem 3 re-establishes the classical PAC result on the finite hypothesis space [23]. In terms of
algorithmic stability, a learning algorithm will enjoy a high stability if the size of the hypothesis
space is small. In terms of uniform generalization, it states that the generalizationp
risk of a learning
algorithm
is
bounded
from
above
uniformly
across
all
parametric
loss
functions
by
H(H)/(2m) ≤
p
log |H|/(2m), where H(H) is the Shannon entropy of H.
Next, we relate algorithmic stability to the Vapnik-Chervonenkis (VC) dimension. Despite the fact
that the VC dimension is defined on binary-valued functions whereas algorithmic stability is a functional of probability distributions, there exists a connection between the two concepts. To show this,
we first introduce a notion of an induced concept class that exists for any learning algorithm L:
m
Definition 8. The concept class C induced by a learning algorithm L : ∪∞
→ H is defined
m=1 Z
to be the set of total Boolean functions c(z) = I{P(Ztrn = z | H) ≥ P(Ztrn = z)} for all H ∈ H.
Intuitively, every hypothesis H ∈ H induces a total partition on the observation space Z given by
the Boolean function in Definition 8. That is, H splits Z into two disjoint sets: the set of values in
Z that are, a posteriori, less likely to have been present in the training set than before given that the
inferred hypothesis is H, and the set of all other values. The complexity (richness) of the induced
concept class C is related to algorithmic stability via the VC dimension.
m
Theorem 4. Let L : ∪∞
→ H be a learning algorithm with an induced concept class C. Let
m=1 Z
dV C (C) be the VC dimension of C. Then, the following bound holds if m > dV C (C) + 1:
p
4 + dV C (C) (1 + log(2m))
√
S(L) ≥ 1 −
2m
In particular, L is algorithmically stable if its induced concept class C has a finite VC dimension.
Proof. The
is bounded from below by 1 −
n proof relies on the fact that algorithmic stability S(L)
o


supP(z) ESm suph∈H EZ∼P(z) ch (Z) − EZ∼Sm ch (Z) , where cH (z) = I{P(Ztrn =
z|H) ≥ P(Ztrn ) = z}. The final bound follows by applying uniform convergence results [23].

6

Conclusions

In this paper, we showed that a probabilistic notion of algorithmic stability was equivalent to uniform
generalization. In informal terms, a learning algorithm is called algorithmically stable if the impact
of a single training example on the probability distribution of the final hypothesis always vanishes at
the limit of large training sets. In other words, the inference process never depends heavily on any
single training example. If algorithmic stability holds, then the learning algorithm generalizes well
regardless of the choice of the parametric loss function. We also provided several interpretations of
this result. For instance, the relationship between algorithmic stability and data processing reveals
that algorithmic stability can be improved by either post-processing the inferred hypothesis or by
augmenting training examples with artificial noise prior to learning. In addition, we established a
relationship between algorithmic stability and the effective size of the observation space, which provided a formal justification for dimensionality reduction methods. Finally, we connected algorithmic
stability to the complexity (richness) of the hypothesis space, which re-established the classical PAC
result that the complexity of the hypothesis space should be controlled in order to improve stability,
and, hence, improve generalization.
8

References
[1] V. N. Vapnik, “An overview of statistical learning theory,” Neural Networks, IEEE Transactions
on, vol. 10, September 1999.
[2] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning, vol. 20, pp. 273–297,
1995.
[3] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, “Learnability and the VapnikChervonenkis dimension,” Journal of the ACM (JACM), vol. 36, no. 4, pp. 929–965, 1989.
[4] M. Talagrand, “Majorizing measures: the generic chaining,” The Annals of Probability, vol. 24,
no. 3, pp. 1049–1103, 1996.
[5] D. A. McAllester, “PAC-Bayesian stochastic model selection,” Machine Learning, vol. 51,
pp. 5–21, 2003.
[6] O. Bousquet and A. Elisseeff, “Stability and generalization,” The Journal of Machine Learning
Research (JMLR), vol. 2, pp. 499–526, 2002.
[7] P. L. Bartlett and S. Mendelson, “Rademacher and gaussian complexities: Risk bounds and
structural results,” The Journal of Machine Learning Research (JMLR), vol. 3, pp. 463–482,
2002.
[8] J.-Y. Audibert and O. Bousquet, “Combining PAC-Bayesian and generic chaining bounds,”
The Journal of Machine Learning Research (JMLR), vol. 8, pp. 863–889, 2007.
[9] H. Xu and S. Mannor, “Robustness and generalization,” Machine learning, vol. 86, no. 3,
pp. 391–423, 2012.
[10] A. Elisseeff, M. Pontil, et al., “Leave-one-out error and stability of learning algorithms with
applications,” NATO-ASI series on Learning Theory and Practice Science Series Sub Series
III: Computer and Systems Sciences, 2002.
[11] S. Kutin and P. Niyogi, “Almost-everywhere algorithmic stability and generalization error,” in
Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence (UAI), 2002.
[12] T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi, “General conditions for predictivity in
learning theory,” Nature, vol. 428, pp. 419–422, 2004.
[13] M. Kearns and D. Ron, “Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation,” Neural Computation, vol. 11, no. 6, pp. 1427–1453, 1999.
[14] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan, “Learnability, stability and uniform convergence,” The Journal of Machine Learning Research (JMLR), vol. 11, pp. 2635–
2670, 2010.
[15] L. Devroye, L. Györfi, and G. Lugosi, A probabilistic theory of pattern recognition. Springer,
1996.
[16] V. Vapnik and O. Chapelle, “Bounds on error expectation for support vector machines,” Neural
Computation, vol. 12, no. 9, pp. 2013–2036, 2000.
[17] H. Robbins, “A remark on stirling’s formula,” American Mathematical Monthly, pp. 26–29,
1955.
[18] T. M. Cover and J. A. Thomas, Elements of information theory. Wiley & Sons, 1991.
[19] T. Downs, K. E. Gates, and A. Masters, “Exact simplification of support vector solutions,”
JMLR, vol. 2, pp. 293–297, 2002.
[20] S. Wager, S. Wang, and P. S. Liang, “Dropout training as adaptive regularization,” in NIPS,
pp. 351–359, 2013.
[21] S. M. Stigler, The history of statistics: The measurement of uncertainty before 1900. Harvard
University Press, 1986.
[22] P. Diaconis and S. Zabell, “Closed form summation for classical distributions: Variations on a
theme of de moivre,” Statlstlcal Science, vol. 6, no. 3, pp. 284–302, 1991.
[23] S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.

9

"
3,6035,Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models,Poster,6035-adaptive-low-complexity-sequential-inference-for-dirichlet-process-mixture-models.pdf,"We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a noveladaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and datapredictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to otheronline state-of-the-art methods.","Adaptive Low-Complexity Sequential Inference for
Dirichlet Process Mixture Models
Theodoros Tsiligkaridis, Keith W. Forsythe
Massachusetts Institute of Technology, Lincoln Laboratory
Lexington, MA 02421 USA
ttsili@ll.mit.edu, forsythe@ll.mit.edu

Abstract
We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when
the number of clusters are unknown a-priori. We present an easily computable,
closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming
conjugate priors. Motivated by large-sample asymptotics, we propose a novel
adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We
further prove that in the large-sample limit, the conditional likelihood and data
predictive distribution become asymptotically Gaussian. We demonstrate through
experiments on synthetic and real data sets that our approach is superior to other
online state-of-the-art methods.

1

Introduction

Dirichlet process mixture models (DPMM) have been widely used for clustering data Neal (1992);
Rasmussen (2000). Traditional finite mixture models often suffer from overfitting or underfitting
of data due to possible mismatch between the model complexity and amount of data. Thus, model
selection or model averaging is required to find the correct number of clusters or the model with
the appropriate complexity. This requires significant computation for high-dimensional data sets or
large samples. Bayesian nonparametric modeling are alternative approaches to parametric modeling,
an example being DPMM’s which can automatically infer the number of clusters from the data via
Bayesian inference techniques.
The use of Markov chain Monte Carlo (MCMC) methods for Dirichlet process mixtures has made
inference tractable Neal (2000). However, these methods can exhibit slow convergence and their
convergence can be tough to detect. Alternatives include variational methods Blei & Jordan (2006),
which are deterministic algorithms that convert inference to optimization. These approaches can
take a significant computational effort even for moderate sized data sets. For large-scale data sets
and low-latency applications with streaming data, there is a need for inference algorithms that are
much faster and do not require multiple passes through the data. In this work, we focus on lowcomplexity algorithms that adapt to each sample as they arrive, making them highly scalable. An
online algorithm for learning DPMM’s based on a sequential variational approximation (SVA) was
proposed in Lin (2013), and the authors in Wang & Dunson (2011) recently proposed a sequential
maximum a-posterior (MAP) estimator for the class labels given streaming data. The algorithm is
called sequential updating and greedy search (SUGS) and each iteration is composed of a greedy
selection step and a posterior update step.
The choice of concentration parameter α is critical for DPMM’s as it controls the number of clusters Antoniak (1974). While most fast DPMM algorithms use a fixed α Fearnhead (2004); Daume
1

(2007); Kurihara et al. (2006), imposing a prior distribution on α and sampling from it provides more
flexibility, but this approach still heavily relies on experimentation and prior knowledge. Thus, many
fast inference methods for Dirichlet process mixture models have been proposed that can adapt α
to the data, including the works Escobar & West (1995) where learning of α is incorporated in the
Gibbs sampling analysis, Blei & Jordan (2006) where a Gamma prior is used in a conjugate manner
directly in the variational inference algorithm. Wang & Dunson (2011) also account for model uncertainty on the concentration parameter α in a Bayesian manner directly in the sequential inference
procedure. This approach can be computationally expensive, as discretization of the domain of α is
needed, and its stability highly depends on the initial distribution on α and on the range of values of
α. To the best of our knowledge, we are the first to analytically study the evolution and stability of
the adapted sequence of α’s in the online learning setting.
In this paper, we propose an adaptive non-Bayesian approach for adapting α motivated by largesample asymptotics, and call the resulting algorithm ASUGS (Adaptive SUGS). While the basic
idea behind ASUGS is directly related to the greedy approach of SUGS, the main contribution is
a novel low-complexity stable method for choosing the concentration parameter adaptively as new
data arrive, which greatly improves the clustering performance. We derive an upper bound on the
number of classes, logarithmic in the number of samples, and further prove that the sequence of
concentration parameters that results from this adaptive design is almost bounded. We finally prove,
that the conditional likelihood, which is the primary tool used for Bayesian-based online clustering,
is asymptotically Gaussian in the large-sample limit, implying that the clustering part of ASUGS
asymptotically behaves as a Gaussian classifier. Experiments show that our method outperforms
other state-of-the-art methods for online learning of DPMM’s.
The paper is organized as follows. In Section 2, we review the sequential inference framework for
DPMM’s that we will build upon, introduce notation and propose our adaptive modification. In
Section 3, the probabilistic data model is given and sequential inference steps are shown. Section
4 contains the growth rate analysis of the number of classes and the adaptively-designed concentration parameters, and Section 5 contains the Gaussian large-sample approximation to the conditional
likelihood. Experimental results are shown in Section 6 and we conclude in Section 7.

2

Sequential Inference Framework for DPMM

Here, we review the SUGS framework of Wang & Dunson (2011) for online clustering. Here, the
nonparametric nature of the Dirichlet process manifests itself as modeling mixture models with
countably infinite components. Let the observations be given by yi ∈ Rd , and γi to denote
the class label of the ith observation (a latent variable). We define the available information at
time i as y(i) = {y1 , . . . , yi } and γ (i−1) = {γ1 , . . . , γi−1 }. The online sequential updating and
greedy search (SUGS) algorithm is summarized next for completeness. Set γ1 = 1 and calculate
π(θ1 |y1 , γ1 ). For i ≥ 2,
1. Choose best class label for yi :

γi ∈ arg max1≤h≤ki−1 +1 P (γi = h|y(i) , γ (i−1) ).

2. Update the posterior distribution
f (yi |θγi )π(θγi |y(i−1) , γ (i−1) ).

using

yi , γi :

π(θγi |y(i) , γ (i) )

∝

where θh are the parameters of class h, f (yi |θh ) is the observation density conditioned on class
h and ki−1 is the number of classes created at time i − 1. The algorithm sequentially allocates
observations yi to classes based on maximizing the conditional posterior probability.
To calculate the posterior probability P (γi = h|y(i) , γ (i−1) ), define the variables:
def

def

Li,h (yi ) = P (yi |γi = h, y(i−1) , γ (i−1) ),

πi,h (α) = P (γi = h|α, y(i−1) , γ (i−1) )

From Bayes’ rule, P (γi = h|y(i) , γ (i−1) ) ∝ Li,h (yi )πi,h (α) for h = 1, . . . , ki−1 + 1. Here, α is
considered fixed at this iteration, and is not updated in a fully Bayesian manner.
According to the Dirichlet process prediction, the predictive probability of assigning observation yi
to a class h is:
 mi−1 (h)
, h = 1, . . . , ki−1
πi,h (α) = i−1+α
(1)
α
,
h = ki−1 + 1
i−1+α
2

Algorithm 1 Adaptive Sequential Updating and Greedy Search (ASUGS)
Input: streaming data {yi }∞
i=1 , rate parameter λ > 0.
Set γ1 = 1 and k1 = 1. Calculate π(θ1 |y1 , γ1 ).
for i ≥ 2: do
ki−1
(a) Update concentration parameter: αi−1 = λ+log(i−1)
.
o
n
(i)
L (yi )πi,h (αi−1 )
(b) Choose best label for yi :
γi ∼ {qh } = P 0 i,h
L 0 (yi )π 0 (αi−1 ) .
h

(c) Update posterior distribution:
end for

i,h

i,h

π(θγi |y(i) , γ (i) ) ∝ f (yi |θγi )π(θγi |y(i−1) , γ (i−1) ).

Pi−1
where mi−1 (h) = l=1 I(γl = h) counts the number of observations labeled as class h at time
i − 1, and α > 0 is the concentration parameter.
2.1

Adaptation of Concentration Parameter α

It is well known that the concentration parameter α has a strong influence on the growth of the number of classes Antoniak (1974). Our experiments show that in this sequential framework, the choice
of α is even more critical. Choosing a fixed α as in the online SVA algorithm of Lin (2013) requires
cross-validation, which is computationally prohibitive for large-scale data sets. Furthermore, in the
streaming data setting where no estimate on the data complexity exists, it is impractical to perform
cross-validation. Although the parameter α is handled from a fully Bayesian treatment in Wang &
Dunson (2011), a pre-specified grid of possible values α can take, say {αl }L
l=1 , along with the prior
distribution over them, needs to be chosen in advance. Storage and updating of a matrix of size
(ki−1 + 1) × L and further marginalization is needed to compute P (γi = h|y(i) , γ (i−1) ) at each
iteration i. Thus, we propose an alternative data-driven method for choosing α that works well in
practice, is simple to compute and has theoretical guarantees.
The idea is to start with a prior distribution on α that favors small α and shape it into a posterior
distribution using the data. Define pi (α) = p(α|y(i) , γ (i) ) as the posterior distribution formed at
time i, which will be used in ASUGS at time i + 1. Let p1 (α) ≡ p1 (α|y(1) , γ (1) ) denote the prior
for α, e.g., an exponential distribution p1 (α) = λe−λα . The dependence on y(i) and γ (i) is trivial
only at this first step. Then, by Bayes rule, pi (α) ∝ p(yi , γi |y(i−1) , γ (i−1) , α)p(α|y(i−1) , γ (i−1) ) ∝
pi−1 (α)πi,γi (α) where πi,γi (α) is given in (1). Once this update is made after the selection of γi , the
α to be used in the next selection step is the mean of the distribution pi (α), i.e., αi = E[α|y(i) , γ (i) ].
As will be shown in Section 5, the distribution pi (α) can be approximated by a Gamma distribution
with shape parameter ki and rate parameter λ + log i. Under this approximation, we have αi =
ki
λ+log i , only requiring storage and update of one scalar parameter ki at each iteration i.
The ASUGS algorithm is summarized in Algorithm 1. The selection step may be implemented
(i)
by sampling the probability mass function {qh }. The posterior update step can be efficiently performed by updating the hyperparameters as a function of the streaming data for the case of conjugate
distributions. Section 3 derives these updates for the case of multivariate Gaussian observations and
conjugate priors for the parameters.

3

Sequential Inference under Unknown Mean & Unknown Covariance

We consider the general case of an unknown mean and covariance for each class. The probabilistic
model for the parameters of each class is given as:
yi |µ, T ∼ N (·|µ, T),

µ|T ∼ N (·|µ0 , co T),

T ∼ W(·|δ0 , V0 )

(2)

where N (·|µ, T) denotes the multivariate normal distribution with mean µ and precision matrix
T, and W(·|δ, V) is the Wishart distribution with 2δ degrees of freedom and scale matrix V. The
d
follow a normal-Wishart joint distribution. The model (2) leads
parameters θ = (µ, T) ∈ Rd × S++
to closed-form expressions for Li,h (yi )’s due to conjugacy Tzikas et al. (2008).
To calculate the class posteriors, the conditional likelihoods of yi given assignment to class h and
the previous class assignments need to be calculated first. The conditional likelihood of yi given
3

assignment to class h and the history (y(i−1) , γ (i−1) ) is given by:
Z
Li,h (yi ) = f (yi |θh )π(θh |y(i−1) , γ (i−1) )dθh

(3)

Due to the conjugacy of the distributions, the posterior π(θh |y(i−1) , γ (i−1) ) always has the form:
(i−1)

π(θh |y(i−1) , γ (i−1) ) = N (µh |µh
(i−1)

(i−1)

(i−1)

(i−1)

, ch

(i−1)

Th )W(Th |δh

(i−1)

, Vh

)

(i−1)

where µh
, ch
, δh
, Vh
are hyperparameters that can be recursively computed as new
samples come in. The form of this recursive computation of the hyperparameters is derived in
(i)

Appendix A. For ease of interpretation and numerical stability, we define Σh :=
(i)
(i)
W(·|δh , Vh ).

(i)

(Vh )−1
(i)

2δh

as the

(i)
Σh

inverse of the mean of the Wishart distribution
The matrix
has the natural
interpretation as the covariance matrix of class h at iteration i. Once the γi th component is chosen,
the parameter updates for the γi th class become:
µ(i)
γi =

(i−1)

1

y +
(i−1) i

1 + cγi

cγi
1+

(i−1)
cγi

µγ(i−1)
i

(4)

(i−1)
c(i)
+1
γi = cγi

Σ(i)
γi =

1

(5)

(i−1)
2δγi
Σ(i−1)
(i−1) γi
+ 2δγi

δγ(i)
= δγ(i−1)
+
i
i

1

+

(i−1)

1 + 2δγi

1

(i−1)
cγi
(yi
(i−1)
+ cγi

− µγ(i−1)
)(yi − µ(i−1)
)T
γi
i

1
2

(6)
(7)

(0)

(i)

If the starting matrix Σh is positive definite, then all the matrices {Σh } will remain positive
definite. Let us return to the calculation of the conditional likelihood (3). By iterated integration, it
follows that:
!d/2
(i−1)
(i−1)
(i−1) −1/2
rh
ρd (δh
) det(Σh
)
Li,h (yi ) ∝
(i−1)
δh(i−1) + 21 (8)

(i−1)
2δh
rh
(i−1) T
(i−1) −1
(i−1)
) (Σh
) (yi − µh
)
1 + (i−1) (yi − µh
2δh

def

where ρd (a) =

Γ(a+ 21 )
Γ(a+ 1−d
2 )

(i−1) def

and rh

=

(i−1)

ch

(i−1)

1+ch

. A detailed mathematical derivation of this

conditional likelihood is included in Appendix B. We remark that for the new class h = ki−1 + 1,
Li,ki−1 +1 has the form (8) with the initial choice of hyperparameters r(0) , δ (0) , µ(0) , Σ(0) .

4

Growth Rate Analysis of Number of Classes & Stability

In this section, we derive a model for the posterior distribution pn (α) using large-sample approximations, which will allow us to derive growth rates on the number of classes and the sequence of
concentration parameters, showing that the number of classes grows as E[kn ] = O(log1+ n) for 
arbitarily small under certain mild conditions.
The probability density of the α parameter is updated at the jth step in the following fashion:
 α
innovation class chosen
j+α
,
pj+1 (α) ∝ pj (α) ·
1
otherwise
j+α
where only the α-dependent factors in the update are shown. The α-independent factors are absorbed
by the normalization to a probability density. Choosing the innovation class pushes mass toward
infinity while choosing any other class pushes mass toward zero. Thus there is a possibility that
the innovation probability grows in a undesired manner. We assess the growth of the number of
def
innovations rn = kn − 1 under simple assumptions on some likelihood functions that appear
naturally in the ASUGS algorithm.
Assuming that the initial distribution of α is p1 (α) = λe−λα , the distribution used at step n + 1 is
Qn−1
proportional to αrn j=1 (1 + αj )−1 e−λα . We make use of the limiting relation
4

Theorem 1. The following asymptotic behavior holds: limn→∞

log

Qn−1

α
j=1 (1+ j )
α log n

= 1.

Proof. See Appendix C.
Using Theorem 1, a large-sample model for pn (α) is αrn e−(λ+log n)α , suitably normalized. Recognizing this as the Gamma distribution with shape parameter rn + 1 and rate parameter λ + log n, its
rn +1
mean is given by αn = λ+log
n . We use the mean in this form to choose class membership in Alg. 1.
This asymptotic approximation leads to a very simple scalar update of the concentration parameter;
there is no need for discretization for tracking the evolution of continuous probability distributions
on α. In our experiments, this approximation is very accurate.
Recall that the innovation class is labeled K+ = kn−1 + 1 at the nth step. The modeled updates
randomly select a previous class or innovation (new class) by sampling from the probability distriP
K+
(n)
bution {qk = P (γn = k|y(n) , γ (n−1) )}k=1
. Note that n − 1 = k6=K+ mn (k) , where mn (k)
represents the number of members in class k at time n.
We assume the data follows the Gaussian mixture distribution:
def

pT (y) =

K
X

πh N (y|µh , Σh )

(9)

h=1

where πh are the prior probabilities, and µh , Σh are the parameters of the Gaussian clusters.
Define the mixture-model probability density function, which plays the role of the predictive distribution:
X mn−1 (k)
def
L̃n,K+ (y) =
Ln,k (y),
(10)
n−1
k6=K+

so that the probabilities of choosing a previous class or an innovation (using Equ. (1)) are proporP
mn−1 (k)
αn−1
(n−1)
tional to k6=K+ n−1+α
Ln,k (yn ) = n−1+α
L̃n,K+ (yn ) and n−1+α
Ln,K+ (yn ), respecn−1
n−1
n−1
tively. If τn−1 denotes the innovation probability at step n, then we have
!
(n − 1)L̃n,K+ (yn )
αn−1 Ln,K+ (yn )
= (τn−1 , 1 − τn−1 )
(11)
, ρn−1
ρn−1
n − 1 + αn−1
n − 1 + αn−1
for some positive proportionality factor ρn−1 .
Define the likelihood ratio (LR) at the beginning of stage n as 1 :
def

ln (y) =

Ln,K+ (y)
L̃n,K+ (y)

(12)

Conceptually, the mixture (10) represents a modeled distribution fitting the currently observed data.
If all “modes” of the data have been observed, it is reasonable to expect that L̃n,K+ is a good model
for future observations. The LR ln (yn ) is not large when the future observations are well-modeled
by (10). In fact, we expect L̃n,K+ → pT as n → ∞, as discussed in Section 5.


ln (yn )αn−1
ln (yn )αn−1
Lemma 1. The following bound holds: τn−1 = n−1+l
≤
min
,
1
.
n−1
n (yn )αn−1
Proof. The result follows directly from (11) after a simple calculation.
The innovation random variable rn is described by the random process associated with the probabilities of transition

τn ,
k = rn + 1
P (rn+1 = k|rn ) =
.
(13)
1 − τn , k = rn
1

def

Here, L0 (·) = Ln,K+ (·) is independent of n and only depends on the initial choice of hyperparameters
as discussed in Sec. 3.

5

The expectation of rn is majorized by the expectation of a similar random process, r̄n , based on the
def
transition probability σn = min( rna+1
, 1) instead of τn as Appendix D shows, where the random
n
−1
sequence {an } is given by ln+1 (yn+1 ) n(λ + log n). The latter can be described as a modification
of a Polya urn process with selection probability σn . The asymptotic behavior of rn and related
variables is described in the following theorem.
Theorem 2. Let τn be a sequence of real-valued random variables 0 ≤ τn ≤ 1 satisfying τn ≤ rna+1
n
for n ≥ N , where an = ln+1 (yn+1 )−1 n(λ + log n), and where the nonnegative, integer-valued
random variables rn evolve according to (13). Assume the following for n ≥ N :
1. ln (yn ) ≤ ζ

(a.s.)

2. D(pT k L̃n,K+ ) ≤ δ

(a.s.)

where D(p k q) is the Kullback-Leibler divergence between distributions p(·) and q(·). Then, as
n → ∞,
√
√
rn = OP (log1+ζ δ/2 n),
αn = OP (logζ δ/2 n)
(14)
Proof. See Appendix E.
Theorem 2 bounds the growth rate of the mean of the number of class innovations and the concentration parameter αn in terms of the sample size n and parameter ζ. The bounded LR and bounded
KL divergence conditions of Thm. 2 manifest themselves in the rate exponents of (14). The experiments section shows that both of the conditions of Thm. 2 hold for all iterations n ≥ N for
some N ∈ N. In fact, assuming the correct clustering, the mixture distribution L̃n,kn−1 +1 converges
to the true mixture distribution pT , implying that the number of class innovations grows at most
as O(log1+ n) and the sequence of concentration parameters is O(log n), where  > 0 can be
arbitrarily small.

5

Asymptotic Normality of Conditional Likelihood

In this section, we derive an asymptotic expression for the conditional likelihood (8) in order to gain
insight into the steady-state of the algorithm.
We let πh denote the true prior probability of class h. Using the bounds of the Gamma function
ρd (a)
in Theorem 1.6 from Batir (2008), it follows that lima→∞ e−d/2 (a−1/2)
d/2 = 1. Under normal
convergence conditions of the algorithm (with the pruning and merging steps included), all classes
h = 1, . . . , K will be correctly identified and populated with approximately ni−1 (h) ≈ πh (i − 1)
observations at time i − 1. Thus, the conditional class prior for each class h converges to πh as
i→∞
ni−1 (h)
π√
h
i → ∞, in virtue of (14), πi,h (αi−1 ) = i−1+α
−→ πh . According
=
ζ δ/2
i−1
1+

(i−1)
rh

(i−1)
ch

OP (log

(i−1))

i−1

(i−1)

to (5), we expect
→ 1 as i → ∞ since
∼ πh (i − 1). Also, we expect 2δh
∼
(i−1)
(i−1)
πh (i − 1) as i → ∞ according to (7). Also, from before, ρd (δh
) ∼ e−d/2 (δh
− 1/2)d/2 ∼
(i)
(i)
1 d/2
e−d/2 (πh i−1
. The parameter updates (4)-(7) imply µh → µh and Σh → Σh as i → ∞.
2 − 2)
This follows from the strong law of large numbers, as the updates are recursive implementations
of the sample mean and sample covariance matrix. Thus, the large-sample approximation to the
conditional likelihood becomes:

− i−1
−1
−1
πh
(i−1) T
(i−1) −1
(i−1)
2π
h
(y
−
µ
)
(Σ
)
(y
−
µ
)
lim
1
+
i→∞
i
i
h
h
h
i−1
i→∞
Li,h (yi ) ∝
(i−1) 1/2
limi→∞ det(Σh
)
1

i→∞

∝

T

−1

e− 2 (yi −µh ) Σh (yi −µh )
√
det Σh

(15)

where we used limu→∞ (1+ uc )u = ec . The conditional likelihood (15) corresponds to the multivariate Gaussian distribution with mean µh and covariance matrix Σh . A similar asymptotic normality
6

result was recently obtained in Tsiligkaridis & Forsythe (2015) for Gaussian observations with a von
(n)
(n)
(h)
Mises prior. The asymptotics mn−1
→ πh , µh → µh , Σh → Σh , Ln,h (y) → N (y|µh , Σh )
n−1
as n → ∞ imply that the mixture distribution L̃n,K+ in (10) converges to the true Gaussian mixture
distribution pT of (9). Thus, for any small δ, we expect D(pT k L̃n,K+ ) ≤ δ for all n ≥ N ,
validating the assumption of Theorem 2.

6

Experiments

We apply the ASUGS learning algorithm to a synthetic 16-class example and to a real data set, to
verify the stability and accuracy of our method. The experiments show the value of adaptation of
the Dirichlet concentration parameter for online clustering and parameter estimation.
Since it is possible that multiple clusters are similar and classes might be created due to outliers, or
due to the particular ordering of the streaming data sequence, we add the pruning and merging step
in the ASUGS algorithm as done in Lin (2013). We compare ASUGS and ASUGS-PM with SUGS,
SUGS-PM, SVA and SVA-PM proposed in Lin (2013), since it was shown in Lin (2013) that SVA
and SVA-PM outperform the block-based methods that perform iterative updates over the entire data
set including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Variational
Inference.
6.1

Synthetic Data set

We consider learning the parameters of a 16-class Gaussian mixture each with equal variance of
σ 2 = 0.025. The training set was made up of 500 iid samples, and the test set was made up of
1000 iid samples. The clustering results are shown in Fig. 1(a), showing that the ASUGS-based approaches are more stable than SVA-based algorithms. ASUGS-PM performs best and identifies the
correct number of clusters, and their parameters. Fig. 1(b) shows the data log-likelihood on the test
set (averaged over 100 Monte Carlo trials), the mean and variance of the number of classes at each iteration. The ASUGS-based approaches achieve a higher log-likelihood than SVA-based approaches
asymptotically. Fig. 6.1 provides some numerical verification for the assumptions of Theorem 2.
As expected, the predictive likelihood L̃i,K+ (10) converges to the true mixture distribution pT (9),
and the likelihood ratio li (yi ) is bounded after enough samples are processed.
SVA-PM

0

0

-2

-2

-4
-4

-2

0

2

4

-4
-4

-2

-2

ASUGS

0

2

4

ASUGS-PM

4

4

2

2

0

0

-2

-2

25

Mean Number of Classes

2

Avg. Joint Log-likelihood

4

2

20

-4

15

-6

-8

-10

-2

0

2

4

-4
-4

-2

0

2

4

5
0

0
-4
-4

ASUGS
ASUGS-PM
SUGS
SUGS-PM
SVA
SVA-PM

10

100

200

300

Iteration

400

500

0

100

200

300

Iteration

(a)

400

Variance of Number of Classes

SVA
4

500

5
4
3
2
1
0
0

100

200

300

400

500

Iteration

(b)

Figure 1: (a) Clustering performance of SVA, SVA-PM, ASUGS and ASUGS-PM on synthetic data
set. ASUGS-PM identifies the 16 clusters correctly. (b) Joint log-likelihood on synthetic data, mean
and variance of number of classes as a function of iteration. The likelihood values were evaluated on
a held-out set of 1000 samples. ASUGS-PM achieves the highest log-likelihood and has the lowest
asymptotic variance on the number of classes.

6.2

Real Data Set

We applied the online nonparametric Bayesian methods for clustering image data. We used the
MNIST data set, which consists of 60, 000 training samples, and 10, 000 test samples. Each sample
7

10000

3

9000
2.5

~ i;K+ ! pT k2
kL
2

8000

li (yi )

7000

2

6000
5000

1.5

4000

1

3000
2000

0.5

1000
0

0
100

200

300

400

500

Sample i

L

0

100

200

300

400

500

Sample i

(y )

Figure 2: Likelihood ratio li (yi ) = L̃i,K+ (yi ) (left) and L2 -distance between L̃i,K+ (·) and true
i
i,K+
mixture distribution pT (right) for synthetic example (see 1).
is a 28 × 28 image of a handwritten digit (total of 784 dimensions), and we perform PCA preprocessing to reduce dimensionality to d = 50 dimensions as in Kurihara et al. (2006).
We use only a random 1.667% subset, consisting of 1000 random samples for training. This training
set contains data from all 10 digits with an approximately uniform proportion. Fig. 3 shows the
predictive log-likelihood over the test set, and the mean images for clusters obtained using ASUGSPM and SVA-PM, respectively. We note that ASUGS-PM achieves higher log-likelihood values and
finds all digits correctly using only 23 clusters, while SVA-PM finds some digits using 56 clusters.
0

Predictive Log-Likelihood

-500

ASUGS-PM
SUGS-PM
SVA-PM

-1000
-1500
-2000
-2500
-3000
-3500
-4000
-4500
-5000
0

100

200

300

400

500

600

700

800

900

1000

Iteration

(a)

(b)

(c)

Figure 3: Predictive log-likelihood (a) on test set, mean images for clusters found using ASUGS-PM
(b) and SVA-PM (c) on MNIST data set.

6.3

Discussion

Although both SVA and ASUGS methods have similar computational complexity and use decisions
and information obtained from processing previous samples in order to decide on class innovations, the mechanics of these methods are quite different. ASUGS uses an adaptive α motivated
by asymptotic theory, while SVA uses a fixed α. Furthermore, SVA updates the parameters of all
the components at each iteration (in a weighted fashion) while ASUGS only updates the parameters
of the most-likely cluster, thus minimizing leakage to unrelated components. The λ parameter of
ASUGS does not affect performance as much as the threshold parameter  of SVA does, which often
leads to instability requiring lots of pruning and merging steps and increasing latency. This is critical for large data sets or streaming applications, because cross-validation would be required to set
 appropriately. We observe higher log-likelihoods and better numerical stability for ASUGS-based
methods in comparison to SVA. The mathematical formulation of ASUGS allows for theoretical
guarantees (Theorem 2), and asymptotically normal predictive distribution.

7

Conclusion

We developed a fast online clustering and parameter estimation algorithm for Dirichlet process mixtures of Gaussians, capable of learning in a single data pass. Motivated by large-sample asymptotics,
we proposed a novel low-complexity data-driven adaptive design for the concentration parameter
and showed it leads to logarithmic growth rates on the number of classes. Through experiments on
synthetic and real data sets, we show our method achieves better performance and is as fast as other
state-of-the-art online learning DPMM methods.
8

References
Antoniak, C. E. Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric
Problems. The Annals of Statistics, 2(6):1152–1174, 1974.
Batir, N. Inequalities for the Gamma Function. Archiv der Mathematik, 91(6):554–563, 2008.
Blei, D. M. and Jordan, M. I. Variational Inference for Dirichlet Process Mixtures. Bayesian Analysis, 1(1):121–144, 2006.
Daume, H. Fast Search for Dirichlet Process Mixture Models. In Conference on Artificial Intelligence and Statistics, 2007.
Escobar, M. D. and West, M. Bayesian Density Estimation and Inference using Mixtures. Journal
of the American Statistical Association, 90(430):577–588, June 1995.
Fearnhead, P. Particle Filters for Mixture Models with an Uknown Number of Components. Statistics and Computing, 14:11–21, 2004.
Kurihara, K., Welling, M., and Vlassis, N. Accelerated Variational Dirichlet Mixture Models. In
Advances in Neural Information Processing Systems (NIPS), 2006.
Lin, Dahua. Online learning of nonparametric mixture models via sequential variational approximation. In Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.Q. (eds.),
Advances in Neural Information Processing Systems 26, pp. 395–403. Curran Associates, Inc.,
2013.
Neal, R. M. Bayesian Mixture Modeling. In Proceedings of the Workshop on Maximum Entropy
and Bayesian Methods of Statistical Analysis, volume 11, pp. 197–211, 1992.
Neal, R. M. Markov chain sampling methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249–265, June 2000.
Rasmussen, C. E. The infinite gaussian mixture model. In Advances in Neural Information Processing Systems 12, pp. 554–560. MIT Press, 2000.
Tsiligkaridis, T. and Forsythe, K. W. A Sequential Bayesian Inference Framework for Blind Frequency Offset Estimation. In Proceedings of IEEE International Workshop on Machine Learning
for Signal Processing, Boston, MA, September 2015.
Tzikas, D. G., Likas, A. C., and Galatsanos, N. P. The Variational Approximation for Bayesian
Inference. IEEE Signal Processing Magazine, pp. 131–146, November 2008.
Wang, L. and Dunson, D. B. Fast Bayesian Inference in Dirichlet Process Mixture Models. Journal
of Computational and Graphical Statistics, 20(1):196–216, 2011.

9

"
4,5978,Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling,Poster,5978-covariance-controlled-adaptive-langevin-thermostat-for-large-scale-bayesian-sampling.pdf,"Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.","Covariance-Controlled Adaptive Langevin
Thermostat for Large-Scale Bayesian Sampling

Xiaocheng Shang∗
University of Edinburgh
x.shang@ed.ac.uk

Zhanxing Zhu∗
University of Edinburgh
zhanxing.zhu@ed.ac.uk

Benedict Leimkuhler
University of Edinburgh
b.leimkuhler@ed.ac.uk

Amos J. Storkey
University of Edinburgh
a.storkey@ed.ac.uk

Abstract
Monte Carlo sampling for Bayesian posterior inference is a common approach
used in machine learning. The Markov chain Monte Carlo procedures that are
used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior
distribution. An area of current research addresses the computational benefits of
stochastic gradient methods in this setting. Existing techniques rely on estimating
the variance or covariance of the subsampling error, and typically assume constant
variance. In this article, we propose a covariance-controlled adaptive Langevin
thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial
speedup over popular alternative schemes for large-scale machine learning applications.

1

Introduction

In machine learning applications, direct sampling with the entire large-scale dataset is computationally infeasible. For instance, standard Markov chain Monte Carlo (MCMC) methods [16], as well
as typical hybrid Monte Carlo (HMC) methods [3, 6, 9], require the calculation of the acceptance
probability and the creation of informed proposals based on the whole dataset.
In order to improve the computational efficiency, a number of stochastic gradient methods [4, 5, 20,
21] have been proposed in the setting of Bayesian sampling based on random (and much smaller)
subsets to approximate the likelihood of the whole dataset, thus substantially reducing the computational cost in practice. Welling and Teh proposed the so-called stochastic gradient Langevin
dynamics (SGLD) [21], combining the ideas of stochastic optimization [18] and traditional Brownian dynamics, with a sequence of stepsizes decreasing to zero. A fixed stepsize is often adopted
in practice which is the choice in this article as in Vollmer et al. [20], where a modified SGLD
(mSGLD) was also introduced that was designed to reduce the sampling bias.
SGLD generates samples from first order Brownian dynamics, and thus, with a fixed timestep, one
can show that it is unable to dissipate excess noise in gradient approximations while maintaining the
desired invariant distribution [4]. A stochastic gradient Hamiltonian Monte Carlo (SGHMC) method
was proposed by Chen et al. [4], which relies on second order Langevin dynamics and incorporates a
parameter-dependent diffusion matrix that is intended to effectively offset the stochastic perturbation
of the gradient. However, it is difficult to accommodate the additional diffusion term in practice.
∗

The first and second authors contributed equally, and the listed author order was decided by lot.

1

Moreover, as pointed out in [5], poor estimation of it may have a significant adverse influence on the
sampling of the target distribution; for example, the effective system temperature may be altered.
The “thermostat” idea, which is widely used in molecular dynamics [7, 13], was recently adopted
in the stochastic gradient Nosé-Hoover thermostat (SGNHT) by Ding et al. [5] in order to adjust
the kinetic energy during simulation in such a way that the canonical ensemble is preserved (i.e. so
that a prescribed constant temperature distribution is maintained). In fact, the SGNHT method is
essentially equivalent to the adaptive Langevin (Ad-Langevin) thermostat proposed earlier by Jones
and Leimkuhler [10] in the molecular dynamics setting (see [15] for discussions).
Despite the substantial interest generated by these methods, the mathematical foundation for
stochastic gradient methods has been incomplete. The underlying dynamics of the SGNHT
method [5] was taken up by Leimkuhler and Shang [15], together with the design of discretization schemes with high effective order of accuracy. SGNHT methods are designed based on the
assumption of constant noise variance. In this article, we propose a covariance-controlled adaptive
Langevin (CCAdL) thermostat, that can handle parameter-dependent noise, improving both robustness and reliability in practice, and which can effectively speed up the convergence to the desired
invariant distribution in large-scale machine learning applications.
The rest of the article is organized as follows. In Section 2, we describe the setting of Bayesian
sampling with noisy gradients and briefly review existing techniques. Section 3 considers the construction of the novel CCAdL method that can effectively dissipate parameter-dependent noise while
maintaining the correct distribution. Various numerical experiments are performed in Section 4 to
verify the usefulness of CCAdL in a wide range of large-scale machine learning applications. Finally, we summarize our findings in Section 5.

2

Bayesian Sampling with Noisy Gradients

In the typical setting of Bayesian sampling [3, 19], one is interested in drawing states from a posterior
distribution defined as
π(θ|X) ∝ π(X|θ)π(θ) ,
(1)
where θ ∈ RNd is the parameter vector of interest, X denotes the entire dataset, and, π(X|θ)
and π(θ) are the likelihood and prior distributions, respectively. We introduce a potential energy
function U (θ) by defining π(θ|X) ∝ exp(−βU (θ)), where β is a positive parameter and can be
interpreted as being proportional to the reciprocal temperature in an associated physical system, i.e.
β −1 = kB T (kB is the Boltzmann constant and T is the temperature). In practice, β is often set to
be unity for notational simplicity. Taking the logarithm of (1) yields
U (θ) = − log π(X|θ)−log π(θ) .
(2)
Assuming the data are independent and identically distributed (i.i.d.), the logarithm of the likelihood
can be calculated as
N
X
log π(X|θ) =
log π(xi |θ) ,
(3)
i=1

where N is the size of the entire dataset.
However, as already mentioned, it is computationally infeasible to deal with the entire large-scale
dataset at each timestep as would typically be required in MCMC and HMC methods. Instead, in
order to improve the efficiency, a random (and much smaller, i.e. n  N ) subset is preferred in
stochastic gradient methods, in which the likelihood of the dataset for given parameters is approximated by
n
NX
log π(X|θ) ≈
log π(xri |θ) ,
(4)
n i=1
where {xri }ni=1 represents a random subset of X. Thus, the “noisy” potential energy can be written
as
n
NX
Ũ (θ) = −
log π(xri |θ)−log π(θ) ,
(5)
n i=1
where the negative gradient of the potential is referred to as the “noisy” force, i.e. F̃(θ) = −∇Ũ (θ).
2

Our goal is to correctly sample the Gibbs distribution ρ(θ) ∝ exp(−βU (θ)) (1). As in [4, 5], the
gradient noise is assumed to be Gaussian with mean zero and unknown variance, in which case one
may rewrite the noisy force as
p
F̃(θ) = −∇U (θ)+ Σ(θ)M1/2 R ,
(6)
where M typically is a diagonal matrix, Σ(θ) represents the covariance
matrix
of
the
noise,
and,
p
R is a vector of i.i.d. standard normal random variables. Note that Σ(θ)M1/2 R here is actually
equivalent to N (0, Σ(θ)M).
In a typical setting 
of numerical integration withassociated stepsize h,one has 
p
√ p
hF̃(θ) = h −∇U (θ)+ Σ(θ)M1/2 R = −h∇U (θ)+ h
hΣ(θ) M1/2 R ,

(7)

2

and therefore, assuming a constant covariance matrix (i.e. Σ = σ I, where I is the identity matrix),
the SGNHT method by Ding et al. [5] has the following underlying dynamics, written as a standard
Itō stochastic differential equation (SDE) system [15]:
dθ = M−1 pdt ,
p
√
(8)
dp = −∇U (θ)dt+σ hM1/2 dW−ξpdt+ 2Aβ −1 M1/2 dWA ,
 T −1

−1
dξ = µ
p M p−Nd kB T dt ,
where, colloquially, dW and dWA represent vectors of independent
Wiener increments; and are
p
often informally denoted by N (0, dtI) [4]. The coefficient 2Aβ −1 M1/2 represents the strength
of artificial noise added into the system to improve ergodicity, and A, which can be termed the “effective friction”, is a positive parameter and proportional to the variance of the noise. The auxiliary
variable ξ ∈ R is governed by a Nosé-Hoover device [8, 17] via a negative feedback mechanism,
i.e. when the instantaneous temperature (average kinetic energy per degree of freedom) calculated
as
kB T = pT M−1 p/Nd
(9)
is below the target temperature, the “dynamical friction” ξ would decrease allowing an increase
of temperature, while ξ would increase when the temperature is above the target. µ is a coupling
parameter which is referred to as the “thermal mass” in the molecular dynamics setting.
Proposition 1 (See Jones and Leimkuhler [10]). The SGNHT method (8) preserves the modified
Gibbs (stationary) distribution:

¯ 2 /2 ,
ρ̃β (θ, p, ξ) = Z −1 exp (−βH(θ, p)) exp −βµ(ξ − ξ)
(10)
where Z is the normalizing constant, H(θ, p) = pT M−1 p/2+U (θ) is the Hamiltonian, and
ξ¯ = A+βhσ 2 /2 .
(11)
Proposition 1 tells us that the SGNHT method can adaptively dissipate excess noise pumped into
the system while maintaining the correct distribution. The variance of the gradient noise, σ 2 , does
not need to be known a priori. As long as σ 2 is constant, the auxiliary variable ξ will be able to
automatically find its mean value ξ¯ on the fly. However, with a parameter-dependent covariance
matrix Σ(θ), the SGNHT method (8) would not produce the required target distribution (10).
Ding et al. [5] claimed that it is reasonable to assume the covariance matrix Σ(θ) is constant when
the size of the dataset, N , is large, in which case the variance of the posterior of θ is small. The
magnitude of the posterior variance does not actually relate to the constancy of the Σ, however,
in general, Σ is not constant. Simply assuming the non-constancy of the Σ can have a significant
impact on the performance of the method (most notably the stability measured by the largest usable
stepsize). Therefore, it is essential to have an approach that can handle parameter-dependent noise.
In the following section, we propose a covariance-controlled thermostat that can effectively dissipate
parameter-dependent noise while maintaining the target stationary distribution.

3

Covariance-Controlled Adaptive Langevin Thermostat

As mentioned in the previous section, the SGNHT method (8) can only dissipate noise with a constant covariance matrix. When the covariance matrix becomes parameter-dependent, in general, a
parameter-dependent covariance matrix does not imply the required “thermal equilibrium”, i.e. the
system cannot be expected to converge to the desired invariant distribution (10), typically resulting
in poor estimation of functions of parameters of interest. In fact, in that case it is not clear whether
or not there exists an invariant distribution at all.
3

In order to construct a stochastic-dynamical system that preserves the canonical distribution, we
suggest adding a suitable damping (viscous) term to effectively dissipate the parameter-dependent
gradient noise. To this end, we propose the following covariance-controlled adaptive Langevin
(CCAdL) thermostat:
dθ = M−1 pdt ,
p
p
dp = −∇U (θ)dt+ hΣ(θ)M1/2 dW−(h/2)βΣ(θ)pdt−ξpdt+ 2Aβ −1 M1/2 dWA , (12)


dξ = µ−1 pT M−1 p−Nd kB T dt .
Proposition 2. The CCAdL thermostat (12) preserves the modified Gibbs (stationary)
distribution:

ρ̂β (θ, p, ξ) = Z −1 exp (−βH(θ, p)) exp −βµ(ξ −A)2 /2 .
(13)
Proof. The Fokker-Planck equation corresponding to (12) is
ρt = L† ρ := −M−1 p·∇θ ρ+∇U (θ)·∇p ρ+(h/2)∇p ·(Σ(θ)M∇p ρ)+(h/2)β∇p ·(Σ(θ)pρ)


+ξ∇p ·(pρ)+Aβ −1 ∇p ·(M∇p ρ)−µ−1 pT M−1 p−Nd kB T ∇ξ ρ .
Just insert ρ̂β (13) into the Fokker-Planck operator L† to see that it vanishes.
The incorporation of the parameter-dependent covariance matrix Σ(θ) in (12) is intended to offset
the covariance matrix coming from the gradient approximation. However, in practice, one does not
know Σ(θ) a priori. Thus instead one must estimate Σ(θ) during the simulation, a task which will
be addressed in Section 3.1. This procedure is related to the method used in the SGHMC method
proposed by Chen et al. [4], which uses dynamics of the following form:
dθ = M−1 pdt ,
p
p
(14)
dp = −∇U (θ)dt+ hΣ(θ)M1/2 dW−Apdt+ 2β −1 (AI−βhΣ(θ)/2)M1/2 dWA .
It can be shown that the SGHMC method preserves the Gibbs canonical distribution:
ρβ (θ, p) = Z −1 exp (−βH(θ, p)) .
(15)
Although both CCAdL (12) and SGHMC (14) preserve their respective invariant distributions, let
us note several advantages of the former over the latter in practice:
(i) CCAdL and SGHMC both require estimation of the covariance matrix Σ(θ) during simulation, which can be costly in high dimension. In numerical experiments, we have found
that simply using the diagonal of the covariance matrix, at significantly reduced computational cost, works quite well in CCAdL. By contrast, it is difficult to find a suitable value
of the parameter A in SGHMC since one has to make sure the matrix AI−βhΣ(θ)/2 is
positive semi-definite. One may attempt to use a large value of the “effective friction” A
and/or a small stepsize h. However, too-large a friction would essentially reduce SGHMC
to SGLD, which is not desirable, as pointed out in [4], while extremely small stepsize
would significantly impact the computational efficiency.
(ii) Estimation of the covariance matrix Σ(θ) unavoidably introduces additional noise in both
CCAdL and SGHMC. Nonetheless, CCAdL can still effectively control the system temperature (i.e. maintaining the correct distribution of the momenta) due to the use of the
stabilizing Nosé-Hoover control, while in SGHMC, poor estimation of the covariance matrix may lead to significant deviations of the system temperature (as well as the distribution
of the momenta), resulting in poor sampling of the parameters of interest.
3.1

Covariance Estimation of Noisy Gradients

Under the assumption that the noise of the stochastic gradient follows a normal distribution, we
apply a similar method to that of [2] to estimate the covariance matrix associated with the noisy
gradient. If we let g(θ; x) = ∇θ log π(x|θ) and assume that the size of subset n is large enough for
the central limit theorem to hold, we have


n
1
1X
g(θ t ; xri ) ∼ N Ex [g(θ t ; x)], It ,
(16)
n i=1
n
where It = Cov[g(θ t ; x)] is the covariance of the gradient at θ t . Given the noisy (stochastic)
Pn
gradient based on the current subset ∇Ũ (θ t ) = − N
i=1 g(θ t ; xri )−∇ log π(θ t ) and the clean
n
4

Algorithm 1 Covariance-Controlled Adaptive Langevin (CCAdL) Thermostat
1:
2:
3:
4:
5:
6:
7:
8:

Input: h, A, {κt }T̂t=1 .
Initialize θ 0 , p0 , I0 , and ξ0 = A.
for t = 1, 2, . . . , T̂ do
θ t = θ t−1 +pt−1 h;
Estimate Ît using Eq. (18);
√
2
pt = pt−1 −∇Ũ (θ t )h− h2 Nn Ît pt−1 h−ξt−1 pt−1 h+ 2AhN (0, I);
ξt = ξt−1 + pTt pt /Nd −1 h;
end for

(full) gradient ∇U (θ t ) = −
and thus

PN

g(θ t ; xi )−∇ log π(θ t ), we have Ex [∇Ũ (θ t )] = Ex [∇U (θ t )],


N2
It ,
(17)
∇Ũ (θ t ) = ∇U (θ t )+N 0,
n
i.e. Σ(θ t ) = N 2 It /n. Assuming θ t does not change dramatically over time, we use the moving
average update to estimate It :
Ît = (1−κt )Ît−1 +κt V(θ t ) ,
(18)
where κt = 1/t and
n
1 X
T
V(θ t ) =
(g(θ t ; xri )−ḡ(θ t )) (g(θ t ; xri )−ḡ(θ t ))
(19)
n−1 i=1
is the empirical covariance of the gradient. ḡ(θ t ) represents the mean gradient of the log likelihood
computed from a subset. As proved in [2], this estimator has a convergence order of O(1/N ).
i=1

As already mentioned, estimating the full covariance matrix is computationally infeasible in high
dimension. However, we have found that employing a diagonal approximation of the covariance
matrix (i.e. estimating the variance only along each dimension of the noisy gradient) works quite
well in practice, as demonstrated in Section 4.
The procedure of the CCAdL method is summarized in Algorithm 1, where we simply used M = I,
β = 1, and µ = Nd in order to be consistent with the original implementation of SGNHT [5].
Note that this is a simple, first order (in terms of the stepsize) algorithm. A recent article [15] has
introduced higher order of accuracy schemes which can improve accuracy, but our interest here is in
the direct comparison of the underlying machinery of SGHMC, SGNHT, and CCAdL, so we avoid
further modifications and enhancements related to timestepping at this stage.
In the following section, we compare the newly established CCAdL method with SGHMC and
SGNHT on various machine learning tasks to demonstrate the benefits of CCAdL in Bayesian sampling with a noisy gradient.

4
4.1

Numerical Experiments
Bayesian Inference for Gaussian Distribution

We first compare the performance of the newly established CCAdL method with SGHMC and
SGNHT for a simple task using synthetic data, i.e. Bayesian inference of both the mean and variance of a one-dimensional normal distribution. We apply the same experimental setting as in [5]. We
generated N = 100 samples from a standard normal distribution N (0, 1). We used the likelihood
function of N (xi |µ, γ −1 ) and assigned a Normal-Gamma distribution as their prior distribution, i.e.
µ, γ ∼ N (µ|0, γ)Gam(γ|1, 1). Then the corresponding posterior distribution is another NormalGamma distribution, i.e. (µ, γ)|X ∼ N (µ|µN , (κN γ)−1 )Gam(γ|αN , βN ), with
N
X
N
(xi − x̄)2
N x̄2
N x̄
,
κN = 1+N ,
αN = 1+ ,
βN = 1+
+
,
µN =
N +1
2
2
2(1+N )
i=1
PN
where x̄ = i=1 xi /N . A random subset of size n = 10 was selected at each timestep to approximate the full gradient, resulting in the
following stochastic gradients:
n
n
γN X
N +1 µ2 N X
∇µ Ũ = (N +1)µγ −
x ri ,
∇γ Ũ = 1−
+ +
(xr −µ)2 .
n i=1
2γ
2 2n i=1 i
5

It can be seen that the variance of the stochastic gradient noise is no longer constant and actually
depends on the size of the subset, n, and the values of µ and γ in each iteration. This directly violates
the constant noise variance assumption of SGNHT [5], while CCAdL adjusts to the varying noise
variance.
The marginal distributions of µ and γ obtained from various methods with different combinations
of h and A were compared and plotted in Figure 1, with Table 1 consisting of the corresponding
root mean square error (RMSE) of the distribution and autocorrelation time from 106 samples. In
most of the cases, both SGNHT and CCAdL easily outperform the SGHMC method possibly due
to the presence of the Nosé-Hoover device, with SGHMC only showing superiority with a small
value of h and a large value of A, neither of which is desirable in practice as discussed in Section 3.
Between SGNHT and the newly proposed CCAdL method, the latter achieves better performance in
each of the cases investigated, highlighting the importance of the covariance control with parameterdependent noise.
3

3

2

2

1

1

1

0
−0.5

0
−0.5

0
−0.5

2

0
µ

0.5

True
SGHMC
SGNHT
CCAdL

3
2
1

1

0
0.5

0
0.5

0
0.5

1
γ

1.5

(a) h = 0.001, A = 1

1
γ

0
µ

1.5

(b) h = 0.001, A = 10

0.5

True
SGHMC
SGNHT
CCAdL

2

1

4

True
SGHMC
SGNHT
CCAdL

3
2
1

3
Density

True
SGHMC
SGNHT
CCAdL

3
Density

0.5

Density

0
µ

True
SGHMC
SGNHT
CCAdL

Density

2

4

True
SGHMC
SGNHT
CCAdL

0
−0.5

0
µ

0.5

True
SGHMC
SGNHT
CCAdL

3
Density

Density

3

4

Density

True
SGHMC
SGNHT
CCAdL

Density

4

2
1

1
γ

1.5

(c) h = 0.01, A = 1

0
0.5

1
γ

1.5

(d) h = 0.01, A = 10

Figure 1: Comparisons of marginal distribution (density) of µ (top row) and γ (bottom row) with various
values of h and A indicated in each column. The peak region is highlighted in the inset.

Table 1: Comparisons of (RMSE, Autocorrelation time) of (µ, γ) of various methods for Bayesian inference
of the mean and variance of a Gaussian distribution.
Methods
h = 0.001, A = 1 h = 0.001, A = 10 h = 0.01, A = 1 h = 0.01, A = 10
SGHMC
(0.0148, 236.12)
(0.0029, 333.04)
(0.0531, 29.78)
(0.0132, 39.33)
SGNHT
(0.0037, 238.32)
(0.0035, 406.71)
(0.0044, 26.71)
(0.0043, 55.00)
CCAdL
(0.0034, 238.06)
(0.0031, 402.45)
(0.0021, 26.71) (0.0035, 54.43)

4.2

Large-scale Bayesian Logistic Regression

We then consider a Bayesian logistic regression model trained on the benchmark MNIST dataset
for binary classification of digits 7 and 9 using 12, 214 training data points, with a test set of size
2037. A 100-dimensional random projection of the original features was used. We used the likeliQN
T
hood function of π {xi , yi }N
i=1 |w ∝
i=1 1/ 1+exp(−yi w xi ) and the prior distribution of
T
π(w) ∝ exp(−w w/2). A subset of size n = 500 was used at each timestep. Since the dimensionality of this problem is not that high, a full covariance estimation was used for CCAdL.
We investigate in Figure 2 (top row) the convergence speed of each method through measuring test
log likelihood using the posterior mean against the number of passes over the entire dataset. CCAdL
displays significant improvements over SGHMC and SGNHT with different values of h and A:
(1) CCAdL converges much faster than the other two, which also indicates its faster mixing speed
and shorter burn-in period; (2) CCAdL shows robustness in different values of the effective friction
A, with SGHMC and SGNHT relying on a relative large value of A (especially for the SGHMC
method), which is intended to dominate the gradient noise.
To compare the sample quality obtained from each method, Figure 2 (bottom row) plots the twodimensional marginal posterior distribution in randomly selected dimensions of 2 and 5 based on
106 samples from each method after the burn-in period (i.e. we start to collect samples when the test
6

log likelihood stabilizes). The true (reference) distribution was obtained by a sufficiently long run of
standard HMC. We implemented 10 runs of standard HMC and found there was no variation between
these runs, which guarantees its qualification as the true (reference) distribution. Again, CCAdL
shows much better performance than SGHMC and SGNHT. Note that the contour of SGHMC does
not even fit in the region of the plot, and in fact it shows significant deviation even in the estimation
of the mean.

−500
SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

−700
−800
0

200
400
Number of Passes

−500
−600
−700
−800

600

0

−3

5
0

−800

300

0

10
5

300

True(HMC)
SGHMC
SGNHT
CCAdL

15
10
5
0

−5
0.03 0.035 0.04 0.045 0.05 0.055
w2

(a) h = 0.2×10−4

100
200
Number of Passes

x 10
True(HMC)
SGHMC
SGNHT
CCAdL

0

−5
0.03 0.035 0.04 0.045 0.05 0.055
w2

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

−700

−3

15

w5

5

w

−600

x 10
True(HMC)
SGHMC
SGNHT
CCAdL

10

100
200
Number of Passes

−500

−3

x 10
15

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

−400

w5

−600

−300

−400

Test Log Likelihood

−300

−400

Test Log Likelihood

Test Log Likelihood

−300

(b) h = 0.5×10−4

−5
0.03 0.035 0.04 0.045 0.05 0.055
w2

(c) h = 1×10−4

Figure 2: Comparisons of Bayesian logistic regression of various methods on the MNIST dataset of digits 7
and 9 with various values of h and A: (top row) test log likelihood using the posterior mean against the number
of passes over the entire dataset; (bottom row) two-dimensional marginal posterior distribution in (randomly
selected) dimensions 2 and 5 with A = 10 fixed, based on 106 samples from each method after the burn-in
period (i.e. we start to collect samples when the test log likelihood stabilizes). Magenta circle is the true
(reference) posterior mean obtained from standard HMC and crosses represent the sample means computed
from various methods. Ellipses represent iso-probability contours covering 95% probability mass. Note that
the contour of SGHMC is well beyond the scale of the plot especially in the large stepsize regime, in which
case we do not include it here.

4.3

Discriminative Restricted Boltzmann Machine (DRBM)

DRBM [11] is a self-contained non-linear classifier, and the gradient of its discriminative objective
can be explicitly computed. Due to the limited space, we refer the readers to [11] for more details.
We trained a DRBM on different large-scale multi-class datasets from the LIBSVM1 dataset collection, including connect-4, letter, and SensIT Vehicle acoustic. The detailed information of these
datasets are presented in Table 2.
We selected the number of hidden units using cross-validation to achieve their best results. Since the
dimension of parameters, Nd , is relatively high, we used only diagonal covariance matrix estimation
for CCAdL to significantly reduce the computational cost, i.e. estimating the variance only along
each dimension. The size of the subset was chosen as 500–1000 to obtain a reasonable variance
estimation. For each dataset, we chose the first 20% of the total number of passes over the entire
dataset as the burn-in period and collected the remaining samples for prediction.
Table 2: Datasets used in DRBM with corresponding parameter configurations.
Datasets
connect-4
letter
acoustic

training/test set
54,046/13,511
10,500/5,000
78,823/19,705

classes
3
26
3

features
126
16
50

hidden units
20
100
20

total number of parameters Nd
2603
4326
1083

The error rates computed by various methods on the test set using the posterior mean against the
number of passes over the entire dataset were plotted in Figure 3. It can be observed that SGHMC
and SGNHT only work well with a large value of the effective friction A, which corresponds to a
strong random walk effect and thus slows down the convergence. On the contrary, CCAdL works
1

http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/multiclass.html

7

reliably (much better than the other two) in a wide range of A, and more importantly in the large
stepsize regime, which speeds up the convergence rate in relation to the computational work performed. It can be easily seen that the performance of SGHMC heavily relies on using a small value
of h and a large value of A, which significantly limits its usefulness in practice.

0.29

0.29
0.28

0.27

0.27

50

100
150
Number of Passes

200

Test Error

0.15
0.1

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

100

100
150
Number of Passes

0.3

50

100
150
Number of Passes

(3a) acoustic, h = 0.2×10

200

−3

100
150
Number of Passes

200

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

0.25
0.2
0.15
0.1

200
300
Number of Passes

0.4

400

100

0.3

50

100
150
Number of Passes

(3b) acoustic, h = 0.5×10

200
300
Number of Passes

400

(2c) letter, h = 5×10−3

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

0.35

0.25

50

(1c) connect-4, h = 2×10−3

(2b) letter, h = 2×10−3

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

0.35

0.15

100

Test Error

0.4

0.3
0.29

0.27

200

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

0.2

400

(2a) letter, h = 1×10−3

0.25

50

0.1

200
300
Number of Passes

0.31

0.28

0.25

0.2

SGHMC, A=10
SGHMC, A=50
SGNHT, A=10
SGNHT, A=50
CCAdL, A=10
CCAdL, A=50

0.32

(1b) connect-4, h = 1×10−3

0.25
Test Error

0.3

0.28

(1a) connect-4, h = 0.5×10−3

Test Error

0.31

0.33

Test Error

0.3

SGHMC, A=10
SGHMC, A=50
SGNHT, A=10
SGNHT, A=50
CCAdL, A=10
CCAdL, A=50

0.32

200

−3

0.4

Test Error

Test Error

0.31

0.33

Test Error

SGHMC, A=10
SGHMC, A=50
SGNHT, A=10
SGNHT, A=50
CCAdL, A=10
CCAdL, A=50

0.32

Test Error

0.33

SGHMC, A=1
SGHMC, A=10
SGNHT, A=1
SGNHT, A=10
CCAdL, A=1
CCAdL, A=10

0.35

0.3

0.25

50

100
150
Number of Passes

200

−3

(3c) acoustic, h = 1×10

Figure 3: Comparisons of DRBM on datasets connect-4 (top row), letter (middle row), and acoustic (bottom
row) with various values of h and A indicated: test error rates of various methods using the posterior mean
against the number of passes over the entire dataset.

5

Conclusions and Future Work

In this article, we have proposed a novel CCAdL formulation that can effectively dissipate
parameter-dependent noise while maintaining a desired invariant distribution. CCAdL combines
ideas of SGHMC and SGNHT from the literature, but achieves significant improvements over each
of these methods in practice. The additional error introduced by covariance estimation is expected
to be small in a relative sense, i.e. substantially smaller than the error arising from the noisy gradient. Our findings have been verified in large-scale machine learning applications. In particular, we
have consistently observed that SGHMC relies on a small stepsize h and a large friction A, which
significantly reduces its usefulness in practice as discussed. The techniques presented in this article
could be of use in more general settings of large-scale Bayesian sampling and optimization, which
we leave for future work.
A naive nonsymmetric splitting method has been applied for CCAdL for fair comparison in this
article. However, we point out that optimal design of splitting methods in ergodic SDE systems has
been explored recently in the mathematics community [1, 13, 14]. Moreover, it has been shown
in [15] that a certain type of symmetric splitting method for the Ad-Langevin/SGNHT method with
a clean (full) gradient inherits the superconvergence property (i.e. fourth order convergence to the
invariant distribution for configurational quantities) recently demonstrated in the setting of Langevin
dynamics [12, 14]. We leave further exploration of this direction in the context of noisy gradients
for future work.
8

References
[1] A. Abdulle, G. Vilmart, and K. C. Zygalakis. Long time accuracy of Lie-Trotter splitting
methods for Langevin dynamics. SIAM Journal on Numerical Analysis, 53(1):1–16, 2015.
[2] S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient
Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning,
pages 1591–1598, 2012.
[3] S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov Chain Monte Carlo.
CRC Press, 2011.
[4] T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proceedings of the 31st International Conference on Machine Learning, pages 1683–1691, 2014.
[5] N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using
stochastic gradient thermostats. In Advances in Neural Information Processing Systems 27,
pages 3203–3211, 2014.
[6] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics
Letters B, 195(2):216–222, 1987.
[7] D. Frenkel and B. Smit. Understanding Molecular Simulation: From Algorithms to Applications, Second Edition. Academic Press, 2001.
[8] W. G. Hoover. Computational Statistical Mechanics, Studies in Modern Thermodynamics.
Elsevier Science, 1991.
[9] A. M. Horowitz. A generalized guided Monte Carlo algorithm. Physics Letters B, 268(2):247–
252, 1991.
[10] A. Jones and B. Leimkuhler. Adaptive stochastic methods for sampling driven molecular systems. The Journal of Chemical Physics, 135(8):084125, 2011.
[11] H. Larochelle and Y. Bengio. Classification using discriminative restricted Boltzmann machines. In Proceedings of the 25th International Conference on Machine Learning, pages
536–543, 2008.
[12] B. Leimkuhler and C. Matthews. Rational construction of stochastic numerical methods for
molecular sampling. Applied Mathematics Research eXpress, 2013(1):34–56, 2013.
[13] B. Leimkuhler and C. Matthews. Molecular Dynamics: With Deterministic and Stochastic
Numerical Methods. Springer, 2015.
[14] B. Leimkuhler, C. Matthews, and G. Stoltz. The computation of averages from equilibrium and
nonequilibrium Langevin molecular dynamics. IMA Journal of Numerical Analysis, 36(1):13–
79, 2016.
[15] B. Leimkuhler and X. Shang. Adaptive thermostats for noisy gradient systems. SIAM Journal
on Scientific Computing, 2016.
[16] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of
state calculations by fast computing machines. The Journal of Chemical Physics, 21(6):1087,
1953.
[17] S. Nosé. A unified formulation of the constant temperature molecular dynamics methods. The
Journal of Chemical Physics, 81(1):511, 1984.
[18] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22(2):400–407, 1951.
[19] C. Robert and G. Casella. Monte Carlo Statistical Methods, Second Edition. Springer, 2004.
[20] S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (Non-) asymptotic properties of stochastic
gradient Langevin dynamics. arXiv preprint arXiv:1501.00438, 2015.
[21] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning, pages 681–688, 2011.

9

"
5,5714,Robust Portfolio Optimization,Poster,5714-robust-portfolio-optimization.pdf,"We propose a robust portfolio optimization approach based on quantile statistics. The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data. The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.","Robust Portfolio Optimization

Fang Han
Department of Biostatistics
Johns Hopkins University
Baltimore, MD 21205
fhan@jhu.edu

Huitong Qiu
Department of Biostatistics
Johns Hopkins University
Baltimore, MD 21205
hqiu7@jhu.edu
Han Liu
Department of Operations Research
and Financial Engineering
Princeton University
Princeton, NJ 08544 hanliu@princeton.edu

Brian Caffo
Department of Biostatistics
Johns Hopkins University
Baltimore, MD 21205
bcaffo@jhsph.edu

Abstract
We propose a robust portfolio optimization approach based on quantile statistics.
The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the
risk of the estimated portfolio converges to the oracle optimal risk with parametric
rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover,
the rate of convergence quantifies that the size of the portfolio under management
is allowed to scale exponentially with the sample size of the historical data. The
empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness
in high dimensions, and by allowing serial dependence.

1

Introduction

Markowitz’s mean-variance analysis sets the basis for modern portfolio optimization theory [1].
However, the mean-variance analysis has been criticized for being sensitive to estimation errors in
the mean and covariance matrix of the asset returns [2, 3]. Compared to the covariance matrix,
the mean of the asset returns is more influential and harder to estimate [4, 5]. Therefore, many
studies focus on the global minimum variance (GMV) formulation, which only involves estimating
the covariance matrix of the asset returns.
Estimating the covariance matrix of asset returns is challenging due to the high dimensionality and
heavy-tailedness of asset return data. Specifically, the number of assets under management is usually
much larger than the sample size of exploitable historical data. On the other hand, extreme events
are typical in financial asset prices, leading to heavy-tailed asset returns.
To overcome the curse of dimensionality, structured covariance matrix estimators are proposed for
asset return data. [6] considered estimators based on factor models with observable factors. [7,
8, 9] studied covariance matrix estimators based on latent factor models. [10, 11, 12] proposed to
shrink the sample covariance matrix towards highly structured covariance matrices, including the
identity matrix, order 1 autoregressive covariance matrices, and one-factor-based covariance matrix
estimators. These estimators are commonly based on the sample covariance matrix. (sub)Gaussian
tail assumptions are required to guarantee consistency.
For heavy-tailed data, robust estimators of covariance matrices are desired. Classic robust covariance
matrix estimators include M -estimators, minimum volume ellipsoid (MVE) and minimum covari1

ance determinant (MCD) estimators, S-estimators, and estimators based on data outlyingness and
depth [13]. These estimators are specifically designed for data with very low dimensions and large
sample sizes. For generalizing the robust estimators to high dimensions, [14] proposed the Orthogonalized Gnanadesikan-Kettenring (OGK) estimator, which extends [15]’s estimator by re-estimating
the eigenvalues; [16, 17] studied shrinkage estimators based on Tyler’s M -estimator. However, although OGK is computationally tractable in high dimensions, consistency is only guaranteed under
fixed dimension. The shrunken Tylor’s M -estimator involves iteratively inverting large matrices.
Moreover, its consistency is only guaranteed when the dimension is in the same order as the sample size. The aforementioned robust estimators are analyzed under independent data points. Their
performance under time series data is questionable.
In this paper, we build on a quantile-based scatter matrix1 estimator, and propose a robust portfolio
optimization approach. Our contributions are in three aspects. First, we show that the proposed
method accommodates high dimensional data by allowing the dimension to scale exponentially
with sample size. Secondly, we verify that consistency of the proposed method is achieved without
any tail conditions, thus allowing for heavy-tailed asset return data. Thirdly, we consider weakly
dependent time series, and demonstrate how the degree of dependence affects the consistency of the
proposed method.

2

Background

In this section, we introduce the notation system, and provide a review on the gross-exposure constrained portfolio optimization that will be exploited in this paper.
2.1

Notation

Let v = (v1 , . . . , vd )T be a d-dimensional real vector, and M = [Mjk ] ∈ Rd1 ×d2 be a d1 × d2
matrix with Mjk as the (j, k) entry. For 0 < q < ∞, we define the `q vector norm of v as
Pd
kvkq := ( j=1 |vj |)1/q and the `∞ vector norm of v as kvk∞ := maxdj=1 |vj |. Let the matrix
qP
2
`max norm of M be kMkmax := maxjk |Mjk |, and the Frobenius norm be kMkF :=
jk Mjk .
d

Let X = (X1 , . . . , Xd )T and Y = (Y1 , . . . , Yd )T be two random vectors. We write X = Y if X
and Y are identically distributed. We use 1, 2, . . . to denote vectors with 1, 2, . . . at every entry.
2.2

Gross-exposure Constrained GMV Formulation

Under the GMV formulation, [18] found that imposing a no-short-sale constraint improves portfolio
efficiency. [19] relaxed the no-short-sale constraint by a gross-exposure constraint, and showed that
portfolio efficiency can be further improved.
Let X ∈ Rd be a random vector of asset returns. A portfolio is characterized by a vector of
investment allocations, w = (w1 , . . . , wd )T , among the d assets. The gross-exposure constrained
GMV portfolio optimization can be formulated as
min wT Σw s.t. 1T w = 1, kwk1 ≤ c.
(2.1)
w

Here Σ is the covariance matrix of X, 1T w = 1 is the budget constraint, and kwk1 ≤ c is the grossexposure constraint. c ≥ 1 is called the gross exposure constant, which controls the percentage
of long and short positions allowed in the portfolio [19]. The optimization problem (2.1) can be
converted into a quadratic programming problem, and solved by standard software [19].

3

Method

In this section, we introduce the quantile-based portfolio optimization approach. Let Z ∈ R be a
random variable with distribution function F , and {zt }Tt=1 be a sequence of observations from Z.
For a constant q ∈ [0, 1], we define the q-quantiles of Z and {zt }Tt=1 to be
Q(Z; q) = Q(F ; q) := inf{z : P(Z ≤ z) ≥ q},
n
o
b t }T ; q) := z (k) where k = min t : t ≥ q .
Q({z
t=1
T

1

A scatter matrix is defined to be any matrix proportional to the covariance matrix by a constant.

2

Here z (1) ≤ . . . ≤ z (T ) are the order statistics of {zt }Tt=1 . We say Q(Z; q) is unique if there
b t }T ; q) is unique if there exists a unique
exists a unique z such that P(Z ≤ z) = q. We say Q({z
t=1
(k)
z ∈ {z1 , . . . , zT } such that z = z . Following the estimator Qn [20], we define the population
and sample quantile-based scales to be
e 1/4) and σ
b
σ Q (Z) := Q(|Z − Z|;
bQ ({zt }Tt=1 ) := Q({|z
(3.1)
s − zt |}1≤s<t≤T ; 1/4).
Q
Q
Here Ze is an independent copy of Z. Based on σ and σ
b , we can further define robust scatter matrices for asset returns. In detail, let X = (X1 , . . . , Xd )T ∈ Rd be a random vector
representing the returns of d assets, and {Xt }Tt=1 be a sequence of observations from X, where
Xt = (Xt1 , . . . , Xtd )T . We define the population and sample quantile-based scatter matrices (QNE)
to be
bQ
bQ
RQ := [RQ
jk ] and R := [Rjk ],
b Q are given by
where the entries of RQ and R
Q
Q
b Q := σ
bQ ({Xtj }Tt=1 )2 ,
Rjj := σ (Xj )2 , R
jj
i
1h Q
2
Q
2
RQ
:=
,
σ
(X
+
X
)
−
σ
(X
−
X
)
j
k
j
k
jk
4
h
i
Q
T
2
Q
T
2
b Q := 1 σ
R
b
({X
+
X
}
)
−
σ
({X
−
X
}
)
.
tj
tk
tj
tk
t=1
t=1
jk
4
b Q is
Since σ
bQ can be computed using O(T log T ) time [20], the computational complexity of R
2
Q
b
O(d T log T ). Since T  d in practice, R can be computed almost as efficiently as the sample
covariance matrix, which has O(d2 T ) complexity.
Let w = (w1 , . . . , wd )T be the vector of investment allocations among the d assets. For a matrix
M, we define a risk function R : Rd × Rd×d → R by
R(w; M) := wT Mw.
When X has covariance matrix Σ, R(w; Σ) = Var(wT X) is the variance of the portfolio return,
wT X, and is employed as the objected function in the GMV formulation. However, estimating Σ
is difficult due to the heavy tails of asset returns. In this paper, we adopt R(w; RQ ) as a robust
alternative to the moment-based risk metric, R(w; Σ), and consider the following oracle portfolio
optimization problem:
wopt = argmin R(w; RQ ) s.t. 1T w = 1, kwk1 ≤ c.
(3.2)
w

Here kwk1 ≤ c is the gross-exposure constraint introduced in Section 2.2. In practice, RQ is
b Q onto the cone
unknown and has to be estimated. For convexity of the risk function, we project R
of positive definite matrices:


 Q
b − R
e Q = argminR 
R
R
max
(3.3)
s.t. R ∈ Sλ := {M ∈ Rd×d : MT = M, λmin Id  M  λmax Id }.
e Q . The optimization
Here λmin and λmax set the lower and upper bounds for the eigenvalues of R
problem (3.3) can be solved by a projection and contraction algorithm [21]. We summarize the
e Q , we formulate the empirical robust portfolio
algorithm in the supplementary material. Using R
optimization by
e Q ) s.t. 1T w = 1, kwk1 ≤ c.
e opt = argmin R(w; R
w
(3.4)
w

Remark 3.1. The robust portfolio optimization approach involves three parameters: λmin , λmax ,
and c. Empirically, setting λmin = 0.005 and λmax = ∞ proves to work well. c is typically provided
by investors for controlling the percentages of short positions. When a data-driven choice is desired,
we refer to [19] for a cross-validation-based approach.
Remark 3.2. The rationale behind the positive definite projection (3.3) lies in two aspects. First, in
order that the portfolio optimization is convex and well conditioned, a positive definite matrix with
lower bounded eigenvalues is needed. This is guaranteed by setting λmin > 0. Secondly, the projection (3.3) is more robust compared to the OGK estimate [14]. OGK induces positive definiteness
by re-estimating the eigenvalues using the variances of the principal components. Robustness is lost
when the data, possibly containing outliers, are projected onto the principal directions for estimating
the principal components.
3

Remark 3.3. We adopt the 1/4 quantile in the definitions of σ Q and σ
bQ to achieve 50% breakdown
point. However, we note that our methodology and theory carries through if 1/4 is replaced by any
absolute constant q ∈ (0, 1).

4

Theoretical Properties

In this section, we provide theoretical analysis of the proposed portfolio optimization approach. For
b opt , based on an estimate, R, of RQ , the next lemma shows that the error
an optimized portfolio, w
opt
b ; RQ ) and R(wopt ; RQ ) is essentially related to the estimation error in R.
between the risks R(w
opt
b
Lemma 4.1. Let w
be the solution to
min R(w; R) s.t. 1T w = 1, kwk1 ≤ c
(4.1)
w

for an arbitrary matrix R. Then, we have
b opt ; RQ ) − R(wopt ; RQ )| ≤ 2c2 kR − RQ kmax ,
|R(w
opt
where w
is the solution to the oracle portfolio optimization problem (3.2), and c is the grossexposure constant.
e opt ; RQ ), which relates to the rate of convergence
Next, we derive the rate of convergence for R(w
Q
Q
e − R kmax . To this end, we first introduce a dependence condition on the asset return series.
in kR
0
:= σ(Xt : t ≤ 0) and
Definition 4.2. Let {Xt }t∈Z be a stationary process. Denote by F−∞
Fn∞ := σ(Xt : t ≥ n) the σ-fileds generated by {Xt }t≤0 and {Xt }t≥n , respectively. The φ-mixing
coefficient is defined by
φ(n) :=
sup
|P(A | B) − P(A)|.
0
∞ ,P(B)>0
B∈F−∞
,A∈Fn

The process {Xt }t∈Z is φ-mixing if and only if limn→∞ φ(n) = 0.
Condition 1. {Xt ∈ Rd }t∈Z is a stationary process such that for any j 6= k ∈ {1, . . . , d},
{Xtj }t∈Z , {Xtj + Xtk }t∈Z , and {Xtj − Xtk }t∈Z are φ-mixing processes satisfying φ(n) ≤ 1/n1+
for any n > 0 and some constant  > 0.
The parameter  determines the rate of decay in φ(n), and characterizes the degree of dependence
in {Xt }t∈Z . Next, we introduce an identifiability condition on the distribution function of the asset
returns.
f = (X
e1 , . . . , X
ed )T be an independent copy of X1 . For any j 6= k ∈ {1, . . . , d},
Condition 2. Let X
ej |, |X1j + X1k − X
ej − X
ek |, and
let F1;j , F2;j,k , and F3;j,k be the distribution functions of |X1j − X
e
e
|X1j − X1k − Xj + Xk |. We assume there exist constants κ > 0 and η > 0 such that
d
inf
F (y) ≥ η
|y−Q(F ;1/4)|≤κ dy
for any F ∈ {F1;j , F2;j,k , F3;j,k : j 6= k = 1, . . . , d}.
Condition 2 guarantees the identifiability of the 1/4 quantiles, and is standard in the literature on
quantile statistics [22, 23]. Based on Conditions 1 and 2, we can present the rates of convergence
b Q and R
e Q.
for R
Theorem 4.3. Let {Xt }t∈Z be an absolutely continuous stationary process satisfying Conditions
1 and 2. Suppose log d/T → 0 as T → ∞. Then, for any α ∈ (0, 1) and T large enough, with
probability no smaller than 1 − 8α2 , we have
b Q − RQ kmax ≤ rT .
kR
(4.2)
Here the rate of convergence rT is defined by
r
n 2 h 4(1 + 2C )(log d − log α) 4C i2


rT = max 2
+
,
η
T
T
r
Q h
4(1 + 2C )(log d − log α) 4C io
4σmax
+
,
(4.3)
η
T
T
Q
Q
Q
Q
where
P∞ σmax1+:= max{σ (Xj ),Qσ (Xj + Xk ), σ (Xj − Xk ) : j 6= k ∈ {1, . . . , d}} and C :=
. Moreover, if R ∈ Sλ for Sλ defined in (3.3), we further have
k=1 1/k
e Q − RQ kmax ≤ 2rT .
kR
(4.4)
4

The implications of Theorem 4.3 are as follows.
Q
1. When the
p parameters η, , and σmax do not scale with T , the rate of convergence reduces
to OP ( log d/T ). Thus, the number of assets under management is allowed to scale
exponentially with sample size T . Compared to similar rates of convergence obtained
for sample-covariance-based estimators [24, 25, 9], we do not require any moment or tail
conditions, thus accommodating heavy-tailed asset return data.
2. The effect of serial dependence P
on the rate of convergence is characterized by C . Specif∞
ically, as  approaches 0, C = k=1 1/k 1+ increases towards infinity, inflating rT .  is
allowed to scale with T such that C = o(T / log d).
3. The rate of convergence rT is inversely related to the lower bound, η, on the marginal
density functions around the 1/4 quantiles. This is because when η is small, the distribution functions are flat around the 1/4 quantiles, making the population quantiles harder to
estimate.

e opt ; RQ ).
Combining Lemma 4.1 and Theorem 4.3, we obtain the rate of convergence for R(w
Theorem 4.4. Let {Xt }t∈Z be an absolutely continuous stationary process satisfying Conditions 1
and 2. Suppose that log d/T → 0 as T → ∞ and RQ ∈ Sλ . Then, for any α ∈ (0, 1) and T large
enough, we have
e opt ; RQ ) − R(wopt ; RQ )| ≤ 2c2 rT ,
|R(w
(4.5)
where rT is defined in (4.3) and c is the gross-exposure constant.
Theorem 4.4 shows that the risk of the estimated portfolio converges to the oracle optimal risk with
parametric rate rT . The number of assets, d, is allowed to scale exponentially with sample size T .
Moreover, the rate of convergence does not rely on any tail conditions on the distribution of the asset
returns.
For the rest of this section, we build the connection between the proposed robust portfolio optimization and its moment-based counterpart. Specifically, we show that they are consistent under the
elliptical model.
Definition 4.5. [26] A random vector X ∈ Rd follows an elliptical distribution with location µ ∈
Rd and scatter S ∈ Rd×d if and only if there exist a nonnegative random variable ξ ∈ R, a matrix
A ∈ Rd×r with rank(A) = r, a random vector U ∈ Rr independent from ξ and uniformly
distributed on the r-dimensional sphere, Sr−1 , such that
d

X = µ + ξAU .
T
Here S = AA has rank r. We denote X ∼ ECd (µ, S, ξ). ξ is called the generating variate.
Commonly used elliptical distributions include Gaussian distribution and t-distribution. Elliptical
distributions have been widely used for modeling financial return data, since they naturally capture
many stylized properties including heavy tails and tail dependence [27, 28, 29, 30, 31, 32]. The next
theorem relates RQ and R(w; RQ ) to their moment-based counterparts, Σ and R(w; Σ), under the
elliptical model.
Theorem 4.6. Let X = (X1 , . . . , Xd )T ∼ ECd (µ, S, ξ) be an absolutely continuous elliptical
f = (X
e1 , . . . , X
ed )T be an independent copy of X. Then, we have
random vector and X
RQ = mQ S
(4.6)
Q
for some constant m only depending on the distribution of X. Moreover, if 0 < Eξ 2 < ∞, we
have
RQ = cQ Σ and R(w; RQ ) = cQ R(w; Σ),
(4.7)
Q
where Σ = Cov(X) is the covariance matrix of X, and c is a constant given by
n (X + X − X
n (X − X
ej )2 1 o
ej − X
ek )2 1 o
j
j
k
cQ =Q
;
=Q
;
Var(Xj ) 4
Var(Xj + Xk )
4
n (X − X − X
o
2
ej + X
ek ) 1
j
k
=Q
;
.
(4.8)
Var(Xj − Xk )
4
Here the last two inequalities hold when Var(Xj + Xk ) > 0 and Var(Xj − Xk ) > 0.
5

By Theorem 4.6, under the elliptical model, minimizing the robust risk metric, R(w; RQ ), is equivalent with minimizing the standard moment-based risk metric, R(w; Σ). Thus, the robust portfolio
optimization (3.2) is equivalent to its moment-based counterpart (2.1) in the population level. Plugging (4.7) into (4.5) leads to the following theorem.
Theorem 4.7. Let {Xt }t∈Z be an absolutely continuous stationary process satisfying Conditions 1
and 2. Suppose that X1 ∼ ECd (µ, S, ξ) follows an elliptical distribution with covariance matrix
Σ, and log d/T → 0 as T → ∞. Then, we have
2c2
e opt ; Σ) − R(wopt ; Σ)| ≤ Q rT ,
|R(w
c
where c is the gross-exposure constant, cQ is defined in (4.8), and rT is defined in (4.3).
e opt , obtained from the robust portfolio
Thus, under the elliptical model, the optimal portfolio, w
optimization also leads to parametric rate of convergence for the standard moment-based risk.

5

Experiments

In this section, we investigate the empirical performance of the proposed portfolio optimization
approach. In Section 5.1, we demonstrate the robustness of the proposed approach using synthetic
heavy-tailed data. In Section 5.2, we simulate portfolio management using the Standard & Poor’s
500 (S&P 500) stock index data.
The proposed portfolio optimization approach (QNE) is compared with three competitors. These
competitors are constructed by replacing the covariance matrix Σ in (2.1) by commonly used covariance/scatter matrix estimators:
1. OGK: The orthogonalized Gnanadesikan-Kettenring estimator constructs a pilot scatter
matrix estimate using a robust τ -estimator of scale, then re-estimates the eigenvalues using
the variances of the principal components [14].
2. Factor: The principal factor estimator iteratively solves for the specific variances and the
factor loadings [33].
3. Shrink: The shrinkage estimator shrinkages the sample covariance matrix towards a onefactor covariance estimator[10].
5.1

Synthetic Data

Following [19], we construct the covariance matrix of the asset returns using a three-factor model:
Xj = bj1 f1 + bj2 f2 + bj3 f3 + εj , j = 1, . . . , d,
(5.1)
where Xj is the return of the j-th stock, bjk is the loadings of the j-th stock on factor fk , and εj is
the idiosyncratic noise independent of the three factors. Under this model, the covariance matrix of
the stock returns is given by
Σ = BΣf BT + diag(σ12 , . . . , σd2 ),
(5.2)
where B = [bjk ] is a d × 3 matrix consisting of the factor loadings, Σf is the covariance matrix
of the three factors, and σj2 is the variance of the noise εi . We adopt the covariance in (5.2) in our
simulations. Following [19], we generate the factor loadings B from a trivariate normal distribution,
Nd (µb , Σb ), where the mean, µb , and covariance, Σb , are specified in Table 1. After the factor
loadings are generated, they are fixed as parameters throughout the simulations. The covariance
matrix, Σf , of the three factors is also given in Table 1. The standard deviations, σ1 , . . . , σd , of the
idiosyncratic noises are generated independently from a truncated gamma distribution with shape
3.3586 and scale 0.1876, restricting the support to [0.195, ∞). Again these standard deviations are
fixed as parameters once they are generated. According to [19], these parameters are obtained by
fitting the three-factor model, (5.1), using three-year daily return data of 30 Industry Portfolios from
May 1, 2002 to Aug. 29, 2005. The covariance matrix, Σ, is fixed throughout the simulations. Since
we are only interested in risk optimization, we set the mean of the asset returns to be µ = 0. The
dimension of the stocks under consideration is fixed at d = 100.
Given the covariance matrix Σ, we generate the asset return data from the following three distributions.
D1 : multivariate Gaussian distribution, Nd (0, Σ);
6

Table 1: Parameters for generating the covariance matrix in Equation (5.2).
Parameters for factor loadings

1.8

2.0

risk

0.4
1.0

gross−exposure constant (c)

1.2

1.0

1.8

2.0

1.0

Factor
Shrink

1.0

1.2

1.4

1.6

1.8

gross−exposure constant (c)

Gaussian

2.0

1.6

1.8

2.0

QNE
OGK

Factor
Shrink

0.2
0.0

0.2

0.4

0.6

matching rate

0.8

QNE
OGK

1.4

elliptical log-normal

0.0

0.0

0.2

0.4

0.6

matching rate

0.8

Factor
Shrink

1.2

gross−exposure constant (c)

0.8

1.0

1.6

multivariate t

Gaussian
QNE
OGK

1.4

gross−exposure constant (c)

1.0

1.6

Factor
Shrink

0.6

1.4

Oracle
QNE
OGK

0.4

1.2

-0.2042
-0.0023
0.1930

0.2

0.4

risk

Factor
Shrink

0.2

0.2
1.0

-0.035
0.3156
-0.0023

0.8

Oracle
QNE
OGK

1.2507
-0.0350
-0.2042
1.0

1.0

0.01018
-0.00697
0.08686

0.8

Factor
Shrink

0.6

0.8

Oracle
QNE
OGK

0.02387
0.05395
-0.00697

0.4

risk

0.02915
0.02387
0.01018

Σf

0.6

1.0

0.7828
0.5180
0.4100

matching rate

Parameters for factor returns

Σb

0.6

µb

1.0

1.2

1.4

1.6

1.8

gross−exposure constant (c)

multivariate t

2.0

1.0

1.2

1.4

1.6

1.8

2.0

gross−exposure constant (c)

elliptical log-normal

Figure 1: Portfolio risks, selected number of stocks, and matching rates to the oracle optimal portfolios.
D2 : multivariate t distribution with degree of freedom 3 and covariance matrix Σ;
D2 : elliptical distribution with log-normal generating variate, log N (0, 2), and covariance matrix Σ.
Under each distribution, we generate asset return series of half a year (T = 126). We estimate
the covariance/scatter matrices using QNE and the three competitors, and plug them into (2.1) to
optimize the portfolio allocations. We also solve (2.1) with the true covariance matrix, Σ, to obtain
the oracle optimal portfolios as benchmarks. We range the gross-exposure constraint, c, from 1 to 2.
The results are based on 1,000 simulations.
b Σ) and the matching rates between the optimized portfolios
Figure 1 shows the portfolio risks R(w;
and the oracle optimal portfolios2 . Here the matching rate is defined as follows. For two portfolios
P1 and P2 , let S1 and S2 be the corresponding sets of selected assets, i.e., the assets for which
the T
weights, wS
i , are non-zero. The matching rate between P1 and P2 is defined as r(P1 , P2 ) =
|S1 S2 |/|S1 S2 |, where |S| denotes the cardinality of set S.
We note two observations from Figure 1. (i) The four estimators leads to comparable portfolio
risks under the Gaussian model D1 . However, under heavy-tailed distributions D2 and D3 , QNE
achieves lower portfolio risk. (ii) The matching rates of QNE are stable across the three models,
and are higher than the competing methods under heavy-tailed distributions D2 and D3 . Thus, we
conclude that QNE is robust to heavy tails in both risk minimization and asset selection.
5.2

Real Data

In this section, we simulate portfolio management using the S&P 500 stocks. We collect 1,258
adjusted daily closing prices3 for 435 stocks that stayed in the S&P 500 index from January 1, 2003
2

Due to the `1 regularization in the gross-exposure constraint, the solution is generally sparse.
The adjusted closing prices accounts for all corporate actions including stock splits, dividends, and rights
offerings.
3

7

Table 2: Annualized Sharpe ratios, returns, and risks under 4 competing approaches, using S&P 500
index data.

Sharpe ratio

c=1.0
c=1.2
c=1.4
c=1.6
c=1.8
c=2.0

QNE
2.04
1.89
1.61
1.56
1.55
1.53

OGK
1.64
1.39
1.24
1.31
1.48
1.51

Factor
1.29
1.22
1.34
1.38
1.41
1.43

Shrink
0.92
0.74
0.72
0.75
0.78
0.83

return (in %)

c=1.0
c=1.2
c=1.4
c=1.6
c=1.8
c=2.0

20.46
18.41
15.58
15.02
14.77
14.51

16.59
13.15
11.30
11.48
12.39
12.27

13.18
10.79
10.88
10.68
10.57
10.60

9.84
7.20
6.55
6.49
6.58
6.76

risk (in %)

c=1.0
c=1.2
c=1.4
c=1.6
c=1.8
c=2.0

10.02
9.74
9.70
9.63
9.54
9.48

10.09
9.46
9.10
8.75
8.39
8.13

10.19
8.83
8.12
7.71
7.51
7.43

10.70
9.76
9.14
8.68
8.38
8.18

to December 31, 2007. Using the closing prices, we obtain 1,257 daily returns as the daily growth
rates of the prices.
We manage a portfolio consisting of the 435 stocks from January 1, 2003 to December 31, 20074 .
On days i = 42, 43, . . . , 1, 256, we optimize the portfolio allocations using the past 2 months stock
return data (42 sample points). We hold the portfolio for one day, and evaluate the portfolio return
on day i + 1. In this way, we obtain 1,215 portfolio returns. We repeat the process for each of the
four methods under comparison, and range the gross-exposure constant c from 1 to 25 .
Since the true covariance matrix of the stock returns is unknown, we adopt the Sharpe ratio for
evaluating the performances of the portfolios. Table 2 summarizes the annualized Sharpe ratios,
mean returns, and empirical risks (i.e., standard deviations of the portfolio returns). We observe that
QNE achieves the largest Sharpe ratios under all values of the gross-exposure constant, indicating
the lowest risks under the same returns (or equivalently, the highest returns under the same risk).

6

Discussion

In this paper, we propose a robust portfolio optimization framework, building on a quantile-based
scatter matrix. We obtain non-asymptotic rates of convergence for the scatter matrix estimators and
the risk of the estimated portfolio. The relations of the proposed framework with its moment-based
counterpart are well understood.
The main contribution of the robust portfolio optimization approach lies in its robustness to heavy
tails in high dimensions. Heavy tails present unique challenges in high dimensions compared to
low dimensions.
For example, asymptotic theory of M -estimators guarantees consistency in the rate
p
OP ( d/n) even for non-Gaussian data [34, 35]. If d  n, statistical error diminishes rapidly with
increasing n. However, when d  n, statistical error may scale rapidly with dimension. Thus,
stringent tail conditions, such as subGaussian conditions, are required to guarantee consistency for
moment-based estimators in high dimensions [36]. In this paper, based on quantile statistics, we
achieve consistency for portfolio risk without assuming any tail conditions, while allowing d to
scale nearly exponentially with n.
Another contribution of his work lies in the theoretical analysis of how serial dependence may affect
consistency of the estimation. We measure the degree of serial dependence using the φ-mixing
coefficient, φ(n). We show that the effect of the serial dependence
P∞on the rate of convergence is
summarized by the parameter C , which characterizes the size of n=1 φ(n).
4

We drop the data after 2007 to avoid the financial crisis, when the stock prices are likely to violate the
stationary assumption.
5
c = 2 imposes a 50% upper bound on the percentage of short positions. In practice, the percentage of
short positions is usually strictly controlled to be much lower.

8

References
[1] Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91, 1952.
[2] Michael J Best and Robert R Grauer. On the sensitivity of mean-variance-efficient portfolios to changes
in asset means: some analytical and computational results. Review of Financial Studies, 4(2):315–342,
1991.
[3] Vijay Kumar Chopra and William T Ziemba. The effect of errors in means, variances, and covariances on
optimal portfolio choice. The Journal of Portfolio Management, 19(2):6–11, 1993.
[4] Robert C Merton. On estimating the expected return on the market: An exploratory investigation. Journal
of Financial Economics, 8(4):323–361, 1980.
[5] Jarl G Kallberg and William T Ziemba. Mis-specifications in portfolio selection problems. In Risk and
Capital, pages 74–87. Springer, 1984.
[6] Jianqing Fan, Yingying Fan, and Jinchi Lv. High dimensional covariance matrix estimation using a factor
model. Journal of Econometrics, 147(1):186–197, 2008.
[7] James H Stock and Mark W Watson. Forecasting using principal components from a large number of
predictors. Journal of the American Statistical Association, 97(460):1167–1179, 2002.
[8] Jushan Bai, Kunpeng Li, et al. Statistical analysis of factor models of high dimension. The Annals of
Statistics, 40(1):436–465, 2012.
[9] Jianqing Fan, Yuan Liao, and Martina Mincheva. Large covariance estimation by thresholding principal
orthogonal complements. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
75(4):603–680, 2013.
[10] Olivier Ledoit and Michael Wolf. Improved estimation of the covariance matrix of stock returns with an
application to portfolio selection. Journal of Empirical Finance, 10(5):603–621, 2003.
[11] Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance matrices. Journal of Multivariate Analysis, 88(2):365–411, 2004.
[12] Olivier Ledoit and Michael Wolf. Honey, I shrunk the sample covariance matrix. The Journal of Portfolio
Management, 30(4):110–119, 2004.
[13] Peter J Huber. Robust Statistics. Wiley, 1981.
[14] Ricardo A Maronna and Ruben H Zamar. Robust estimates of location and dispersion for highdimensional datasets. Technometrics, 44(4):307–317, 2002.
[15] Ramanathan Gnanadesikan and John R Kettenring. Robust estimates, residuals, and outlier detection with
multiresponse data. Biometrics, 28(1):81–124, 1972.
[16] Yilun Chen, Ami Wiesel, and Alfred O Hero. Robust shrinkage estimation of high-dimensional covariance
matrices. IEEE Transactions on Signal Processing, 59(9):4097–4107, 2011.
[17] Romain Couillet and Matthew R McKay. Large dimensional analysis and optimization of robust shrinkage
covariance matrix estimators. Journal of Multivariate Analysis, 131:99–120, 2014.
[18] Ravi Jagannathan and T Ma. Risk reduction in large portfolios: Why imposing the wrong constraints
helps. The Journal of Finance, 58(4):1651–1683, 2003.
[19] Jianqing Fan, Jingjin Zhang, and Ke Yu. Vast portfolio selection with gross-exposure constraints. Journal
of the American Statistical Association, 107(498):592–606, 2012.
[20] Peter J Rousseeuw and Christophe Croux. Alternatives to the median absolute deviation. Journal of the
American Statistical Association, 88(424):1273–1283, 1993.
[21] M. H. Xu and H. Shao. Solving the matrix nearness problem in the maximum norm by applying a
projection and contraction method. Advances in Operations Research, 2012:1–15, 2012.
[22] Alexandre Belloni and Victor Chernozhukov. `1 -penalized quantile regression in high-dimensional sparse
models. The Annals of Statistics, 39(1):82–130, 2011.
[23] Lan Wang, Yichao Wu, and Runze Li. Quantile regression for analyzing heterogeneity in ultra-high
dimension. Journal of the American Statistical Association, 107(497):214–222, 2012.
[24] Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics,
36(6):2577–2604, 2008.
[25] T Tony Cai, Cun-Hui Zhang, and Harrison H Zhou. Optimal rates of convergence for covariance matrix
estimation. The Annals of Statistics, 38(4):2118–2144, 2010.
[26] Kai-Tai Fang, Samuel Kotz, and Kai Wang Ng. Symmetric Multivariate and Related Distributions. Chapman and Hall, 1990.
[27] Harry Joe. Multivariate Models and Dependence Concepts. Chapman and Hall, 1997.
[28] Rafael Schmidt. Tail dependence for elliptically contoured distributions. Mathematical Methods of Operations Research, 55(2):301–327, 2002.
[29] Svetlozar Todorov Rachev. Handbook of Heavy Tailed Distributions in Finance. Elsevier, 2003.
[30] Svetlozar T Rachev, Christian Menn, and Frank J Fabozzi. Fat-tailed and Skewed Asset Return Distributions: Implications for Risk Management, Portfolio Selection, and Option Pricing. Wiley, 2005.
[31] Kevin Dowd. Measuring Market Risk. Wiley, 2007.
[32] Torben Gustav Andersen. Handbook of Financial Time Series. Springer, 2009.
[33] Jushan Bai and Shuzhong Shi. Estimating high dimensional covariance matrices and its applications.
Annals of Economics and Finance, 12(2):199–215, 2011.
[34] Sara Van De Geer and SA Van De Geer. Empirical Processes in M -estimation. Cambridge University
Press, Cambridge, 2000.
[35] Alastair R Hall. Generalized Method of Moments. Oxford University Press, Oxford, 2005.
[36] Peter Bühlmann and Sara Van De Geer. Statistics for High-dimensional Data: Methods, Theory and
Applications. Springer, 2011.

9

"
6,5937,Logarithmic Time Online Multiclass prediction,Spotlight,5937-logarithmic-time-online-multiclass-prediction.pdf,"We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.","Logarithmic Time Online Multiclass prediction
Anna Choromanska
Courant Institute of Mathematical Sciences
New York, NY, USA
achoroma@cims.nyu.edu

John Langford
Microsoft Research
New York, NY, USA
jcl@microsoft.com

Abstract
We study the problem of multiclass classification with an extremely large number
of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches
for constructing logarithmic depth trees. On the theoretical front, we formulate a
new objective function, which is optimized at each node of the tree and creates
dynamic partitions of the data which are both pure (in terms of class labels) and
balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective
function at the nodes is challenging to optimize computationally. We address the
empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in
test error compared to more common logarithmic training time approaches, which
makes it a plausible method in computationally constrained large-k applications.

1

Introduction

The central problem of this paper is computational complexity in a setting where the number of
classes k for multiclass prediction is very large. Such problems occur in natural language (Which
translation is best?), search (What result is best?), and detection (Who is that?) tasks. Almost all
machine learning algorithms (with the exception of decision trees) have running times for multiclass
classification which are O(k) with a canonical example being one-against-all classifiers [1].
In this setting, the most efficient possible accurate approach is given by information theory [2].
In essence, any multiclass classification algorithm must uniquely specify the bits of all labels that
it predicts correctly on. Consequently, Kraft’s inequality ([2] equation 5.6) implies that the expected computational complexity of predicting correctly is ⌦(H(Y )) per example where H(Y ) is
the Shannon entropy of the label. For the worst case distribution on k classes, this implies ⌦(log(k))
computation is required.

Hence, our goal is achieving O(log(k)) computational time per example1 for both training and
testing, while effectively using online learning algorithms to minimize passes over the data.
The goal of logarithmic (in k) complexity naturally motivates approaches that construct a logarithmic depth hierarchy over the labels, with one label per leaf. While this hierarchy is sometimes
available through prior knowledge, in many scenarios it needs to be learned as well. This naturally
leads to a partition problem which arises at each node in the hierarchy. The partition problem is
finding a classifier: c : X ! { 1, 1} which divides examples into two subsets with a purer set of
labels than the original set. Definitions of purity vary, but canonical examples are the number of
labels remaining in each subset, or softer notions such as the average Shannon entropy of the class
labels. Despite resulting in a classifier, this problem is fundamentally different from standard binary
classification. To see this, note that replacing c(x) with c(x) is very bad for binary classification,
but has no impact on the quality of a partition2 . The partition problem is fundamentally non-convex
1
2

Throughout the paper by logarithmic time we mean logarithmic time per example.
The problem bears parallels to clustering in this regard.

1

for symmetric classes since the average c(x) 2 c(x) of c(x) and c(x) is a poor partition (the always-0
function places all points on the same side).
The choice of partition matters in problem dependent ways. For example, consider examples on a
line with label i at position i and threshold classifiers. In this case, trying to partition class labels
{1, 3} from class label 2 results in poor performance.

accuracy

The partition problem is typically solved for decision tree learning via an enumerate-and-test approach amongst a small set of possible classifiers (see e.g. [3]). In the multiclass setting, it is
desirable to achieve substantial error reduction for each node in the tree which motivates using a richer set of classifiers in the nodes to minimize the number of nodes, and thereby decrease the computational complexity. The main theoretical contribution of this work is to establish a boosting algorithm for learning trees with O(k) nodes and O(log k) depth, thereby addressing the goal of logarithmic time train and test complexity. Our main theoretical result,
presented in Section 2.3, generalizes a binary boosting-by-decision-tree theorem [4] to multiclass boosting. As in all boosting results, performance is critically dependent on the quality
of the weak learner, supporting intuition that we need sufficiently rich partitioners at nodes.
The approach uses a new objective for decision tree learning, which we optimize at each
node of the tree. The objective and its theoretical properties are presented in Section 2.
A complete system with multiple partitions
LOMtree vs one−against−all
could be constructed top down (as the boost1
OAA
ing theorem) or bottom up (as Filter tree [5]).
LOMtree
A bottom up partition process appears impossi0.8
ble with representational constraints as shown
in Section 6 in the Supplementary material so
we focus on top-down tree creation.
0.6
Whenever there are representational constraints
on partitions (such as linear classifiers), finding a strong partition function requires an efficient search over this set of classifiers. Ef0.2
ficient searches over large function classes are
routinely performed via gradient descent tech0
niques for supervised learning, so they seem
26
105
1000
21841 105033
number of classes
like a natural candidate. In existing literature,
Figure 1: A comparison of One-Against- examples for doing this exist when the problem
All (OAA) and the Logarithmic Online Multi- is indeed binary, or when there is a prespeciclass Tree (LOMtree) with One-Against-All con- fied hierarchy over the labels and we just need
strained to use the same training time as the to find partitioners aligned with that hierarchy.
LOMtree by dataset truncation and LOMtree con- Neither of these cases applies—we have multistrained to use the same representation complex- ple labels and want to dynamically create the
ity as One-Against-All. As the number of class choice of partition, rather than assuming that
labels grows, the problem becomes harder and the one was handed to us. Does there exist a purity criterion amenable to a gradient descent apLOMtree becomes more dominant.
proach? The precise objective studied in theory
fails this test due to its discrete nature, and even natural approximations are challenging to tractably
optimize under computational constraints. As a result, we use the theoretical objective as a motivation and construct a new Logarithmic Online Multiclass Tree (LOMtree) algorithm for empirical
evaluation.
0.4

Creating a tree in an online fashion creates a new class of problems. What if some node is initially
created but eventually proves useless because no examples go to it? At best this results in a wasteful
solution, while in practice it starves other parts of the tree which need representational complexity.
To deal with this, we design an efficient process for recycling orphan nodes into locations where
they are needed, and prove that the number of times a node is recycled is at most logarithmic in the
number of examples. The algorithm is described in Section 3 and analyzed in Section 3.1.
And is it effective? Given the inherent non-convexity of the partition problem this is unavoidably
an empirical question which we answer on a range of datasets varying from 26 to 105K classes in
Section 4. We find that under constrained training times, this approach is quite effective compared
to all baselines while dominating other O(log k) train time approaches.
What’s new? To the best of our knowledge, the splitting criterion, the boosting statement, the
LOMtree algorithm, the swapping guarantee, and the experimental results are all new here.

2

1.1

Prior Work

Only a few authors address logarithmic time training. The Filter tree [5] addresses consistent (and
robust) multiclass classification, showing that it is possible in the statistical limit. The Filter tree
does not address the partition problem as we do here which as shown in our experimental section is
often helpful. The partition finding problem is addressed in the conditional probability tree [6], but
that paper addresses conditional probability estimation. Conditional probability estimation can be
converted into multiclass prediction [7], but doing so is not a logarithmic time operation.
Quite a few authors have addressed logarithmic testing time while allowing training time to be O(k)
or worse. While these approaches are intractable on our larger scale problems, we describe them
here for context. The partition problem can be addressed by recursively applying spectral clustering
on a confusion graph [8] (other clustering approaches include [9]). Empirically, this approach has
been found to sometimes lead to badly imbalanced splits [10]. In the context of ranking, another
approach uses k-means hierarchical clustering to recover the label sets for a given partition [11].
The more recent work [12] on the multiclass classification problem addresses it via sparse output
coding by tuning high-cardinality multiclass categorization into a bit-by-bit decoding problem. The
authors decouple the learning processes of coding matrix and bit predictors and use probabilistic
decoding to decode the optimal class label. The authors however specify a class similarity which is
O(k 2 ) to compute (see Section 2.1.1 in [12]), and hence this approach is in a different complexity
class than ours (this is also born out experimentally). The variant of the popular error correcting
output code scheme for solving multi-label prediction problems with large output spaces under the
assumption of output sparsity was also considered in [13]. Their approach in general requires O(k)
running time to decode since, in essence, the fit of each label to the predictions must be checked
and there are O(k) labels. Another approach [14] proposes iterative least-squares-style algorithms
for multi-class (and multi-label) prediction with relatively large number of examples and data dimensions, and the work of [15] focusing in particular on the cost-sensitive multiclass classification.
Both approaches however have O(k) training time.

Decision trees are naturally structured to allow logarithmic time prediction. Traditional decision
trees often have difficulties with a large number of classes because their splitting criteria are not
well-suited to the large class setting. However, newer approaches [16, 17] have addressed this effectively at significant scales in the context of multilabel classification (multilabel learning, with
missing labels, is also addressed in [18]). More specifically, the first work [16] performs brute force
optimization of a multilabel variant of the Gini index defined over the set of positive labels in the
node and assumes label independence during random forest construction. Their method makes fast
predictions, however has high training costs [17]. The second work [17] optimizes a rank sensitive
loss function (Discounted Cumulative Gain). Additionally, a well-known problem with hierarchical
classification is that the performance significantly deteriorates lower in the hierarchy [19] which
some authors solve by biasing the training distribution to reduce error propagation while simultaneously combining bottom-up and top-down approaches during training [20].
The reduction approach we use for optimizing partitions implicitly optimizes a differential objective.
A non-reductive approach to this has been tried previously [21] on other objectives yielding good
results in a different context.

2

Framework and theoretical analysis

In this section we describe the essential elements of the approach, and outline the theoretical properties of the resulting framework. We begin with high-level ideas.
2.1

Setting

We employ a hierarchical approach for learning a multiclass decision tree structure, training this
structure in a top-down fashion. We assume that we receive examples x 2 X ✓ Rd , with labels
y 2 {1, 2, . . . , k}. We also assume access to a hypothesis class H where each h 2 H is a binary
classifier, h : X 7! { 1, 1}. The overall objective is to learn a tree of depth O(log k), where
each node in the tree consists of a classifier from H. The classifiers are trained in such a way that
hn (x) = 1 (hn denotes the classifier in node n of the tree3 ) means that the example x is sent to the
right subtree of node n, while hn (x) = 1 sends x to the left subtree. When we reach a leaf, we
predict according to the label with the highest frequency amongst the examples reaching that leaf.
3
Further in the paper we skip index n whenever it is clear from the context that we consider a fixed tree
node.

3

In the interest of computational complexity, we want to encourage the number of examples going
to the left and right to be fairly balanced. For good statistical accuracy, we want to send examples
of class i almost exclusively to either the left or the right subtree, thereby refining the purity of the
class distributions at subsequent levels in the tree. The purity of a tree node is therefore a measure
of whether the examples of each class reaching the node are then mostly sent to its one child node
(pure split) or otherwise to both children (impure split). The formal definitions of balancedness and
purity are introduced in Section 2.2. An objective expressing both criteria4 and resulting theoretical
properties are illustrated in the following sections. A key consideration in picking this objective is
that we want to effectively optimize it over hypotheses h 2 H, while streaming over examples in
an online fashion5 . This seems unsuitable with some of the more standard decision tree objectives
such as Shannon or Gini entropy, which leads us to design a new objective. At the same time, we
show in Section 2.3 that under suitable assumptions, optimizing the objective also leads to effective
reduction of the average Shannon entropy over the entire tree.
2.2

An objective and analysis of resulting partitions

We now define a criterion to measure the quality of a hypothesis h 2 H in creating partitions at a
fixed node n in the tree. Let ⇡i denotes the proportion of label i amongst the examples reaching this
node. Let P (h(x) > 0) and P (h(x) > 0|i) denote the fraction of examples reaching n for which
h(x) > 0, marginally and conditional on class i respectively. Then we define the objective6 :
k
X
J(h) = 2
⇡i |P (h(x) > 0) P (h(x) > 0|i)| .
(1)
i=1

We aim to maximize the objective J(h) to obtain high quality partitions. Intuitively, the objective
encourages the fraction of examples going to the right from class i to be substantially different from
the background fraction for each class i. As a concrete simple scenario, if P (h(x) > 0) = 0.5 for
some hypothesis h, then the objective prefers P (h(x) > 0|i) to be as close to 0 or 1 as possible for
each class i, leading to pure partitions. We now make these intuitions more formal.
Definition 1 (Purity). The hypothesis h 2 H induces a pure split if
k
X
↵ :=
⇡i min(P (h(x) > 0|i), P (h(x) < 0|i))  ,
where

i=1

2 [0, 0.5), and ↵ is called the purity factor.

In particular, a partition is called maximally pure if ↵ = 0, meaning that each class is sent exclusively
to the left or the right. We now define a similar definition for the balancedness of a split.
Definition 2 (Balancedness). The hypothesis h 2 H induces a balanced split if
c  P (h(x) > 0)  1 c,
{z
}
|
where c 2 (0, 0.5], and

=

is called the balancing factor.

A partition is called maximally balanced if = 0.5, meaning that an equal number of examples
are sent to the left and right children of the partition. The balancing factor and the purity factor
are related as shown in Lemma 1 (the proofs of Lemma 1 and the following lemma (Lemma 2) are
deferred to the Supplementary material).
Lemma 1. For any hypothesis h, and any distribution over examples (x, y), the purity factor ↵ and
the balancing factor satisfy ↵  min{(2 J(h))/(4 )
, 0.5}.
A partition is called maximally pure and balanced if it satisfies both ↵ = 0 and = 0.5. We see
that J(h) = 1 for a hypothesis h inducing a maximally pure and balanced partition as captured in
the next lemma. Of course we do not expect to have hypotheses producing maximally pure and
balanced splits in practice.
Lemma 2. For any hypothesis h : X 7! { 1, 1}, the objective J(h) satisfies J(h) 2 [0, 1].
Furthermore, if h induces a maximally pure and balanced partition then J(h) = 1.
4

We want an objective to achieve its optimum for simultaneously pure and balanced split. The standard
entropy-based criteria, such as Shannon or Gini entropy, as well as the criterion we will propose, posed in
Equation 1, satisfy this requirement (for the entropy-based criteria see [4], for our criterion see Lemma 2).
5
Our algorithm could also be implemented as batch or streaming, where in case of the latter one can for
example make one pass through the data per every tree level, however for massive datasets making multiple
passes through the data is computationally costly, further justifying the need for an online approach.
6
The proposed objective function exhibits some similarities with the so-called Carnap’s measure [22, 23]
used in probability and inductive logic.

4

2.3

Quality of the entire tree

The above section helps us understand the quality of an individual split produced by effectively
maximizing J(h). We next reason about the quality of the entire tree as we add more and more
nodes. We measure the quality of trees using the average entropy over all the leaves in the tree, and
track the decrease of this entropy as a function of the number of nodes. Our analysis extends the
theoretical analysis in [4], originally developed to show the boosting properties of the decision trees
for binary classification problems, to the multiclass classification setting.
Given a tree T , we consider the entropy function Gt as the measure of the quality of tree:
✓
◆
k
X X
1
Gt =
wl
⇡l,i ln
⇡l,i
i=1
l2L

where ⇡l,i ’s are the probabilities that a randomly chosen data point x drawn from P, where P is
a fixed target distribution over X , has label i given that x reaches node l, L denotes the set of all
tree leaves, t denotes the number of internal tree nodes, and wl is the weight
P of leaf l defined as the
probability a randomly chosen x drawn from P reaches leaf l (note that l2L wl = 1).

We next state the main theoretical result of this paper (it is captured in Theorem 1). We adopt
the weak learning framework. The weak hypothesis assumption, captured in Definition 3, posits that
each node of the tree T has a hypothesis h in its hypothesis class H which guarantees simultaneously
a ”weak” purity and a ”weak” balancedness of the split on any distribution P over X . Under this
assumption, one can use the new decision tree approach to drive the error below any threshold.
Definition 3 (Weak Hypothesis Assumption). Let m denote any node of the tree T , and let m =
P (hm (x) > 0) and Pm,i = P (hm (x) > 0|i). Furthermore, let 2 R+ be such that for all m,
2 (0, min( m , 1
m )]. We say that the weak hypothesis assumption is satisfied when for any
distribution P over X at each node m of the tree T there exists a hypothesis hm 2 H such that
Pk
J(hm )/2 = i=1 ⇡m,i |Pm,i
.
m|
Theorem 1. Under the Weak Hypothesis Assumption, for any ↵ 2 [0, 1], to obtain Gt  ↵ it suffices
to make t

4(1

(1/↵)

)2 ln k
2

splits.

We defer the proof of Theorem 1 to the Supplementary material and provide its sketch now. The
analysis studies a tree construction algorithm where we recursively find the leaf node with the highest
weight, and choose to split it into two children. Let n be the heaviest leaf at time t. Consider splitting
it to two children. The contribution of node n to the tree entropy changes after it splits. This change
(entropy reduction) corresponds to a gap in the Jensen’s inequality applied to the concave function,
and thus can further be lower-bounded (we use the fact that Shannon entropy is strongly concave
with respect to `1 -norm (see e.g., Example 2.5 in Shalev-Shwartz [24])). The obtained lower-bound
turns out to depend proportionally on J(hn )2 . This implies that the larger the objective J(hn )
is at time t, the larger the entropy reduction ends up being, which further reinforces intuitions to
maximize J. In general, it might not be possible to find any hypothesis with a large enough objective
J(hn ) to guarantee sufficient progress at this point so we appeal to a weak learning assumption. This
assumption can be used to further lower-bound the entropy reduction and prove Theorem 1.

3

The LOMtree Algorithm

The objective function of Section 2 has another convenient form which yields a simple online algorithm for tree construction and training. Note that Equation 1 can be written (details are shown in
Section 12 in the Supplementary material) as
J(h) = 2Ei [|Ex [1(h(x) > 0)] Ex [1(h(x) > 0|i)]|].
Maximizing this objective is a discrete optimization problem that can be relaxed as follows
J(h) = 2Ei [|Ex [h(x)] Ex [h(x)|i]|],
where Ex [h(x)|i] is the expected score of class i.
We next explain our empirical approach for maximizing the relaxed objective. The empirical estimates of the expectations can be easily stored and updated online in every tree node. The decision
whether to send an example reaching a node to its left or right child node is based on the sign of the
difference between the two expectations: Ex [h(x)] and Ex [h(x)|y], where y is a label of the data
point, i.e. when Ex [h(x)] Ex [h(x)|y] > 0 the data point is sent to the left, else it is sent to the right.
This procedure is conveniently demonstrated on a toy example in Section 13 in the Supplement.
During training, the algorithm assigns a unique label to each node of the tree which is currently a
leaf. This is the label with the highest frequency amongst the examples reaching that leaf. While
5

Algorithm 1 LOMtree algorithm (online tree training)
Input: regression algorithm R, max number of tree non-leaf nodes T , swap resistance RS
Subroutine SetNode (v)
mv = ; (mv (y) - sum of the scores for class y)
lv = ; (lv (y) - number of points of class y reaching v)
nv = ; (nv (y) - number of points of class y which are used to train regressor in v)
ev = ; (ev (y) - expected score for class y)
Ev = 0 (expected total score)
Cv = 0 (the size of the smallest leaf7 in the subtree with root v)
Subroutine UpdateC (v)
While (v 6= r AND CPARENT(v) 6= Cv )
v = PARENT(v); Cv = min(CLEFT(v) , CRIGHT(v) )8
Subroutine Swap (v)
Find a leaf s for which (Cs = Cr )
sPA=PARENT(s); sGPA= GRANDPA(s); sSIB=SIBLING(s)9
If (sPA = LEFT(sGPA )) LEFT(sGPA ) = sSIB Else RIGHT(sGPA ) = sSIB
UpdateC (sSIB ); SetNode (s); LEFT(v) = s; SetNode (sPA ); RIGHT(v) = sPA
Create root r = 0: SetNode (r); t = 1
For each example (x, y) do
Set j = r
Do
If (lj (y) = ;)
mj (y) = 0; lj (y) = 0; nj (y) = 0; ej (y) = 0
lj (y)++
If(j is a leaf)
If(lj has at least 2 non-zero entries)
If(t<T OR Cj maxi lj (i)>RS (Cr+1))
If (t<T )
SetNode (LEFT(j)); SetNode (RIGHT(j)); t++
Else Swap(j)
CLEFT(j)=bCj /2c; CRIGHT(j)=Cj CLEFT(j) ; UpdateC (LEFT(j))
If(j is not a leaf)
If (Ej > ej (y)) c = 1 Else c = 1
Train hj with example (x, c): R(x, c)
Pk
mj (i) 10
nj (y) ++; mj (y) += hj (x); ej (y) = mj (y)/nj (y); Ej = Pi=1
k
i=1 nj (i)
Set j to the child of j corresponding to hj
Else
Cj ++
break
testing, a test example is pushed down the tree along the path from the root to the leaf, where in each
non-leaf node of the path its regressor directs the example either to the left or right child node. The
test example is then labeled with the label assigned to the leaf that this example descended to.
The training algorithm is detailed in Algorithm 1 where each tree node contains a classifier (we use
linear classifiers), i.e. hj is the regressor stored in node j and hj (x) is the value of the prediction
of hj on example x11 . The stopping criterion for expanding the tree is when the number of non-leaf
nodes reaches a threshold T .
3.1 Swapping
Consider a scenario where the current training example descends to leaf j. The leaf can split (create
two children) if the examples that reached it in the past were coming from at least two different
7

The smallest leaf is the one with the smallest total number of data points reaching it in the past.
PARENT (v), LEFT (v) and RIGHT (v) denote resp. the parent, and the left and right child of node v.
9
GRANDPA (v) and SIBLING (v) denote respectively the grandparent of node v and the sibling of node v, i.e.
the node which has the same parent as v.
10
In the implementation both sums are stored as variables thus updating Ev takes O(1) computations.
11
We also refer to this prediction value as the ’score’ in this section.
8

6

r
...
j

r

...
...

...

...

sGPA
...

s

sSIB

s

...

j

sPA

...
...

sPA

sGPA
...
sSIB
...
...

...
...
Figure 2: Illustration of the swapping procedure. Left: before the swap, right: after the swap.
classes. However, if the number of non-leaf nodes of the tree reaches threshold T , no more nodes
can be expanded and thus j cannot create children. Since the tree construction is done online, some
nodes created at early stages of training may end up useless because no examples reach them later
on. This prevents potentially useful splits such as at leaf j. This problem can be solved by recycling
orphan nodes (subroutine Swap in Algorithm 1). The general idea behind node recycling is to allow
nodes to split if a certain condition is met. In particular, node j splits if the following holds:
Cj

max

i2{1,2,...,k}

lj (i) > RS (Cr + 1),

(2)

where r denotes the root of the entire tree, Cj is the size of the smallest leaf in the subtree with root
j, where the smallest leaf is the one with the smallest total number of data points reaching it in the
past, lj is a k-dimensional vector of non-negative integers where the ith element is the count of the
number of data points with label i reaching leaf j in the past, and finally RS is a “swap resistance”.
The subtraction of maxi2{1,2,...,k} lj (i) in Equation 2 ensures that a pure node will not be recycled.
If the condition in Inequality 2 is satisfied, the swap of the nodes is performed where an orphan
leaf s, which was reached by the smallest number of examples in the past, and its parent sPA are
detached from the tree and become children of node j whereas the old sibling sSIB of an orphan node
s becomes a direct child of the old grandparent sGPA . The swapping procedure is shown in Figure 2.
The condition captured in the Inequality 2 allows us to prove that the number of times any given
node is recycled is upper-bounded by the logarithm of the number of examples whenever the swap
resistance is 4 or more (Lemma 3).
Lemma 3. Let the swap resistance RS be greater or equal to 4. Then for all sequences of examples,
the number of times Algorithm 1 recycles any given node is upper-bounded by the logarithm (with
base 2) of the sequence length.

4

Experiments

We address several hypotheses experimentally.
1. The LOMtree algorithm achieves true logarithmic time computation in practice.
2. The LOMtree algorithm is competitive with or better than all other logarithmic train/test
time algorithms for multiclass classification.
3. The LOMtree algorithm has statistical performance close to more common O(k) approaches.
To address these hypotheses, we conTable 1: Dataset sizes.
ducted experiments on a variety of
Isolet Sector Aloi ImNet ODP
benchmark multiclass datasets: Isosize
52.3MB 19MB 17.7MB104GB12 3GB
let, Sector, Aloi, ImageNet (Im# features 617 54K 128
6144 0.5M
Net) and ODP13 . The details of the
# examples 7797 9619 108K 14.2M 1577418
datasets are provided in Table 1. The
datasets were divided into training
# classes
26
105 1000 ⇠22K ⇠105K
(90%) and testing (10%). Furthermore, 10% of the training dataset was
used as a validation set.
The baselines we compared LOMtree with are a balanced random tree of logarithmic depth (Rtree)
and the Filter tree [5]. Where computationally feasible, we also compared with a one-against-all
classifier (OAA) as a representative O(k) approach. All methods were implemented in the Vowpal
Wabbit [25] learning system and have similar levels of optimization. The regressors in the tree nodes
for LOMtree, Rtree, and Filter tree as well as the OAA regressors were trained by online gradient
descent for which we explored step sizes chosen from the set {0.25, 0.5, 0.75, 1, 2, 4, 8}. We used
12
13

compressed
The details of the source of each dataset are provided in the Supplementary material.

7

linear regressors. For each method we investigated training with up to 20 passes through the data and
we selected the best setting of the parameters (step size and number of passes) as the one minimizing
the validation error. Additionally, for the LOMtree we investigated different settings of the stopping
criterion for the tree expansion: T = {k 1, 2k 1, 4k 1, 8k 1, 16k 1, 32k 1, 64k 1},
and swap resistance RS = {4, 8, 16, 32, 64, 128, 256}.

In Table 2 and 3 we report respectively train time and per-example test time (the best performer is
indicated in bold). Training time (and later reported test error) is not provided for OAA on ImageNet
and ODP due to intractability14 -both are petabyte scale computations15 .
Table 2: Training time on selected problems. Table 3: Per-example test time on all problems.
Isolet Sector
Aloi
Isolet Sector Aloi ImNet ODP
LOMtree 16.27s 12.77s 51.86s
LOMtree 0.14ms 0.13ms 0.06ms 0.52ms 0.26ms
OAA
19.58s 18.37s 11m2.43s
OAA 0.16 ms 0.24ms 0.33ms 0.21s 1.05s

log2(time ratio)

The first hypothesis is consistent with the experimental results. Time-wise LOMtree significantly
outperforms OAA due to building only close-to logarithmic depth trees. The improvement in the
training time increases with the number of classes in the classification problem. For instance on Aloi
training with LOMtree is 12.8 times faster than with OAA. The same can be said about the test time,
where the per-example test time for Aloi, ImageNet and ODP are respectively 5.5, 403.8 and 4038.5
times faster than OAA. The significant advantage of LOMtree over OAA is also captured in Figure 3.
Next, in Table 4 (the best logarithmic time perLOMtree vs one−against−all
former is indicated in bold) we report test error
12
of logarithmic train/test time algorithms. We
10
also show the binomial symmetrical 95% confidence intervals for our results. Clearly the sec8
ond hypothesis is also consistent with the experimental results. Since the Rtree imposes a
6
random label partition, the resulting error it ob4
tains is generally worse than the error obtained
by the competitor methods including LOMtree
2
which learns the label partitioning directly from
the data. At the same time LOMtree beats Fil6
8
10
12
14
16
ter tree on every dataset, though for ImageNet
log2(number of classes)
Figure 3: Logarithm of the ratio of per-example and ODP (both have a high level of noise) the
advantage of LOMtree is not as significant.
test times of OAA and LOMtree on all problems.
Table 4: Test error (%) and confidence interval on all problems.
LOMtree
Rtree
Filter tree
OAA
Isolet 6.36±1.71 16.92±2.63 15.10±2.51 3.56±1.30%
Sector 16.19±2.33 15.77±2.30 17.70±2.41 9.17±1.82%
Aloi 16.50±0.70 83.74±0.70 80.50±0.75 13.78±0.65%
ImNet 90.17±0.05 96.99±0.03 92.12±0.04
NA
ODP 93.46±0.12 93.85±0.12 93.76±0.12
NA
The third hypothesis is weakly consistent with the empirical results. The time advantage of LOMtree
comes with some loss of statistical accuracy with respect to OAA where OAA is tractable. We
conclude that LOMtree significantly closes the gap between other logarithmic time methods and
OAA, making it a plausible approach in computationally constrained large-k applications.

5

Conclusion

The LOMtree algorithm reduces the multiclass problem to a set of binary problems organized in a
tree structure where the partition in every tree node is done by optimizing a new partition criterion
online. The criterion guarantees pure and balanced splits leading to logarithmic training and testing
time for the tree classifier. We provide theoretical justification for our approach via a boosting
statement and empirically evaluate it on multiple multiclass datasets. Empirically, we find that this
is the best available logarithmic time approach for multiclass classification problems.
14
Note however that the mechanics of testing datastes are much easier - one can simply test with effectively
untrained parameters on a few examples to measure the test speed thus the per-example test time for OAA on
ImageNet and ODP is provided.
15
Also to the best of our knowledge there exist no state-of-the-art results of the OAA performance on these
datasets published in the literature.

8

Acknowledgments
We would like to thank Alekh Agarwal, Dean Foster, Robert Schapire and Matus Telgarsky for
valuable discussions.

References
[1] R. Rifkin and A. Klautau. In defense of one-vs-all classification. J. Mach. Learn. Res., 5:101–141, 2004.
[2] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc., 1991.
[3] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. CRC
Press LLC, Boca Raton, Florida, 1984.
[4] M. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. Journal
of Computer and Systems Sciences, 58(1):109–128, 1999 (also In STOC, 1996).
[5] A. Beygelzimer, J. Langford, and P. D. Ravikumar. Error-correcting tournaments. In ALT, 2009.
[6] A. Beygelzimer, J. Langford, Y. Lifshits, G. B. Sorkin, and A. L. Strehl. Conditional probability tree
estimation analysis and algorithms. In UAI, 2009.
[7] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[8] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In NIPS, 2010.
[9] G. Madzarov, D. Gjorgjevikj, and I. Chorbev. A multi-class svm classifier utilizing binary decision tree.
Informatica, 33(2):225–233, 2009.
[10] J. Deng, S. Satheesh, A. C. Berg, and L. Fei-Fei. Fast and balanced: Efficient label tree learning for large
scale object recognition. In NIPS, 2011.
[11] J. Weston, A. Makadia, and H. Yee. Label partitioning for sublinear ranking. In ICML, 2013.
[12] B. Zhao and E. P. Xing. Sparse output coding for large-scale visual recognition. In CVPR, 2013.
[13] D. Hsu, S. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In NIPS,
2009.
[14] A. Agarwal, S. M. Kakade, N. Karampatziakis, L. Song, and G. Valiant. Least squares revisited: Scalable
approaches for multi-class prediction. In ICML, 2014.
[15] O. Beijbom, M. Saberian, D. Kriegman, and N. Vasconcelos. Guess-averse loss functions for costsensitive multiclass boosting. In ICML, 2014.
[16] R. Agarwal, A. Gupta, Y. Prabhu, and M. Varma. Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages. In WWW, 2013.
[17] Y. Prabhu and M. Varma. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label
learning. In ACM SIGKDD, 2014.
[18] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon. Large-scale multi-label learning with missing labels. In ICML,
2014.
[19] T.-Y. Liu, Y. Yang, H. Wan, H.-J. Zeng, Z. Chen, and W.-Y. Ma. Support vector machines classification
with a very large-scale taxonomy. In SIGKDD Explorations, 2005.
[20] P. N. Bennett and N. Nguyen. Refined experts: improving classification in large taxonomies. In SIGIR,
2009.
[21] A. Montillo, J. Tu, J. Shotton, J. Winn, J.E. Iglesias, D.N. Metaxas, and A. Criminisi. Entanglement and
differentiable information gain maximization. Decision Forests for Computer Vision and Medical Image
Analysis, 2013.
[22] K. Tentori, V. Crupi, N. Bonini, and D. Osherson. Comparison of confirmation measures. Cognition,
103(1):107 – 119, 2007.
[23] R. Carnap. Logical Foundations of Probability. 2nd ed. Chicago: University of Chicago Press. Par. 87
(pp. 468-478), 1962.
[24] S. Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107–194, 2012.
[25] J. Langford, L. Li, and A. Strehl. http://hunch.net/˜vw, 2007.
[26] Y. Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization, Kluwer
Academic Publ., 2004.
[27] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009.

9

"
7,5802,Planar Ultrametrics for Image Segmentation,Poster,5802-planar-ultrametrics-for-image-segmentation.pdf,We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an LP relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.,"Planar Ultrametrics for Image Segmentation

Charless C. Fowlkes
Department of Computer Science
University of California Irvine
fowlkes@ics.uci.edu

Julian Yarkony
Experian Data Lab
San Diego, CA 92130
julian.yarkony@experian.com

Abstract
We study the problem of hierarchical clustering on planar graphs. We formulate
this in terms of finding the closest ultrametric to a specified set of distances and
solve it using an LP relaxation that leverages minimum cost perfect matching as
a subroutine to efficiently explore the space of planar partitions. We apply our
algorithm to the problem of hierarchical image segmentation.

1

Introduction

We formulate hierarchical image segmentation from the perspective of estimating an ultrametric
distance over the set of image pixels that agrees closely with an input set of noisy pairwise distances.
An ultrametric space replaces the usual triangle inequality with the ultrametric inequality d(u, v) ≤
max{d(u, w), d(v, w)} which captures the transitive property of clustering (if u and w are in the
same cluster and v and w are in the same cluster, then u and v must also be in the same cluster).
Thresholding an ultrametric immediately yields a partition into sets whose diameter is less than
the given threshold. Varying this distance threshold naturally produces a hierarchical clustering in
which clusters at high thresholds are composed of clusters at lower thresholds.
Inspired by the approach of [1], our method represents an ultrametric explicitly as a hierarchical
collection of segmentations. Determining the appropriate segmentation at a single distance threshold
is equivalent to finding a minimum-weight multicut in a graph with both positive and negative edge
weights [3, 14, 2, 11, 20, 21, 4, 19, 7]. Finding an ultrametric imposes the additional constraint that
these multicuts are hierarchically consistent across different thresholds. We focus on the case where
the input distances are specified by a planar graph. This arises naturally in the domain of image
segmentation where elements are pixels or superpixels and distances are defined between neighbors
and allows us to exploit fast combinatorial algorithms for partitioning planar graphs that yield tighter
LP relaxations than the local polytope relaxation often used in graphical inference [20].
The paper is organized as follows. We first introduce the closest ultrametric problem and the relation between multicuts and ultrametrics. We then describe an LP relaxation that uses a delayed
column generation approach and exploits planarity to efficiently find cuts via the classic reduction
to minimum-weight perfect matching [13, 8, 9, 10]. We apply our algorithm to the task of natural
image segmentation and demonstrate that our algorithm converges rapidly and produces optimal or
near-optimal solutions in practice.

2

Closest Ultrametric and Multicuts

Let G = (V, E) be a weighted graph with non-negative edge weights θ indexed by edges e =
(u, v) ∈ E. Our goal is to find an ultrametric distance d(u,v) over vertices of the graph that is
P
close to θ in the sense that the distortion (u,v)∈E kθ(u,v) − d(u,v) k22 is minimized. We begin by
reformulating this closest ultrametric problem in terms of finding a set of nested multicuts in a family
of weighted graphs.
1

We specify a partitioning or multicut of the vertices of the graph G into components using a binary
vector X̄ ∈ {0, 1}|E| where X̄e = 1 indicates that the edge e = (u, v) is “cut” and that the vertices
u and v associated with the edge are in separate components of the partition. We use MCUT(G)
to denote the set of binary indicator vectors X̄ that represent valid multicuts of the graph G. For
notational simplicity, in the remainder of the paper we frequently omit the dependence on G which
is given as a fixed input.
A necessary and sufficient condition for an indicator vector X̄ to define a valid multicut in G is that
for every cycle of edges, if one edge on the cycle is cut then at least one other edge in the cycle must
also be cut. Let C denote the set of all cycles in G where each cycle c ∈ C is a set of edges and
c − ê is the set of edges in cycle c excluding edge ê. We can express MCUT in terms of these cycle
inequalities as:
(
)
X
|E|
X̄e ≥ X̄ê , ∀c ∈ C, ê ∈ c
(1)
MCUT = X̄ ∈ {0, 1} :
e∈c−ê

A hierarchical clustering of a graph can be described by a nested collection of multicuts. We denote
the space of valid hierarchical partitions with L layers by Ω̄L which we represent by a set of L
edge-indicator vectors X = (X̄ 1 , X̄ 2 , X̄ 3 , . . . , X̄ L ) in which any cut edge remains cut at all finer
layers of the hierarchy.
Ω̄L = {(X̄ 1 , X̄ 2 , . . . X̄ L ) : X̄ l ∈ MCUT, X̄ l ≥ X̄ l+1 ∀l}

(2)

Given a valid hierarchical clustering X , an ultrametric d can be specified over the vertices of the
graph by choosing a sequence of real values 0 = δ 0 < δ 1 < δ 2 < . . . < δ L that indicate a distance
threshold associated with each level l of the hierarchical clustering. The ultrametric distance d
specified by the pair (X , δ) assigns a distance to each pair of vertices d(u,v) based on the coarsest
level of the clustering at which they remain in separate clusters. For pairs corresponding to an edge
in the graph (u, v) = e ∈ E we can write this explicitly in terms of the multicut indicator vectors
as:
L
X
de =
max δ l X̄el =
δ l [X̄el > X̄el+1 ]
(3)
l∈{0,1,...,L}

l=0

X̄eL+1

X̄e0

= 0. Pairs (u, v) that do not correspond to an
= 1 and
We assume by convention that
edge in the original graph can still be assigned a unique distance based on the coarsest level l at
which they lie in different connected components of the cut specified by X l .
To compute the quality of an ultrametric d with respect to an input set of edge weights θ, we measure
the squared L2 difference between the edge weights and the ultrametric distance kθ − dk22 . To write
this compactly in terms of multicut
Pm indicator vectors, we construct a set of weights for each edge
and layer, denoted θel so that l=0 θel = kθe − δ m k2 . These weights are given explicitly by the
telescoping series:
θe0 = kθe k2
l

We use θ ∈ R

|E|

θel = kθe − δ l k2 − kθe − δ l−1 k2

to denote the vector containing

θel

∀l > 1

(4)

for all e ∈ E.

For a fixed number of levels L and fixed set of thresholds δ, the problem of finding the closest
ultrametric d can then be written as an integer linear program (ILP) over the edge cut indicators.


2
L
L

X
X
XX

l
l
l+1 
min
δ [X̄e > X̄e ]
 = min
kθe − δ l k2 (X̄el − X̄el+1 )
(5)

θe −


X ∈Ω̄L
X ∈Ω̄L
e∈E
e∈E l=0
l=0
!
L
X
X
 l
2 0
l 2
l−1 2
L 2 L+1
kθe − δ k − kθe − δ k X̄e + kθe − δ k X̄e
= min
kθe k X̄e +
X ∈Ω̄L

= min
X ∈Ω̄L

e∈E
L X
X
l=0 e∈E

l=1

θel X̄el = min

X ∈Ω̄L

L
X

θl · X̄ l

(6)

l=0

This optimization corresponds to solving a collection of minimum-weight multicut problems where
the multicuts are constrained to be hierarchically consistent.
2

(a) Linear combination of cut vectors

(b) Hierarchical cuts

Figure 1: (a) Any partitioning X can be represented as a linear superposition of cuts Z where
each cut isolates a connected component of the partition and is assigned a weight γ = 12 [20]. By
introducing an auxiliary slack variables β, we are able to represent a larger set of valid indicator
vectors X using fewer columns of Z. (b) By introducing additional slack variables at each layer of
the hierarchical segmentation, we can efficiently represent many hierarchical segmentations (here
{X 1 , X 2 , X 3 }) that are consistent from layer to layer while using only a small number of cut indicators as columns of Z.
Computing minimum-weight multicuts (also known as correlation clustering) is NP hard even in the
case of planar graphs [6]. A direct approach to finding an approximate solution to Eq 6 is to relax
the integrality constraints on X̄ l and instead optimize over the whole polytope defined by the set of
cycle inequalities. We use ΩL to denote the corresponding relaxation of Ω̄L . While the resulting
polytope is not the convex hull of MCUT, the integral vertices do correspond exactly to the set of
valid multicuts [12].
In practice, we found that applying a straightforward cutting-plane approach that successively adds
violated cycle inequalities to this relaxation of Eq 6 requires far too many constraints and is too
slow to be useful. Instead, we develop a column generation approach tailored for planar graphs that
allows for efficient and accurate approximate inference.

3

The Cut Cone and Planar Multicuts

Consider a partition of a planar graph into two disjoint sets of nodes. We denote the space of
indicator vectors corresponding to such two-way cuts by CUT. A cut may yield more than two
connected components but it can not produce every possible multicut (e.g., it can not split a triangle
of three nodes into three separate components). Let Z ∈ {0, 1}|E|×|CUT| be an indicator matrix
where each column specifies a valid two-way cut with Zek = 1 if and only if edge e is cut in twoway cut k. The indicator vector of any multicut in a planar graph can be generated by a suitable
linear combination of of cuts (columns of Z) that isolate the individual components from the rest of
the graph where the weight of each such cut is 12 .
Let γ ∈ R|CUT| be a vector specifying a positive weighted combination of cuts. The set CUT4 =
{Zγ : γ ≥ 0} is the conic hull of CUT or “cut cone”. Since any multicut can be expressed as a
superposition of cuts, the cut cone is identical to the conic hull of MCUT. This equivalence suggests
an LP relaxation of the minimum-cost multicut given by
min θ · Zγ

s.t. Zγ ≤ 1

γ≥0

(7)

where the vector θ ∈ R|E| specifies the edge weights. For the case of planar graphs, any solution to
this LP relaxation satisfies the cycle inequalities (see supplement and [12, 18, 10]).
Expanded Multicut Objective: Since the matrix Z contains an exponential number of cuts, Eq. 7
is still intractable. Instead we consider an approximation using a constraint set Ẑ which is a subset
3

of columns of Z. In previous work [20], we showed that since the optimal multicut may no longer
lie in the span of the reduced cut matrix Ẑ, it is useful to allow some values of Ẑγ exceed 1 (see
Figure 1(a) for an example).
We introduce a slack vector β ≥ 0 that tracks the presence of any “overcut” edges and prevents
them from contributing to the objective when the corresponding edge weight is negative. Let θe− =
min(θe , 0) denote the non-positive component of θe . The expanded multi-cut objective is given by:
min θ · Ẑγ − θ− · β

s.t. Ẑγ − β ≤ 1

γ≥0
β≥0

(8)

For any edge e such that θe < 0, any decrease in the objective from overcutting by an amount βe is
exactly compensated for in the objective by the term −θe− βe .
When Ẑ contains all cuts (i.e., Ẑ = Z) then Eq 7 and Eq 8 are equivalent [20]. Further, if γ ? is the
minimizer of Eq 8 when Ẑ only contains a subset of columns, then the edge indicator vector given
by X = min(1, Ẑγ ? ) still satisfies the cycle inequalities (see supplement for details).

4

Expanded LP for Finding the Closest Ultrametric

To develop an LP relaxation of the closest ultrametric problem, we replace the multicut problem at
each layer l with the expanded multicut objective described by Eq 8. We let γ = {γ 1 , γ 2 , γ 3 . . . γ L }
and β = {β 1 , β 2 , β 3 . . . β L } denote the collection of weights and slacks for the levels of the hierarchy and let θe+l = max(0, θel ) and θe−l = min(0, θel ) denote the positive and negative components
of θl .
To enforce hierarchical consistency between layers, we would like to add the constraint that
Zγ l+1 ≤ Zγ l . However, this constraint is too rigid when Z does not include all possible cuts.
It is thus computationally useful to introduce an additional slack vector associated with each level l
and edge e which we denote as α = {α1 , α2 , α3 . . . αL−1 }. The introduction of αel allows for cuts
represented by Zγ l to violate the hierarchical constraint. We modify the objective so that violations
to the original hierarchy constraint are paid for in proportion to θe+l . The introduction of α allows
us to find valid ultrametrics while using a smaller number of columns of Z to be used than would
otherwise be required (illustrated in Figure 1(b)).
We call this relaxed closest ultrametric problem including the slack variable α the expanded closest
ultrametric objective, written as:
min

L
L
L−1
X
X
X
θl · Zγ l +
−θ−l · β l +
θ+l · αl

γ≥0
β≥0 l=1
α≥0

l=1

s.t. Zγ l+1 + αl+1 ≤ Zγ l + αl
l

l

Zγ − β ≤ 1

(9)

l=1

∀l < L

∀l

where by convention we define αL = 0 and we have dropped the constant l = 0 term from Eq 6.
Given a solution (α, β, γ) we can recover a relaxed solution to the closest ultrametric problem (Eq.
6) over ΩL by setting Xel = min(1, maxm≥l (Zγ m )e ). In the supplement, we demonstrate that for
any (α, β, γ) that obeys the constraints in Eq 9, this thresholding operation yields a solution X that
lies in ΩL and achieves the same or lower objective value.

5

The Dual Objective

We optimize the dual of the objective in Eq 9 using an efficient column generation approach based
on perfect matching. We introduce two sets of Lagrange multipliers ω = {ω 1 , ω 2 , ω 3 . . . ω L−1 } and
λ = {λ1 , λ2 , λ3 . . . λL } corresponding to the between and within layer constraints respectively. For
4

Algorithm 1 Dual Closest Ultrametric via Cutting Planes
Ẑ l ← {} ∀l, residual ← −∞
while residual < 0 do
{ω}, {λ} ← Solve Eq 10 given Ẑ
residual = 0
for l = 1 : L do
z l ← arg minz∈CUT (θl + λl + ω l−1 − ω l ) · z
residual ← residual + 32 (θl + λl + ω l−1 − ω l ) · z l
{z(1), z(2), . . . , z(M )} ← isocuts(z l )
Ẑ l ← Ẑ l ∪ {z(1), z(2), . . . , z(M )}
end for
end while
notational convenience, let ω 0 = 0. The dual objective can then be written as
max

L
X

ω≥0,λ≥0

l=1
−l

θ

−λl · 1
≤ −λl

− (ω

l−1

l

(10)
∀l

− ω l ) ≤ θ+l

l

(θ + λ + ω

l−1

l

∀l

−ω )·Z ≥0

∀l

The dual LP can be interpreted as finding a small modification of the original edge weights θl so
that every possible two-way cut of each resulting graph at level l has non-negative weight. Observe
that the introduction of the two slack terms α and β in the primal problem (Eq 9) results in bounds
on the Lagrange multipliers λ and ω in the dual problem in Eq 10. In practice these dual constraints
turn out to be essential for efficient optimization and constitute the core contribution of this paper.

6

Solving the Dual via Cutting Planes

The chief complexity of the dual LP is contained in the constraints including Z which encodes
non-negativity of an exponential number of cuts of the graph represented by the columns of Z. To
circumvent the difficulty of explicitly enumerating the columns of Z, we employ a cutting plane
method that efficiently searches for additional violated constraints (columns of Z) which are then
successively added.
Let Ẑ denote the current working set of columns. Our dual optimization algorithm iterates over
the following three steps: (1) Solve the dual LP with Ẑ, (2) find the most violated constraint of the
form (θl + λl + ω l−1 − ω l ) · Z ≥ 0 for layer l, (3) Append a column to the matrix Ẑ for each
such cut found. We terminate when no violated constraints exist or a computational budget has been
exceeded.
Finding Violated Constraints: Identifying columns to add to Ẑ is carried out for each layer l
separately. Finding the most violated constraint of the full problem corresponds to computing the
minimum-weight cut of a graph with edge weights θl + λl + ω l−1 − ω l . If this cut has non-negative
weight then all the constraints are satisfied, otherwise we add the corresponding cut indicator vector
as an additional column of Z.
To generate a new constraint for layer l based on the current Lagrange multipliers, we solve
X
z l = arg min
(θel + λle + ωel−1 − ωel )ze
z∈CUT

(11)

e∈E

and subsequently add the new constraints from all layers to our LP, Ẑ ← [Ẑ, z 1 , z 2 , . . . z L ].
Unlike the multicut problem, finding a (two-way) cut in a planar graph can be solved exactly by a
reduction to minimum-weight perfect matching. This is a classic result that, e.g. provides an exact
solution for the ground state of a 2D lattice Ising model without a ferromagnetic field [13, 8, 9, 10]
3
in O(N 2 log N ) time [15].
5

80

UB
LB

60

−2

Bound

Counts

10

40

−4

10

20

0

10

1

10

2

10

0
0.2

3

10

Time (sec)

0.4
0.6
0.8
Objective ratio (UCM / UM)

1

Figure 2: (a): The average convergence of the upper (blue) and lower-bounds (red) as a function
of running time. Values plotted are the gap between the bound and the best lower-bound computed
(at termination) for a given problem instance. This relative gap is averaged over problem instances
which have not yet converged at a given time point. We indicate the percentage of problem instances
that have yet to terminate using black bars marking [95, 85, 75, 65, .....5] percent. (b) Histogram of
the ratio of closest ultrametric objective values for our algorithm (UM) and the baseline clustering
produced by UCM. All ratios were less than 1 showing that in no instances did UM produce a worse
solution than UCM

Computing a lower bound: At a given iteration, prior to adding a newly generated set of constraints
P
we can compute the total residual constraint violation over all layers of hierarchy by ∆ = l (θl +
λl + ω l−1 − ω l ) · z l . In the supplement we demonstrate that the value of the dual objective plus
3
2 ∆ is a lower-bound on the relaxed closest ultrametric problem in Eq 9. Thus, as the costs of the
minimum-weight matchings approach zero from below, the objective of the reduced problem over
Ẑ approaches an accurate lower-bound on optimization over Ω̄L
Expanding generated cut constraints: When a given cut z l produces more than two connected
components, we found it useful to add a constraint corresponding to each component, following the
approach of [20]. Let the number of connected components of z l be denoted M . For each of the
M components then we add one column to Z corresponding to the cut that isolates that connected
component from the rest. This allows more flexibility in representing the final optimum multicut as
superpositions of these components. In addition, we also found it useful in practice to maintain a
separate set of constraints Ẑ l for each layer l. Maintaining independent constraints Ẑ 1 , Ẑ 2 , . . . , Ẑ L
can result in a smaller overall LP.
Speeding convergence of ω: We found that adding an explicit penalty term to the objective that
encourages small values of ω speeds up convergence dramatically with no loss in solution quality.
In our experiments, this penalty is scaled by a parameter  = 10−4 which is chosen to be extremely
small in magnitude relative to the values of θ so that it only has an influence when no other “forces”
are acting on a given term in ω.
Primal Decoding: Algorithm 1 gives a summary of the dual solver which produces a lower-bound
as well as a set of cuts described by the constraint matrices Ẑ l . The subroutine isocuts(z l ) computes
the set of cuts that isolate each connected component of z l . To generate a hierarchical clustering,
we solve the primal, Eq 9, using the reduced set Ẑ in order to recover a fractional solution Xel =
min(1, maxm≥l (Ẑ m γ m )e ). We use an LP solver (IBM CPLEX) which provides this primal solution
“for free” when solving the dual in Alg. 1.
We round the fractional primal solution X to a discrete hierarchical clustering by thresholding:
X̄el ← [Xel > t]. We then repair (uncut) any cut edges that lie inside a connected component. In our
implementation we test a few discrete thresholds t ∈ {0, 0.2, 0.4, 0.6, 0.8} and take that threshold
that yields X̄ with the lowest cost. After each pass through the loop of Alg. 1 we compute these
upper-bounds and retain the optimum solution observed thus far.
6

1

Precision

0.8

Maximum F−measure

UCM
UCM−L
UM

0.9

0.7
0.6
0.5
0.4
0

0.2

0.4

0.6

0.8

0.7
0.6
0.5

0.3 0
10

1

Recall

UM
UCM−L
UCM

0.4

1

10

2

10
Time (sec)

3

10

Figure 3: (a) Boundary detection performance of our closest ultrametric algorithm (UM) and the
baseline ultrametric contour maps algorithm with (UCM) and without (UCM-L) length weighting
[5] on BSDS. Black circles indicate thresholds used in the closest UM optimization. (b) Anytime
performance: F-measure on the BSDS benchmark as a function of run-time. UM, UCM with and
without length weighting achieve a maximum F-measure of 0.728, 0.726, and 0.718 respectively.

7

Experiments

We applied our algorithm to segmenting images from the Berkeley Segmentation Data set (BSDS)
[16]. We use superpixels generated by performing an oriented watershed transform on the output
of the global probability of boundary (gPb) edge detector [17] and construct a planar graph whose
vertices are superpixels with edges connecting neighbors in the image plane whose base distance θ
is derived from gP b.
Let gP be be the local estimate of boundary contrast given by averaging the gP b classifier output
over the boundary between a pair of neighboring superpixels.
We 
truncate extreme values to enforce


gP be
that gP be ∈ [, 1 − ] with  = 0.001 and set θe = log 1−gP be + log 1−
The additive offset

assures that θe ≥ 0. In our experiments we use a fixed set of eleven distance threshold levels {δl }
chosen to uniformly span the useful range of threshold values [9.6, 12.6]. Finally, we weighted edges
proportionally to the length of the corresponding boundary in the image.
We performed dual cutting plane iterations until convergence or 2000 seconds had passed. Lowerbounds for the BSDS segmentations were on the order of −103 or −104 . We terminate when the
total residual is greater than −2 × 10−4 . All codes were written in MATLAB using the Blossom
V implementation of minimum-weight perfect matching [15] and the IBM ILOG CPLEX LP solver
with default options.
Baseline: We compare our results with the hierarchical clusterings produced by the Ultrametric
Contour Map (UCM) [5]. UCM performs agglomerative clustering of superpixels and assigns the
length-weighted averaged gP b value as the distance between each pair of merged regions. While
UCM was not explicitly designed to find the closest ultrametric, it provides a strong baseline for
hierarchical clustering. To compute the closest l-level ultrametric corresponding to the UCM clustering result, we solve the minimization in Eq. 6 while restricting each multicut to be the partition
at some level of the UCM hierarchy.
Convergence and Timing: Figure 2 shows the average behavior of convergence as a function of
runtime. We found the upper-bound given by the cost of the decoded integer solution and the lowerbound estimated by the dual LP are very close. The integrality gap is typically within 0.1% of the
lower-bound and never more than 1 %. Convergence of the dual is achieved quite rapidly; most
instances require less than 100 iterations to converge with roughly linear growth in the size of the
LP at each iteration as cutting planes are added. In Fig 2 we display a histogram, computed over test
image problem instances, of the cost of UCM solutions relative to those produced by closest ultrametric (UM) estimated by our method. A ratio of less than 1 indicates that our approach generated
a solution with a lower distortion ultrametric. In no problem instance did UCM outperform our UM
algorithm.
7

UM

MC

UM

MC

Figure 4: The proposed closest ultrametric (UM) enforces consistency across levels while performing independent multi-cut clustering (MC) at each threshold does not guarantee a hierarchical
segmentation (c.f. first image, columns 3 and 4). In the second image, hierarchical segmentation
(UM) better preserves semantic parts of the two birds while correctly merging the background regions.
Segmentation Quality: Figure 3 shows the segmentation benchmark accuracy of our closest ultrametric algorithm (denoted UM) along with the baseline ultrametric contour maps algorithm (UCM)
with and without length weighting [5]. In terms of segmentation accuracy, UM performs nearly identically to the state of the art UCM algorithm with some small gains in the high-precision regime. It
is worth noting that the BSDS benchmark does not provide strong penalties for small leaks between
two segments when the total number of boundary pixels involved is small. Our algorithm may find
strong application in domains where the local boundary signal is noisier (e.g., biological imaging)
or when under-segmentation is more heavily penalized.
While our cutting-plane approach is slower than agglomerative clustering, it is not necessary to wait
for convergence in order to produce high quality results. We found that while the upper and lower
bounds decrease as a function of time, the clustering performance as measured by precision-recall
is often nearly optimal after only ten seconds and remains stable. Figure 3 shows a plot of the
F-measure achieved by UM as a function of time.
Importance of enforcing hierarchical constraints: Although independently finding multicuts at
different thresholds often produces hierarchical clusterings, this is by no means guaranteed. We ran
Algorithm 1 while setting ωel = 0, allowing each layer to be solved independently. Fig 4 shows
examples where hierarchical constraints between layers improves segmentation quality relative to
independent clustering at each threshold.

8

Conclusion

We have introduced a new method for approximating the closest ultrametric on planar graphs that
is applicable to hierarchical image segmentation. Our contribution is a dual cutting plane approach
that exploits the introduction of novel slack terms that allow for representing a much larger space of
solutions with relatively few cutting planes. This yields an efficient algorithm that provides rigorous
bounds on the quality the resulting solution. We empirically observe that our algorithm rapidly
produces compelling image segmentations along with lower- and upper-bounds that are nearly tight
on the benchmark BSDS test data set.
Acknowledgements: JY acknowledges the support of Experian, CF acknowledges support of NSF
grants IIS-1253538 and DBI-1262547
8

References
[1] Nir Ailon and Moses Charikar. Fitting tree metrics: Hierarchical clustering and phylogeny. In
Foundations of Computer Science, 2005., pages 73–82, 2005.
[2] Bjoern Andres, Joerg H. Kappes, Thorsten Beier, Ullrich Kothe, and Fred A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proc. of ICCV, pages 2611–2618,
2011.
[3] Bjoern Andres, Thorben Kroger, Kevin L. Briggman, Winfried Denk, Natalya Korogod, Graham Knott, Ullrich Kothe, and Fred. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proc. of ECCV, 2012.
[4] Bjoern Andres, Julian Yarkony, B. S. Manjunath, Stephen Kirchhoff, Engin Turetken, Charless
Fowlkes, and Hanspeter Pfister. Segmenting planar superpixel adjacency graphs w.r.t. nonplanar superpixel affinity graphs. In Proc. of EMMCVPR, 2013.
[5] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and
hierarchical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):898–916,
May 2011.
[6] Yoram Bachrach, Pushmeet Kohli, Vladimir Kolmogorov, and Morteza Zadimoghaddam. Optimal coalition structure generation in cooperative graph games. In Proc. of AAAI, 2013.
[7] Shai Bagon and Meirav Galun. Large scale correlation clustering. In CoRR, abs/1112.2903,
2011.
[8] F Barahona. On the computational complexity of ising spin glass models. Journal of Physics
A: Mathematical, Nuclear and General, 15(10):3241–3253, april 1982.
[9] F Barahona. On cuts and matchings in planar graphs. Mathematical Programming, 36(2):53–
68, november 1991.
[10] F Barahona and A Mahjoub. On the cut polytope. Mathematical Programming, 60(1-3):157–
173, September 1986.
[11] Thorsten Beier, Thorben Kroeger, Jorg H Kappes, Ullrich Kothe, and Fred A Hamprecht. Cut,
glue, and cut: A fast, approximate solver for multicut partitioning. In Computer Vision and
Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 73–80, 2014.
[12] Michel Deza and Monique Laurent. Geometry of cuts and metrics, volume 15. Springer
Science & Business Media, 1997.
[13] Michael Fisher. On the dimer solution of planar ising models. Journal of Mathematical
Physics, 7(10):1776–1781, 1966.
[14] Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and Chang Dong Yoo. Higher-order
correlation clustering for image segmentation. In Advances in Neural Information Processing
Systems,25, pages 1530–1538, 2011.
[15] Vladimir Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching
algorithm. Mathematical Programming Computation, 1(1):43–67, 2009.
[16] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring
ecological statistics. In Proc. of ICCV, pages 416–423, 2001.
[17] David Martin, Charless C. Fowlkes, and Jitendra Malik. Learning to detect natural image
boundaries using local brightness, color, and texture cues. IEEE Trans. Pattern Anal. Mach.
Intell., 26(5):530–549, May 2004.
[18] Julian Yarkony. Analyzing PlanarCC. NIPS 2014 workshop, 2014.
[19] Julian Yarkony, Thorsten Beier, Pierre Baldi, and Fred A Hamprecht. Parallel multicut segmentation via dual decomposition. In New Frontiers in Mining Complex Patterns, 2014.
[20] Julian Yarkony, Alexander Ihler, and Charless Fowlkes. Fast planar correlation clustering for
image segmentation. In Proc. of ECCV, 2012.
[21] Chong Zhang, Julian Yarkony, and Fred A. Hamprecht. Cell detection and segmentation using
correlation clustering. In MICCAI, volume 8673, pages 9–16, 2014.

9

"
8,5776,Expressing an Image Stream with a Sequence of Natural Sentences,Poster,5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf,"We propose an approach for generating a sequence of natural sentences for an image stream. Since general users usually take a series of pictures on their special moments, much online visual information exists in the form of image streams, for which it would better take into consideration of the whole set to generate natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a novel architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional networks, bidirectional recurrent networks, and entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.","Expressing an Image Stream with a Sequence of
Natural Sentences
Cesc Chunseong Park
Gunhee Kim
Seoul National University, Seoul, Korea
{park.chunseong,gunhee}@snu.ac.kr
https://github.com/cesc-park/CRCN

Abstract
We propose an approach for retrieving a sequence of natural sentences for an
image stream. Since general users often take a series of pictures on their special
moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt
with the relation between a single image and a single natural sentence, our work
extends both input and output dimension to a sequence of images and a sequence
of sentences. To this end, we design a multimodal architecture called coherence
recurrent convolutional network (CRCN), which consists of convolutional neural
networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of
blog posts as text-image parallel training data. We demonstrate that our approach
outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.

1

Introduction

Recently there has been a hike of interest in automatically generating natural language descriptions
for images in the research of computer vision, natural language processing, and machine learning
(e.g. [5, 8, 9, 12, 14, 15, 26, 21, 30]). While most of existing work aims at discovering the relation
between a single image and a single natural sentence, we extend both input and output dimension to
a sequence of images and a sequence of sentences, which may be an obvious next step toward joint
understanding of the visual content of images and language descriptions, albeit under-addressed in
current literature. Our problem setup is motivated by that general users often take a series of pictures
on their memorable moments. For example, many people who visit New York City (NYC) would
capture their experiences with large image streams, and thus it would better take the whole photo
stream into consideration for the translation to a natural language description.

Figure 1: An intuition of our problem statement with a New York City example. We aim at expressing an image
stream with a sequence of natural sentences. (a) We leverage natural blog posts to learn the relation between
image streams and sentence sequences. (b) We propose coherence recurrent convolutional networks (CRCN)
that integrate convolutional networks, bidirectional recurrent networks, and the entity-based coherence model.

1

Fig.1 illustrates an intuition of our problem statement with an example of visiting NYC. Our objective
is, given a photo stream, to automatically produce a sequence of natural language sentences that
best describe the essence of the input image set. We propose a novel multimodal architecture named
coherence recurrent convolutional networks (CRCN) that integrate convolutional neural networks
for image description [13], bidirectional recurrent neural networks for the language model [20], and
the local coherence model [1] for a smooth flow of multiple sentences. Since our problem deals with
learning the semantic relations between long streams of images and text, it is more challenging to
obtain appropriate text-image parallel corpus than previous research of single sentence generation.
Our idea to this issue is to directly leverage online natural blog posts as text-image parallel training
data, because usually a blog consists of a sequence of informative text and multiple representative
images that are carefully selected by authors in a way of storytelling. See an example in Fig.1.(a).
We evaluate our approach with the blog datasets of the NYC and Disneyland, consisting of more than
20K blog posts with 140K associated images. Although we focus on the tourism topics in our experiments, our approach is completely unsupervised and thus applicable to any domain that has a large
set of blog posts with images. We demonstrate the superior performance of our approach by comparing with other state-of-the-art alternatives, including [9, 12, 21]. We evaluate with quantitative
measures (e.g. BLEU and Top-K recall) and user studies via Amazon Mechanical Turk (AMT).
Related work. Due to a recent surge of volume of literature on this subject of generating natural language descriptions for image data, here we discuss a representative selection of ideas that are closely
related to our work. One of the most popular approaches is to pose the text generation as a retrieval
problem that learns ranking and embedding, in which the caption of a test image is transferred from
the sentences of its most similar training images [6, 8, 21, 26]. Our approach partly involves the
text retrieval, because we search for candidate sentences for each image of a query sequence from
training database. However, we then create a final paragraph by considering both compatibilities
between individual images and text, and the coherence that captures text relatedness at the level of
sentence-to-sentence transitions. There have been also video-sentence works (e.g. [23, 32]); our key
novelty is that we explicitly include the coherence model. Unlike videos, consecutive images in the
streams may show sharp changes of visual content, which cause the abrupt discontinuity between
consecutive sentences. Thus the coherence model is more demanded to make output passages fluent.
Many recent works have exploited multimodal networks that combine deep convolutional neural networks (CNN) [13] and recurrent neural network (RNN) [20]. Notable architectures in this category
integrate the CNN with bidirectional RNNs [9], long-term recurrent convolutional nets [5], longshort term memory nets [30], deep Boltzmann machines [27], dependency-tree RNN [26], and other
variants of multimodal RNNs [3, 19]. Although our method partly take advantage of such recent
progress of multimodal neural networks, our major novelty is that we integrate it with the coherence
model as a unified end-to-end architecture to retrieve fluent sequential multiple sentences.
In the following, we compare more previous work that bears a particular resemblance to ours.
Among multimodal neural network models, the long-term recurrent convolutional net [5] is related
to our objective because their framework explicitly models the relations between sequential inputs
and outputs. However, the model is applied to a video description task of creating a sentence for a
given short video clip and does not address the generation of multiple sequential sentences. Hence,
unlike ours, there is no mechanism for the coherence between sentences. The work of [11] addresses
the retrieval of image sequences for a query paragraph, which is the opposite direction of our problem. They propose a latent structural SVM framework to learn the semantic relevance relations from
text to image sequences. However, their model is specialized only for the image sequence retrieval,
and thus not applicable to the natural sentence generation.
Contributions. We highlight main contributions of this paper as follows. (1) To the best of our
knowledge, this work is the first to address the problem of expressing image streams with sentence
sequences. We extend both input and output to more elaborate forms with respect to a whole body of
existing methods: image streams instead of individual images and sentence sequences instead of individual sentences. (2) We develop a multimodal architecture of coherence recurrent convolutional
networks (CRCN), which integrates convolutional networks for image representation, recurrent networks for sentence modeling, and the local coherence model for fluent transitions of sentences. (3)
We evaluate our method with large datasets of unstructured blog posts, consisting of 20K blog posts
with 140K associated images. With both quantitative evaluation and user studies, we show that our
approach is more successful than other state-of-the-art alternatives in verbalizing an image stream.

2

2

Text-Image Parallel Dataset from Blog Posts

We discuss how to transform blog posts to a training set B of image-text parallel data streams, each
l
l
of which is a sequence of image-sentence pairs: B l = {(I1l , T1l ),· · ·, (IN
l , TN l )} ∈ B. The training
set size is denoted by L = |B|. Fig.2.(a) shows the summary of pre-processing steps for blog posts.
2.1

Blog Pre-processing

We assume that blog authors augment their text with multiple images in a semantically meaningful
manner. In order to decompose each blog into a sequence of images and associated text, we first
perform text segmentation and then text summarization. The purpose of text segmentation is to
divide the input blog text into a set of text segments, each of which is associated with a single
image. Thus, the number of segments is identical to the number of images in the blog. The objective
of text summarization is to reduce each text segment into a single key sentence. As a result of these
l
l
two processes, we can transform each blog into a form of B l = {(I1l , T1l ), · · · , (IN
l , TN l )}.
Text segmentation. We first divide the blog passage into text blocks according to paragraphs. We
apply a standard paragraph tokenizer of NLTK [2] that uses rule-based regular expressions to detect
paragraph divisions. We then use the heuristics based on the image-to-text block distances proposed
in [10]. Simply, we assign each text block to the image that has the minimum index distance where
each text block and image is counted as a single index distance in the blog.
Text summarization. We summarize each text segment into a single key sentence. We apply the
Latent Semantic Analysis (LSA)-based summarization method [4], which uses the singular value
decomposition to obtain the concept dimension of sentences, and then recursively finds the most
representative sentences that maximize the inter-sentence similarity for each topic in a text segment.
Data augmentation. The data augmentation is a well-known technique for convolutional neural
networks to improve image classification accuracies [13]. Its basic idea is to artificially increase
the number of training examples by applying transformations, horizontal reflection or adding noise
to training images. We empirically observe that this idea leads better performance in our problem
l
l
as well. For each image-sentence sequence B l = {(I1l , T1l ), · · · , (IN
l , TN l )}, we augment each
l
sentence Tn with multiple sentences for training. That is, when we perform the LSA-based text
summarization, we select top-κ highest ranked summary sentences, among which the top-ranked
one becomes the summary sentence for the associated image, and all the top-κ ones are used for
training in our model. With a slight abuse of notation, we let Tnl to denote both the single summary
sentence and κ augmented sentences. We choose κ = 3 after thorough empirical tests.
2.2

Text Description

Once we represent each text segment with κ sentences, we extract the paragraph vector [17] to represent the content of text. The paragraph vector is a neural-network based unsupervised algorithm
that learns fixed-length feature representation from variable-length pieces of passage. We learn 300dimensional dense vector representation separately from the two classes of the blog dataset using
the gensim doc2vec code. We use pn to denote the paragraph vector representation for text Tn .
We then extract a parsed tree for each Tn to identify coreferent entities and grammatical roles of the
words. We use the Stanford core NLP library [18]. The parse trees are used for the local coherence
model, which will be discussed in section 3.2.

3

Our Architecture

Many existing sentence generation models (e.g. [9, 19]) combine words or phrases from training
data to generate a sentence for a novel image. Our approach is one level higher; we use sentences
from training database to author a sequence of sentences for a novel image stream. Although our
model can be easily extended to use words or phrases as basic building blocks, such granularity
makes sequences too long to train the language model, which may cause several difficulties for
learning the RNN models. For example, the vanishing gradient effect is a well-known hardship to
backpropagate an error signal through a long-range temporal interval. Therefore, we design our
approach that retrieves individual candidate sentences for each query image from training database
and crafts a best sentence sequence, considering both the fitness of individual image-to-sentence
pairs and coherence between consecutive sentences.
3

Figure 2: Illustration of (a) pre-processing steps of blog posts, and (b) the proposed CRCN architecture.
Fig.2.(b) illustrates the structure of our CRCN. It consists of three main components, which are
convolutional neural networks (CNN) [13] for image representation, bidirectional recurrent neural
networks (BRNN) [24] for sentence sequence modeling, and the local coherence model [1] for a
smooth flow of multiple sentences. Each data stream is a variable-length sequence denoted by
{(I1 , T1 ), · · · , (IN , TN )}. We use t ∈ {1, · · · , N } to denote a position of a sentence/image in a
sequence. We define the CNN and BRNN model for each position separately, and the coherence
model for a whole data stream. For the CNN component, our choice is the VGGNet [25] that
represents images as 4,096-dimensional vectors. We discuss the details of our BRNN and coherence
model in section 3.1 and section 3.2 respectively, and finally present how to combine the output of
the three components to create a single compatibility score in section 3.3.
3.1

The BRNN Model

The role of BRNN model is to represent a content flow of text sequences. In our problem, the BRNN
is more suitable than the normal RNN, because the BRNN can simultaneously model forward and
backward streams, which allow us to consider both previous and next sentences for each sentence to
make the content of a whole sequence interact with one another. As shown in Fig.2.(b), our BRNN
has five layers: input layer, forward/backward layer, output layer, and ReLU activation layer, which
are finally merged with that of the coherence model into two fully connected layers. Note that each
text is represented by 300-dimensional paragraph vector pt as discussed in section 2.2. The exact
form of our BRNN is as follows. See Fig.2.(b) together for better understanding.
xft = f (Wif pt + bfi );
hft

=

f (xft

+

Wf hft−1

xbt = f (Wib pt + bbi );
+ bf ); hbt = f (xbt + Wb hbt+1 + bb ); ot =

(1)
Wo (hft

+ hbt ) + bo .

The BRNN takes a sequence of text vectors pt as input. We then compute xft and xbt , which are the
activations of input units to forward and backward units. Unlike other BRNN models, we separate
the input activation into forward and backward ones with different sets of parameters Wif and Wib ,
which empirically leads a better performance. We set the activation function f to the Rectified
Linear Unit (ReLU), f (x) = max(0, x). Then, we create two independent forward and backward
hidden units, denoted by hft and hbt . The final activation of the BRNN ot can be regarded as a
description for the content of the sentence at location t, which also implicitly encodes the flow of
the sentence and its surrounding context in the sequence. The parameter sets to learn include weights
{Wif , Wib , Wf , Wb , Wo } ∈ R300×300 and biases {bfi , bbi , bf , bb , bo } ∈ R300×1 .
3.2

The Local Coherence Model

The BRNN model can capture the flow of text content, but it lacks learning the coherence of passage
that reflects distributional, syntactic, and referential information between discourse entities. Thus,
we explicitly include a local coherence model based on the work of [1], which focuses on resolving
the patterns of local transitions of discourse entities (i.e. coreferent noun phrases) in the whole
text. As shown in Fig.2.(b), we first extract parse trees for every summarized text denoted by Zt
and then concatenate all sequenced parse trees into one large one, from which we make an entity
grid for the whole sequence. The entity grid is a table where each row corresponds to a discourse
4

entity and each column represents a sentence. Grammatical role are expressed by three categories
and one for absent (i.e. not referenced in the sentence): S (subjects), O (objects), X (other than
subject or object) and −(absent). After making the entity grid, we enumerate the transitions of the
grammatical roles of entities in the whole text. We set the history parameter to three, which means
we can obtain 43 = 64 transition descriptions (e.g. SO− or OOX). By computing the ratio of
the occurrence frequency of each transition, we finally create a 64-dimensional representation that
captures the coherence of a sequence. Finally, we make this descriptor to a 300-dimensional vector
by zero-padding, and forward it to ReLU layer as done for the BRNN output.
3.3

Combination of CNN, RNN, and Coherence Model

After the ReLU activation layers of the RNN and the coherence model, their output (i.e. {ot }N
t=1 and
q) goes through two fully connected (FC) layers, whose role is to decide a proper combination of the
BRNN language factors and the coherence factors. We drop the bias terms for the fully-connected
layers, and the dimensions of variables are Wf 1 ∈ R512×300 , Wf 2 ∈ R4,096×512 , ot , q ∈ R300×1 ,
st , g ∈ R4,096×1 , O ∈ R300×N , and S ∈ R4,096×N .
O = [o1 |o2 |..|oN ];

S = [s1 |s2 |..|sN ];

Wf 2 Wf 1 [O|q] = [S|g].

(2)

We use the shared parameters for O and q so that the output mixes well the interaction between the
content flows and coherency. In our tests, joint learning outperforms learning the two terms with
separate parameters. Note that the multiplication Wf 2 Wf 1 of the last two FC layers does not reduce
to a single linear mapping, thanks to dropout. We assign 0.5 and 0.7 dropout rates to the two layers.
Empirically, it improves generalization performance much over a single FC layer with dropout.
3.4

Training the CRCN

To train our CRCN model, we first define the compatibility score between an image stream and a
paragraph sequence. While our score function is inspired by Karpathy et al. [9], there are two major
differences. First, the score function of [9] deals between sentence fragments and image fragments,
and thus the algorithm considers all combinations between them to find out the best matching. On
the other hand, we define the score by an ordered and paired compatibility between a sentence
sequence and an image sequence. Second, we also add the term that measures the relevance relation
of coherency between an image sequence and a text sequence. Finally, the score Skl for a sentence
sequence k and an image stream l is defined by
Skl =

X

skt · vtl + g k · vtl

(3)

t=1...N

where vtl denotes the CNN feature vector for t-th image of stream l. We then define the cost function
to train our CRCN model as follows [9].
C(θ) =

XhX
k

max(0, 1 + Skl − Skk ) +

l

X

i
max(0, 1 + Slk − Skk ) ,

(4)

l

where Skk denotes the score between a training pair of corresponding image and sentence sequence.
The objective, based on the max-margin structured loss, encourages aligned image-sentence sequence pairs to have a higher score by a margin than misaligned pairs. For each positive training
example, we randomly sample 100 ne examples from the training set. Since each contrastive example has a random length, and is sampled from the dataset of a wide range of content, it is extremely
unlikely that the negative examples have the same length and the same content order of sentences
with positive examples.
Optimization. We use the backpropagation through time (BPTT) algorithm [31] to train our model.
We apply the stochastic gradient descent (SGD) with mini-batches of 100 data streams. Among
many SGD techniques, we select RMSprop optimizer [28], which leads the best performance in
our experiments. We initialize the weights of our CRCN model using the method of He et al. [7],
which is robust in deep rectified models. We observe that it is better than a simple Gaussian random
initialization, although our model is not extremely deep. We use dropout regularization in all layers
except the BRNN, with 0.7 dropout for the last FC layer and 0.5 for the other remaining layers.
5

3.5

Retrieval of Sentence Sequences

At test time, the objective is to retrieve a best sentence sequence for a given query image stream
{Iq1 , · · · , IqN }. First, we select K-nearest images for each query image from training database using the `2 -distance on the CNN VGGNet fc7 features [25]. In our experiments K = 5 is successful.
We then generate a set of sentence sequence candidates C by concatenating the sentences associated
with the K-nearest images at each location t. Finally, we use our learned CRCN model to compute
the compatibility score between the query image stream and each sequence candidate, according to
which we rank the candidates.
However, one major difficulty of this scenario is that there are exponentially many candidates (i.e.
|C| = K N ). To resolve this issue, we use an approximate divide-and-conquer strategy; we recursively halve the problem into subproblems, until the size of the subproblem is manageable. For
example, if we halve the search candidate length Q times, then the search space of each subproblem
Q
becomes K N/2 . Using the beam search idea, we first find the top-M best sequence candidates in
the subproblem of the lowest level, and recursively increase the candidate lengths while the maximum candidate size is limited to M . We set M = 50. Though it is an approximate search, our
experiments assure that it achieves almost optimal solutions with plausible combinatorial search,
mainly because the local fluency and coherence is undoubtedly necessary for the global one. That
is, in order for a whole sentence sequence to be fluent and coherent, its any subparts must be as well.

4

Experiments

We compare the performance of our approach with other state-of-the-art candidate methods via
quantitative measures and user studies using Amazon Mechanical Turk (AMT). Please refer to the
supplementary material for more results and the details of implementation and experimental setting.
4.1

Experimental Setting

Dataset. We collect blog datasets of the two topics: NYC and Disneyland. We reuse the blog data
of Disneyland from the dataset of [11], and newly collect the data of NYC, using the same crawling
method with [11], in which we first crawl blog posts and their associated pictures from two popular
blog publishing sites, BLOGSPOT and WORDPRESS by changing query terms from Google search.
Then, we manually select the travelogue posts that describe stories and events with multiple images.
Finally, the dataset includes 11,861 unique blog posts and 78,467 images for NYC and 7,717 blog
posts and 60,545 images for Disneyland.
Task. For quantitative evaluation, we randomly split our dataset into 80% as a training set, 10% as
a validation, and the others as a test set. For each test post, we use the image sequence as a query
Iq and the sequence of summarized sentences as groundtruth TG . Each algorithm retrieves the best
sequences from training database for a query image sequence, and ideally the retrieved sequences
match well with TG . Since the training and test data are disjoint, each algorithm can only retrieve
similar (but not identical) sentences at best.
For quantitative measures, we exploit two types of metrics of language similarity (i.e. BLEU [22],
CIDEr [29], and METEOR [16] scores) and retrieval accuracies (i.e. top-K recall and median rank),
which are popularly used in text generation literature [8, 9, 19, 26]. The top-K recall R@K is
the recall rate of a groundtruth retrieval given top K candidates, and the median rank indicates the
median ranking value of the first retrieved groundtruth. A better performance is indicated by higher
BLEU, CIDEr, METEOR, R@K scores, and lower median rank values.
Baselines. Since the sentence sequence generation from image streams has not been addressed yet
in previous research, we instead extend several state-of-the-art single-sentence models that have
publicly available codes as baselines, including the log-bilinear multimodal models by Kiros et
al. [12], and recurrent convolutional models by Karpathy et al. [9] and Vinyals et al. [30]. For
[12], we use the three variants introduced in the paper, which are the standard log-bilinear model
(LBL), and two multi-modal extensions: modality-based LBL (MLBL-B) and factored three-way
LBL (MLBL-F). We use the NeuralTalk package authored by Karpathy et al. for the baseline
of [9] denoted by (CNN+RNN), and [30] denoted by (CNN+LSTM). As the simplest baseline, we
also compare with the global matching (GloMatch) in [21]. For all the baselines, we create final
sentence sequences by concatenating the sentences generated for each image in the query stream.
6

B-1

B-2

(CNN+LSTM) [30]
(CNN+RNN) [9]
(MLBL-F) [12]
(MLBL-B) [12]
(LBL) [12]
(GloMatch) [21]
(1NN)
(RCN)
(CRCN)

21.31
6.21
21.03
20.43
20.96
19.00
25.97
27.09
26.83

3.65
0.01
1.92
1.54
1.68
1.59
3.42
5.45
5.37

(CNN+LSTM) [30]
(CNN+RNN) [9]
(MLBL-F) [12]
(MLBL-B) [12]
(LBL) [12]
(GloMatch) [21]
(1NN)
(RCN)
(CRCN)

27.99
6.04
15.75
15.65
18.94
11.94
25.92
28.15
28.40

3.55
0.00
1.61
1.32
1.70
0.37
3.34
6.84
6.88

Language metrics
Retrieval metrics
B-3 B-4 CIDEr METEOR R@1 R@5 R@10 MedRank
New York City
0.57 0.14
9.1
5.73
0.95 5.24
8.57
84.5
0.00 0.00
0.5
1.34
0.48 2.86
4.29
120.5
0.12 0.01
4.3
6.03
0.71 4.52
7.86
87.0
0.09 0.01
2.6
5.30
0.48 3.57
5.48
101.5
0.08 0.01
2.6
5.29
1.19 4.52
7.38
100.5
0.04 0.0
2.80
5.17
0.24 2.62
4.05
95.00
0.60 0.22 15.9
7.06
5.95 13.57 20.71
63.50
2.56 2.10 33.5
7.87
3.80 18.33 30.24
29.00
2.57 2.08 30.9
7.69
11.67 31.19 43.57
14.00
Disneyland
0.38 0.08 10.0
4.51
3.06 8.16 14.29
65.0
0.00 0.00
0.4
1.34
1.02 3.40
5.78
88.0
0.07 0.01
4.9
7.12
0.68 4.08 10.54
63.0
0.05 0.00
3.8
5.83
0.34 2.72
6.80
69.0
0.06 0.01
3.4
4.99
1.02 4.08
7.82
62.0
0.01 0.00
2.2
4.31
2.04 5.78
7.48
73.0
0.71 0.38 19.5
7.46
9.18 19.05 27.21
45.0
4.11 3.52 51.3
8.87
5.10 20.07 28.57
29.5
4.11 3.49 52.7
8.78
14.29 31.29 43.20
16.0

Table 1: Evaluation of sentence generation for the two datasets, New York City and Disneyland, with language
similarity metrics (BLEU) and retrieval metrics (R@K, median Rank). A better performance is indicated by
higher BLEU, CIDEr, METEOR, R@K scores, and lower median rank values.

We also compare between different variants of our method to validate the contributions of key components of our method. We test the K-nearest search (1NN) without the RNN part as the simplest
variant; for each image in a test query, we find its K(= 1) most similar training images and simply
concatenate their associated sentences. The second variant is the BRNN-only method denoted by
(RCN) that excludes the entity-based coherence model from our approach. Our complete method is
denoted by (CRCN), and this comparison quantifies the improvement by the coherence model. To be
fair, we use the same VGGNet fc7 feature [25] for all the algorithms.
4.2

Quantitative Results

Table 1 shows the quantitative results of experiments using both language and retrieval metrics.
Our approach (CRCN) and (RCN) outperform, with large margins, other state-of-the-art baselines,
which generate passages without consideration of sentence-to-sentence transitions unlike ours. The
(MLBL-F) shows the best performance among the three models of [12] albeit with a small margin,
partly because they share the same word dictionary in training. Among mRNN-based models, the
(CNN+LSTM) significantly outperforms the (CNN+RNN), because the LSTM units help learn models
from irregular and lengthy data of natural blogs more robustly.
We also observe that (CRCN) outperforms (1NN) and (RCN), especially with the retrieval metrics.
It shows that the integration of two key components, the BRNN and the coherence model, indeed
contributes the performance improvement. The (CRCN) is only slightly better than the (RCN) in language metrics but significantly better in retrieval metrics. It means that (RCN) is fine with retrieving
fairly good solutions, but not good at ranking the only correct solution high compared to (CRCN).
The small margins in language metrics are also attributed by their inherent limitation; for example,
the BLEU focuses on counting the matches of n-gram words and thus is not good at comparing
between sentences, even worse between paragraphs for fully evaluating their fluency and coherency.
Fig.3 illustrates several examples of sentence sequence retrieval. In each set, we show a query
image stream and text results created by our method and baselines. Except Fig.3.(d), we show parts
of sequences because they are rather long for illustration. These qualitative examples demonstrate
that our approach is more successful to verbalize image sequences that include a variety of content.
4.3

User Studies via Amazon Mechanical Turk

We perform user studies using AMT to observe general users’ preferences between text sequences
by different algorithms. Since our evaluation involves multiple images and long passages of text, we
design our AMT task to be sufficiently simple for general turkers with no background knowledge.
7

Figure 3: Examples of sentence sequence retrieval for NYC (top) and Disneyland (bottom). In each set, we
present a part of a query image stream, and its corresponding text output by our method and a baseline.
Baselines
(GloMatch)
(CNN+LSTM)
(MLBL-B)
(RCN)
(RCN N>=8)
NYC
92.7% (139/150) 80.0% (120/150) 69.3% (104/150) 54.0% (81/150) 57.0% (131/230)
Disneyland 95.3% (143/150) 82.0% (123/150) 70.7% (106/150) 56.0% (84/150) 60.1% (143/238)

Table 2: The results of AMT pairwise preference tests. We present the percentages of responses that turkers
vote for our (CRCN) over baselines. The length of query streams is 5 except the last column, which has 8–10.

We first randomly sample 100 test streams from the two datasets. We first set the maximum number
of images per query to 5. If a query is longer than that, we uniformly sample it to 5. In an AMT
test, we show a query image stream Iq , and a pair of passages generated by our method (CRCN) and
one baseline in a random order. We ask turkers to choose more agreed text sequence with Iq . We
design the test as a pairwise comparison instead of a multiple-choice question to make answering
and analysis easier. The questions look very similar to the examples of Fig.3. We obtain answers
from three different turkers for each query. We compare with four baselines; we choose (MLBL-B)
among the three variants of [12], and (CNN+LSTM) among mRNN-based methods. We also select
(GloMatch), and (RCN) as the variants of our method.
Table 2 shows the results of AMT tests, which validate that AMT annotators prefer our results to
those of baselines. The (GloMatch) is the worst because it uses too weak image representation
(i.e. GIST and Tiny images). The differences between (CRCN) and (RCN) (i.e. 4th column of Table
2) are not as significant as previous quantitative measures, mainly because our query image stream
is sampled to relatively short 5. The coherence becomes more critical as the passage is longer. To
justify this argument, we run another set of AMT tests in which we use 8–10 images per query. As
shown in the last column of Table 2, the performance margins between (CRCN) and (RCN) become
larger as the lengths of query image streams increase. This result assures that as passages are longer,
the coherence becomes more important, and thus (CRCN)’s output is more preferred by turkers.

5

Conclusion

We proposed an approach for retrieving sentence sequences for an image stream. We developed coherence recurrent convolutional network (CRCN), which consists of convolutional networks, bidirectional recurrent networks, and entity-based local coherence model. With quantitative evaluation
and users studies using AMT on large collections of blog posts, we demonstrated that our CRCN
approach outperformed other state-of-the-art candidate methods.
Acknowledgements. This research is partially supported by Hancom and Basic Science Research
Program through National Research Foundation of Korea (2015R1C1A1A02036562).
8

References
[1] R. Barzilay and M. Lapata. Modeling Local Coherence: An Entity-Based Approach. In ACL, 2008.
[2] S. Bird, E. Loper, and E. Klein. Natural Language Processing with Python. O’Reilly Media Inc., 2009.
[3] X. Chen and C. L. Zitnick. Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation.
In CVPR, 2015.
[4] F. Y. Y. Choi, P. Wiemer-Hastings, and J. Moore. Latent Semantic Analysis for Text Segmentation. In
EMNLP, 2001.
[5] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell.
Long-term Recurrent Convolutional Networks for Visual Recognition and Description. In CVPR, 2015.
[6] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik. Improving Image-Sentence Embeddings
Using Large Weakly Annotated Photo Collections. In ECCV, 2014.
[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance
on ImageNet Classification. In arXiv, 2015.
[8] M. Hodosh, P. Young, and J. Hockenmaier. Framing Image Description as a Ranking Task: Data, Models
and Evaluation Metrics. JAIR, 47:853–899, 2013.
[9] A. Karpathy and L. Fei-Fei. Deep Visual-Semantic Alignments for Generating Image Descriptions. In
CVPR, 2015.
[10] G. Kim, S. Moon, and L. Sigal. Joint Photo Stream and Blog Post Summarization and Exploration. In
CVPR, 2015.
[11] G. Kim, S. Moon, and L. Sigal. Ranking and Retrieval of Image Sequences from Multiple Paragraph
Queries. In CVPR, 2015.
[12] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal Neural Language Models. In ICML, 2014.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet Classification with Deep Convolutional Neural
Networks. In NIPS, 2012.
[14] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby Talk: Understanding
and Generating Image Descriptions. In CVPR, 2011.
[15] P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi. TreeTalk: Composition and Compression of Trees
for Image Descriptions. In TACL, 2014.
[16] S. B. A. Lavie. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with
Human Judgments. In ACL, 2005.
[17] Q. Le and T. Mikolov. Distributed Representations of Sentences and Documents. In ICML, 2014.
[18] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. The Stanford CoreNLP
Natural Language Processing Toolkit. In ACL, 2014.
[19] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. L. Yuille. Deep Captioning with Multimodal Recurrent
Neural Networks (m-RNN). In ICLR, 2015.
[20] T. Mikolov. Statistical Language Models based on Neural Networks. In Ph. D. Thesis, Brno University
of Technology, 2012.
[21] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2Text: Describing Images Using 1 Million Captioned Photographs. In NIPS, 2011.
[22] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: A Method for Automatic Evaluation of Machine
Translation. In ACL, 2002.
[23] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele. Translating Video Content to Natural
Language Descriptions. In ICCV, 2013.
[24] M. Schuster and K. K. Paliwal. Bidirectional Recurrent Neural Networks. In IEEE TSP, 1997.
[25] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition.
In ICLR, 2015.
[26] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. Grounded Compositional Semantics for
Finding and Describing Images with Sentences. In TACL, 2013.
[27] N. Srivastava and R. Salakhutdinov. Multimodal Learning with Deep Boltzmann Machines. In NIPS,
2012.
[28] T. Tieleman and G. E. Hinton. Lecture 6.5 – RMSProp. In Coursera, 2012.
[29] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In
arXiv:1411.5726, 2014.
[30] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and Tell: A Neural Image Caption Generator. In
CVPR, 2015.
[31] P. J. Werbos. Generalization of Backpropagation with Application to a Recurrent Gas Market Model.
Neural Networks, 1:339–356, 1988.
[32] R. Xu, C. Xiong, W. Chen, and J. J. Corso. Jointly Modeling Deep Video and Compositional Text to
Bridge Vision and Language in a Unified Framework. In AAAI, 2015.

9

"
9,5814,Parallel Correlation Clustering on Big Graphs,Poster,5814-parallel-correlation-clustering-on-big-graphs.pdf,"Given a similarity graph between items, correlation clustering (CC) groups similar items together and dissimilar ones apart. One of the most popular CC algorithms is KwikCluster:  an algorithm that serially clusters neighborhoods of vertices, and obtains a 3-approximation ratio. Unfortunately, in practice KwikCluster requires a large number of clustering rounds, a potential bottleneck for large graphs.We present C4 and ClusterWild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds, and provably achieve nearly linear speedups. C4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio. ClusterWild! is a coordination free algorithm that abandons consistency for the benefit of better scaling; this leads to a provably small loss in the 3 approximation ratio.We provide extensive experimental results for both algorithms,  where we outperform the state of the art, both in terms of clustering accuracy and running time. We show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while achieving a 15x speedup.","Parallel Correlation Clustering on Big Graphs
Xinghao Pan↵,✏ , Dimitris Papailiopoulos↵,✏ , Samet Oymak↵,✏ ,
Benjamin Recht↵,✏, , Kannan Ramchandran✏ , and Michael I. Jordan↵,✏,
↵
AMPLab, ✏ EECS at UC Berkeley, Statistics at UC Berkeley

Abstract
Given a similarity graph between items, correlation clustering (CC) groups similar
items together and dissimilar ones apart. One of the most popular CC algorithms
is KwikCluster: an algorithm that serially clusters neighborhoods of vertices, and
obtains a 3-approximation ratio. Unfortunately, in practice KwikCluster requires
a large number of clustering rounds, a potential bottleneck for large graphs.
We present C4 and ClusterWild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds, and provably achieve nearly
linear speedups. C4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio. ClusterWild! is
a coordination free algorithm that abandons consistency for the benefit of better
scaling; this leads to a provably small loss in the 3 approximation ratio.
We demonstrate experimentally that both algorithms outperform the state of the
art, both in terms of clustering accuracy and running time. We show that our
algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while
achieving a 15⇥ speedup.

1

Introduction

Clustering items according to some notion of similarity is a major primitive in machine learning.
Correlation clustering serves as a basic means to achieve this goal: given a similarity measure
between items, the goal is to group similar items together and dissimilar items apart. In contrast to
other clustering approaches, the number of clusters is not determined a priori, and good solutions
aim to balance the tension between grouping all items together versus isolating them.
The simplest CC variant can be described on a
complete signed graph. Our input is a graph
G on n vertices, with +1 weights on edges between similar items, and 1 edges between dissimilar ones. Our goal is to generate a partition
of vertices into disjoint sets that minimizes the
number of disagreeing edges: this equals the
number of “+” edges cut by the clusters plus
the number of “ ” edges inside the clusters.
This metric is commonly called the number of
disagreements. In Figure 1, we give a toy example of a CC instance.

cluster 1

cluster 2

cost = (#“ ” edges inside clusters) + (#“+” edges across clusters) = 2

Figure 1: In the above graph, solid edges denote similarity and dashed dissimilarity. The number of disagreeing edges in the above clustering clustering is 2; we
color the bad edges with red.

Entity deduplication is the archetypal motivating example for correlation clustering, with applications in chat disentanglement, co-reference resolution, and spam detection [1, 2, 3, 4, 5, 6]. The
input is a set of entities (say, results of a keyword search), and a pairwise classifier that indicates—
with some error—similarities between entities. Two results of a keyword search might refer to the
same item, but might look different if they come from different sources. By building a similarity
1

graph between entities and then applying CC, the hope is to cluster duplicate entities in the same
group; in the context of keyword search, this implies a more meaningful and compact list of results.
CC has been further applied to finding communities in signed networks, classifying missing edges
in opinion or trust networks [7, 8], gene clustering [9], and consensus clustering [3].
KwikCluster is the simplest CC algorithm that achieves a provable 3-approximation ratio [10], and
works in the following way: pick a vertex v at random (a cluster center), create a cluster for v
and its positive neighborhood N (v) (i.e., vertices connected to v with positive edges), peel these
vertices and their associated edges from the graph, and repeat until all vertices are clustered. Beyond
its theoretical guarantees, experimentally KwikCluster performs well when combined with local
heuristics [3].
KwikCluster seems like an inherently sequential algorithm, and in most cases of interest it requires
many peeling rounds. This happens because a small number of vertices are clustered per round. This
can be a bottleneck for large graphs. Recently, there have been efforts to develop scalable variants
of KwikCluster [5, 6]. In [6] a distributed peeling algorithm was presented in the context of MapReduce. Using an elegant analysis, the authors establish a (3 + ✏)-approximation in a polylogarithmic
number of rounds. The algorithm employs a simple step that rejects vertices that are executed in
parallel but are “conflicting”; however, we see in our experiments, this seemingly minor coordination step hinders scale-ups in a parallel core setting. In [5], a sketch of a distributed algorithm was
presented. This algorithm achieves the same approximation as KwikCluster, in a logarithmic number of rounds, in expectation. However, it performs significant redundant work, per iteration, in its
effort to detect in parallel which vertices should become cluster centers.
Our contributions We present C4 and ClusterWild!, two parallel CC algorithms with provable
performance guarantees, that in practice outperform the state of the art, both in terms of running
time and clustering accuracy. C4 is a parallel version of KwikCluster that uses concurrency control
to establish a 3-approximation ratio. ClusterWild! is a simple to implement, coordination-free
algorithm that abandons consistency for the benefit of better scaling, while having a provably small
loss in the 3 approximation ratio.
C4 achieves a 3 approximation ratio, in a poly-logarithmic number of rounds, by enforcing consistency between concurrently running peeling threads. Consistency is enforced using concurrency
control, a notion extensively studied for databases transactions, that was recently used to parallelize
inherently sequential machine learning algorithms [11].
ClusterWild! is a coordination-free parallel CC algorithm that waives consistency in favor of speed.
The cost we pay is an arbitrarily small loss in ClusterWild!’s accuracy. We show that ClusterWild!
achieves a (3 + ✏)OPT + O(✏ · n · log2 n) approximation, in a poly-logarithmic number of rounds,
with provable nearly linear speedups. Our main theoretical innovation for ClusterWild! is analyzing
the coordination-free algorithm as a serial variant of KwikCluster that runs on a “noisy” graph.
In our experimental evaluation, we demonstrate that both algorithms gracefully scale up to graphs
with billions of edges. In these large graphs, our algorithms output a valid clustering in less than
5 seconds, on 32 threads, up to an order of magnitude faster than KwikCluster. We observe how,
not unexpectedly, ClusterWild! is faster than C4, and quite surprisingly, abandoning coordination in
this parallel setting, only amounts to a 1% of relative loss in the clustering accuracy. Furthermore,
we compare against state of the art parallel CC algorithms, showing that we consistently outperform
these algorithms in terms of both running time and clustering accuracy.
Notation G denotes a graph with n vertices and m edges. G is complete and only has ±1 edges.
We denote by dv the positive degree of a vertex, i.e., the number of vertices connected to v with
positive edges. denotes the positive maximum degree of G, and N (v) denotes the positive neighborhood of v; moreover, let Cv = {v, N (v)}. Two vertices u, v are termed as “friends” if u 2 N (v)
and vice versa. We denote by ⇡ a permutation of {1, . . . , n}.

2

2

Two Parallel Algorithms for Correlation Clustering

The formal definition of correlation clustering is given below.
Correlation Clustering. Given a graph G on n vertices, partition the vertices into an arbitrary
number k of disjoint subsets C1 , . . . , Ck such that the sum of negative edges within the subsets plus
the sum of positive edges across the subsets is minimized:
OPT = min

1kn

min

Ci \Cj =0,8i6=j

[k
i=1 Ci ={1,...,n}

k
X
i=1

E (Ci , Ci ) +

k
k
X
X

i=1 j=i+1

E + (Ci , Cj )

where E + and E are the sets of positive and negative edges in G.
KwikCluster is a remarkably simple algorithm that approximately solves the above combinatorial
problem, and operates as follows. A random vertex v is picked, a cluster Cv is created with v and
its positive neighborhood, then the vertices in Cv are peeled from the graph, and this process is
repeated until all vertices are clustered KwikCluster can be equivalently executed, as noted by [5], if
we substitute the random choice of a vertex per peeling round, with a random order ⇡ preassigned to
vertices, (see Alg. 1). That is, select a random permutation on vertices, then peel the vertex indexed
by ⇡(1), and its friends. Remove from ⇡ the vertices in Cv and repeat this process. Having an order
among vertices makes the discussion of parallel algorithms more convenient.
C4: Parallel CC using Concurency Control. Algorithm 1 KwikCluster with ⇡
Suppose we now wish to run a parallel version
of KwikCluster, say on two threads: one thread 1: ⇡ = a random permutation of {1, . . . , n}
picks vertex v indexed by ⇡(1) and the other 2: while V 6= ; do
thread picks u indexed by ⇡(2), concurrently. 3: select the vertex v indexed by ⇡(1)
4: Cv = {v, N (v)}
Can both vertices be cluster centers? They can, 5: Remove
clustered vertices from G and ⇡
iff they are not friends in G. If v and u are con- 6: end while
nected with a positive edge, then the vertex with
the smallest order wins. This is our concurency
rule no. 1. Now, assume that v and u are not friends in G, and both v and u become cluster centers.
Moreover, assume that v and u have a common, unclustered friend, say w: should w be clustered
with v, or u? We need to follow what would happen with KwikCluster in Alg. 1: w will go with
the vertex that has the smallest permutation number, in this case v. This is concurency rule no. 2.
Following the above simple rules, we develop C4, our serializable parallel CC algorithm. Since, C4
constructs the same clusters as KwikCluster (for a given ordering ⇡), it inherits its 3 approximation.
The above idea of identifying the cluster centers in rounds was first used in [12] to obtain a parallel
algorithm for maximal independent set (MIS).
C4, shown as Alg. 2, starts by assigning a random permutation ⇡ to the vertices, it then samples an
active set A of n unclustered vertices; this sample is taken from the prefix of ⇡. After sampling
A, each of the P threads picks a vertex with the smallest order in A, then checks if that vertex can
become a cluster center. We first enforce concurrency rule no. 1: adjacent vertices cannot be cluster
centers at the same time. C4 enforces it by making each thread check the friends of the vertex, say
v, that is picked from A. A thread will check in attemptCluster whether its vertex v has any
preceding friends that are cluster centers. If there are none, it will go ahead and label v as cluster
center, and proceed with creating a cluster. If a preceding friend of v is a cluster center, then v is
labeled as not being a cluster center. If a preceding friend of v, call it u, has not yet received a
label (i.e., u is currently being processed and is not yet labeled as cluster center or not), then the
thread processing v, will wait on u to receive a label. The major technical detail is in showing that
this wait time is bounded; we show that no more than O(log n) threads can be in conflict at the
same time, using a new subgraph sampling lemma [13]. Since C4 is serializable, it has to respect
concurrency rule no. 2: if a vertex u is adjacency to two cluster centers, then it gets assigned to the
one with smaller permutation order. This is accomplished in createCluster. After processing
all vertices in A, all threads are synchronized in bulk, the clustered vertices are removed, a new
active set is sampled, and the same process is repeated until everything has been clustered. In the
following section, we present the theoretical guarantees for C4.
3

Algorithm 2 C4 & ClusterWild!
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

createCluster(v):
clusterID(v) = ⇡(v)
for u 2 (v) \ A do
clusterID(u) = min(clusterID(u), ⇡(v))
end for

Input: G, ✏
clusterID(1) = . . . = clusterID(n) = 1
⇡ = a random permutation of {1, . . . , n}
while V 6= ; do
attemptCluster(v):
= maximum vertex degree in G(V )
if clusterID(u) = 1 and isCenter(v) then
A = the first ✏ · n vertices in V [⇡].
createCluster(v)
while A 6= ; do in parallel
end if
v = first element in A
A = A {v}
isCenter(v):
if C4 then // concurrency control
for u 2 (v) do // check friends (in order of ⇡)
attemptCluster(v)
if ⇡(u) < ⇡(v) then // if they precede you, wait
else if ClusterWild! then // coordination free
wait until clusterID(u) 6= 1 // till clustered
createCluster(v)
if isCenter(u) then
end if
return 0 //a friend is center, so you can’t be
end while
end if
Remove clustered vertices from V and ⇡
end if
end while
end for
Output: {clusterID(1), . . . , clusterID(n)}.
return 1 // no earlier friends are centers, so you are

ClusterWild!: Coordination-free Correlation Clustering. ClusterWild! speeds up computation
by ignoring the first concurrency rule. It uniformly samples unclustered vertices, and builds clusters
around all of them, without respecting the rule that cluster centers cannot be friends in G. In ClusterWild!, threads bypass the attemptCluster routine; this eliminates the “waiting” part of C4.
ClusterWild! samples a set A of vertices from the prefix of ⇡. Each thread picks the first ordered
vertex remaining in A, and using that vertex as a cluster center, it creates a cluster around it. It peels
away the clustered vertices and repeats the same process, on the next remaining vertex in A. At
the end of processing all vertices in A, all threads are synchronized in bulk, the clustered vertices
are removed, a new active set is sampled, and the parallel clustering is repeated. A careful analysis along the lines of [6] shows that the number of rounds (i.e., bulk synchronization steps) is only
poly-logarithmic.
Quite unsurprisingly, ClusterWild! is faster than C4. Interestingly, abandoning consistency does not
incur much loss in the approximation ratio. We show how the error introduced in the accuracy of the
solution can be bounded. We characterize this error theoretically, and show that in practice it only
translates to only a relative 1% loss in the objective. The main intuition of why ClusterWild! does
not introduce too much error is that the chance of two randomly selected vertices being friends is
small, hence the concurrency rules are infrequently broken.

3

Theoretical Guarantees

In this section, we bound the number of rounds required for each algorithms, and establish the
theoretical speedup one can obtain with P parallel threads. We proceed to present our approximation
guarantees. We would like to remind the reader that—as in relevant literature—we consider graphs
that are complete, signed, and unweighted. The omitted proofs can be found in the Appendix.
3.1

Number of rounds and running time

Our analysis follows those of [12] and [6]. The main idea is to track how fast the maximum degree
decreases in the remaining graph at the end of each round.
Lemma 1. C4 and ClusterWild! terminate after O 1✏ log n · log
rounds w.h.p.
We now analyze the running time of both algorithms under a simplified BSP model. The main idea
is that the the running time of each “super step” (i.e., round) is determined by the “straggling” thread
(i.e., the one that gets assigned the most amount of work), plus the time needed for synchronization
at the end of each round.
Assumption 1. We assume that threads operate asynchronously within a round and synchronize at
the end of a round. A memory cell can be written/read concurrently by multiple threads. The time
4

spent per round of the algorithm is proportional to the time of the slowest thread. The cost of thread
synchronization at the end of each batch takes time O(P ), where P is the number of threads. The
total computation cost is proportional to the sum of the time spent for all rounds, plus the time spent
during the bulk synchronization step.
Under this simplified model, we show that both algorithms obtain nearly linear speedup, with ClusterWild! being faster than C4, precisely due to lack of coordination. Our main tool for analyzing C4
is a recent graph-theoretic result from [13] (Theorem 1), which guarantees that if one samples an
O(n/ ) subset of vertices in a graph, the sampled subgraph has a connected component of size at
most O(log n). Combining the above, in the appendix we show the following result.
Theorem
2. The ⌘theoretical running
time of C4 on P cores is upper bounded by
⇣⇣
⌘
m+n log n
O
+ P log n · log
as long as the number of cores P is smaller than mini nii ,
P
where nii is the size of the batch in the i-th round of each algorithm. The running time of ClusterWild! on P cores is upper bounded by O m+n
+ P log n · log
.
P
3.2

Approximation ratio

We now proceed with establishing the approximation ratios of C4 and ClusterWild!.
C4 is serializable. It is straightforward that C4 obtains precisely the same approximation ratio as
KwikCluster. One has to simply show that for any permutation ⇡, KwikCluster and C4 will output
the same clustering. This is indeed true, as the two simple concurrency rules mentioned in the
previous section are sufficient for C4 to be equivalent to KwikCluster.
Theorem 3. C4 achieves a 3 approximation ratio, in expectation.
ClusterWild! as a serial procedure on a noisy graph. Analyzing ClusterWild! is a bit more
involved. Our guarantees are based on the fact that ClusterWild! can be treated as if one was
running a peeling algorithm on a “noisy” graph. Since adjacent active vertices can still become
cluster centers in ClusterWild!, one can view the edges between them as “deleted,” by a somewhat
unconventional adversary. We analyze this new, noisy graph and establish our theoretical result.
Theorem 4. ClusterWild! achieves a (3 + ✏)·OPT+O(✏·n·log2 n) approximation, in expectation.
We provide a sketch of the proof, and delegate the details to the appendix. Since ClusterWild!
ignores the edges among active vertices, we treat these edges as deleted. In our main result, we
quantify the loss of clustering accuracy that is caused by ignoring these edges. Before we proceed,
we define bad triangles, a combinatorial structure that is used to measure the clustering quality of a
peeling algorithm.
Definition 1. A bad triangle in G is a set of three vertices, such that two pairs are joined with a
positive edge, and one pair is joined with a negative edge. Let Tb denote the set of bad triangles in
G.
To quantify the cost of ClusterWild!, we make the below observation.
Lemma 5. The cost of any greedy algorithm that picks a vertex v (irrespective of the sampling
order), creates Cv , peels it away and repeats, is equal to the number of bad triangles adjacent to
each cluster center v.
Lemma 6. Let Ĝ denote the random graph induced by deleting all edges between active vertices per
round, for a given run of ClusterWild!, and let ⌧new denote the number of additional bad triangles
thatP
Ĝ has compared to G. Then, the expected cost of ClusterWild! can be upper bounded as
E
t2Tb 1Pt + ⌧new , where Pt is the event that triangle t, with end points i, j, k, is bad, and at
least one of its end points becomes active, while t is still part of the original unclustered graph.
Proof. We begin by bounding the second term E{⌧new }, by considering the number of new bad
triangles ⌧new,i created at each round i:
E {⌧new,i } 

X

(u,v)2E

P(u, v 2 Ai )·|N (u)[N (v)| 

X ✓ ✏ ◆2

(u,v)2E

5

i

·2·

i



2·✏2 ·

E
i



2·✏2 ·n.

Using the result that ClusterWild! terminates after at most O( 1✏ log n log ) rounds, we get that1
E {⌧new }  O(✏ · n · log2 n).
P
P
We are left to bound E
t2Tb 1Pt =
t2Tb pt . To do that we use the following lemma.
P
P
pt
Lemma 7. If pt satisfies 8e,
t:e⇢t2Tb ↵  1, then,
t2Tb pt  ↵ · OP T.

Proof. Let B⇤ be one (of the
many) sets
thatPattribute a +1 in theP
cost of an optimal
P possibly P
P of edges
pt
pt
pt
algorithm. Then, OPT = e2B⇤ 1
e2B⇤
t:e⇢t2Tb ↵ =
t2Tb |B⇤ \ t| ↵
t2Tb ↵ .
| {z }
1

Now, as with [6],
P we will simply have to boundSthe expectation of the bad triangles, adjacent to
an edge (u, v): t:{u,v}⇢t2Tb 1Pt . Let Su,v = {u,v}⇢t2Tb t be the union of the sets of nodes of
the bad triangles that contain both vertices u and v. Observe that if some w 2 S\{u, v} becomes
active before u and v, then a cost of 1 (i.e., the cost of the bad triangle {u, v, w}) is incurred. On
the other hand, if either u or v, or both, are selected as pivots in some round, then Cu,v can be
as high as |S| 2, i.e., at most equal to all bad triangles containing the edge {u, v}. Let Auv =
{u or v are activated before any other vertices in Su,v }. Then,
⇥
⇤
C
E [Cu,v ] = E [ Cu,v | Au,v ] · P(Au,v ) + E Cu,v | AC
u,v · P(Au,v )
 1 + (|S|

2) · P({u, v} \ A =
6 ;|S \ A 6= ;)  1 + 2|S| · P(v \ A =
6 ;|S \ A 6= ;)

where the last inequality is obtained by a union bound over u and v. We now bound the following
probability:
P { v 2 A| S \ A 6= ;} =

P {v 2 A} · P {S \ A 6= ; |v 2 A }
P {v 2 A}
=
=
P {S \ A =
6 ;}
P {S \ A 6= ;}
1

P {v 2 A}
.
P {S \ A = ;}

Observe that P {v 2 A} = ✏ , hence we need to upper bound P {S \ A = ;}. The probability, per
round, that no positive neighbors in S become activated is upper bounded by
""✓
◆ ✓
◆|S|
◆n/P #|S|n/P ✓ ◆|S|n/P
|S| ✓
n |S|
Y
P
P
P
1
P
=
1
 1
=
1

.
n
n
|S|
+
t
n
n
e
P
t=1
Hence, the probability can be upper bounded as

|S|P { v \ A =
6 ;| S \ A =
6 ;} 

✏ · |S|/
.
1 e ✏·|S|/

We know that |S|  2 · + 2 and also ✏  1. Then, 0  ✏ · |S|  ✏ · 2 + 2 n  4 Hence, we have
o
P
4✏
E(Cu,v )  1 + 2 · 1 exp{

t2Tb 1Pt + ⌧new
4✏} . The overall expectation is then bounded by E
⇣

1+2·

4·✏
1 e 4·✏

⌘

· OPT + ✏ · n · ln(n/ ) · log

our approximation ratio for ClusterWild!.
3.3

 (3 + ✏) · OPT + O(✏ · n · log2 n) which establishes

BSP Algorithms as a Proxy for Asynchronous Algorithms

We would like to note that the analysis under the BSP model can be a useful proxy for the performance of completely asynchronous variants of our algorithms. Specifically, see Alg. 3, where we
remove the synchronization barriers.
The only difference between the asynchronous execution in Alg. 3, compared to Alg. 2, is the complete lack of bulk synchronization, at the end of the processing of each active set A. Although the
analysis of the BSP variants of the algorithms is tractable, unfortunately analyzing precisely the
speedup of the asynchronous C4 and the approximation guarantees for the asynchronous ClusterWild! is challenging. However, in our experimental section we test the completely asynchronous
algorithms against the BSP algorithms of the previous section, and observe that they perform quite
similarly both in terms of accuracy of clustering, and running times.
1

We skip the constants to simplify the presentation; however they are all smaller than 10.

6

4

Related Work

Correlation clustering was formally introduced
by Bansal et al. [14]. In the general case, minimizing disagreements is NP-hard and hard to
approximate within an arbitrarily small constant (APX-hard) [14, 15]. There are two variations of the problem: i) CC on complete graphs
where all edges are present and all weights are
±1, and ii) CC on general graphs with arbitrary
edge weights. Both problems are hard, however the general graph setup seems fundamentally harder. The best known approximation ratio for the latter is O(log n), and a reduction to
the minimum multicut problem indicates that
any improvement to that requires fundamental
breakthroughs in theoretical algorithms [16].

Algorithm 3 C4 & ClusterWild!
(asynchronous execution)
1: Input: G
2: clusterID(1) = . . . = clusterID(n) = 1
3: ⇡ = a random permutation of {1, . . . , n}
4: while V 6= ; do
5: v = first element in V
6: V = V {v}
7: if C4 then // concurrency control
8:
attemptCluster(v)
9: else if ClusterWild! then // coordination free
10:
createCluster(v)
11: end if
12: Remove clustered vertices from V and ⇡
13: end while
14: Output: {clusterID(1), . . . , clusterID(n)}.

In the case of complete unweighted graphs, a long series of results establishes a 2.5 approximation
via a rounded linear program (LP) [10]. A recent result establishes a 2.06 approximation using an
elegant rounding to the same LP relaxation [17]. By avoiding the expensive LP, and by just using the
rounding procedure of [10] as a basis for a greedy algorithm yields KwikCluster: a 3 approximation
for CC on complete unweighted graphs.
Variations of the cost metric for CC change the algorithmic landscape: maximizing agreements (the
dual measure of disagreements) [14, 18, 19], or maximizing the difference between the number of
agreements and disagreements [20, 21], come with different hardness and approximation results.
There are also several variants: chromatic CC [22], overlapping CC [23], small number of clusters
CC with added constraints that are suitable for some biology applications [24].
The way C4 finds the cluster centers can be seen as a variation of the MIS algorithm of [12]; the
main difference is that in our case, we “passively” detect the MIS, by locking on memory variables,
and by waiting on preceding ordered threads. This means, that a vertex only “pushes” its cluster
ID and status (cluster center/clustered/unclustered) to its friends, versus “pulling” (or asking) for its
friends’ cluster status. This saves a substantial amount of computational effort.

5

Experiments

Our parallel algorithms were all implemented2 in Scala—we defer a full discussion of the implementation details to Appendix C. We ran all our experiments on Amazon EC2’s r3.8xlarge (32
vCPUs, 244Gb memory) instances, using 1-32 threads. The real graphs listed in Table 1 were each
Graph
DBLP-2011
ENWiki-2013
UK-2005
IT-2004
WebBase-2001

# vertices

# edges

986,324
4,206,785
39,459,925
41,291,594
118,142,155

6,707,236
101,355,853
921,345,078
1,135,718,909
1,019,903,190

Description
2011 DBLP co-authorship network [25, 26, 27].
2013 link graph of English part of Wikipedia [25, 26, 27].
2005 crawl of the .uk domain [25, 26, 27].
2004 crawl of the .it domain [25, 26, 27].
2001 crawl by WebBase crawler [25, 26, 27].

Table 1: Graphs used in the evaluation of our parallel algorithms.
tested with 100 different random ⇡ orderings. We measured the runtimes, speedups (ratio of runtime on 1 thread to runtime on p threads), and objective values obtained by our parallel algorithms.
For comparison, we also implemented the algorithm presented in [6], which we denote as CDK for
short3 . Values of ✏ = 0.1, 0.5, 0.9 were used for C4 BSP, ClusterWild! BSP and CDK. In the interest
of space, we present only representative plots of our results; full results are given in our appendix.
2

Code available at https://github.com/pxinghao/ParallelCorrelationClustering.
CDK was only tested on the smaller graphs of DBLP-2011 and ENWiki-2013, because CDK was prohibitively slow, often 2-3 orders of magnitude slower than C4, ClusterWild!, and even KwikCluster.
3

7

Mean Runtime, UK−2005

Mean runtime / ms

Mean runtime / ms

Mean Runtime, IT−2004

5

10

Serial
C4 As
C4 BSP ε=0.1
CW As
CW BSP ε=0.1

4

10

Mean Speedup, Webbase−2001

Serial
C4 As
C4 BSP ε=0.5
CW As
CW BSP ε=0.5

Ideal
C4 As
C4 BSP ε=0.9
CW As
CW BSP ε=0.9

16
14
12
Speedup

5

10

4

10

10
8
6
4
2

3

1

2

4
8
Number of threads

16

10

32

(a) Mean runtimes, UK-2005, ✏ = 0.1

1

4000

2000

0.08
0.07
% of blocked vertices

Number of rounds

6000

16

0.06

0.2

0.4

0.04
0.03
0.02

0.6

0.8

1

ε

(d)

Mean number of synchronization
rounds for BSP algorithms

5

10

15
20
25
Number of threads

30

10

15
20
25
Number of threads

30

35

(c) Mean speedup, WebBase, ✏ = 0.9
1.12

0.05

0
0

5

Objective Value Relative to Serial, DBLP−2011

C4 BSP ε=0.9 Min
C4 BSP ε=0.9 Mean
C4 BSP ε=0.9 Max
C4 BSP Min
C4 BSP Mean
C4 BSP Max

0.01

0
0

0
0

32

% of Blocked Vertices, ENWiki−2013

C4/CW BSP, UK−2005
C4/CW BSP, IT−2004
C4/CW BSP, Webbase−2001
C4/CW BSP, DBLP−2011
CDK, DBLP−2011
C4/CW BSP, ENWiki−2013
CDK, ENWiki−2013

8000

4
8
Number of threads

(b) Mean runtimes, IT-2004, ✏ = 0.5

Mean Number of Rounds
10000

2

Algo obj value : Serial obj value

10

3

1.1
1.08
1.06
1.04
1.02
1
0

35

(e)

Percent of blocked vertices for C4,
ENWiki-2013. BSP run with ✏ = 0.9.

CW BSP ε=0.9 mean
CW BSP ε=0.9 median
CW As mean
CW As median
CDK ε=0.9 mean
CDK ε=0.9 median

5

10

15
20
25
Number of threads

30

35

(f)

Median objective values, DBLP-2011.
CW BSP and CDK run with ✏ = 0.9

Figure 2: In the above figures, ‘CW’ is short for ClusterWild!, ‘BSP’ is short for the bulk-synchronous variants
of the parallel algorithms, and ‘As’ is short for the asynchronous variants.

Runtimes & Speedups: C4 and ClusterWild! are initially slower than serial, due to the overheads
required for atomic operations in the parallel setting. However, all our parallel algorithms outperform KwikCluster with 3-4 threads. As more threads are added, the asychronous variants become
faster than their BSP counterparts as there are no synchronization barrriers. The difference between
BSP and asychronous variants is greater for smaller ✏. ClusterWild! is also always faster than
C4 since there are no coordination overheads. The asynchronous algorithms are able to achieve a
speedup of 13-15x on 32 threads. The BSP algorithms have a poorer speedup ratio, but nevertheless
achieve 10x speedup with ✏ = 0.9.
Synchronization rounds: The main overhead of the BSP algorithms lies in the need for synchronization rounds. As ✏ increases, the amount of synchronization decreases, and with ✏ = 0.9, our
algorithms have less than 1000 synchronization rounds, which is small considering the size of the
graphs and our multicore setting.
Blocked vertices: Additionally, C4 incurs an overhead in the number of vertices that are blocked
waiting for earlier vertices to complete. We note that this overhead is extremely small in practice—
on all graphs, less than 0.2% of vertices are blocked. On the larger and sparser graphs, this drops to
less than 0.02% (i.e., 1 in 5000) of vertices.
Objective value: By design, the C4 algorithms also return the same output (and thus objective
value) as KwikCluster. We find that ClusterWild! BSP is at most 1% worse than serial across all
graphs and values of ✏. The behavior of asynchronous ClusterWild! worsens as threads are added,
reaching 15% worse than serial for one of the graphs. Finally, on the smaller graphs we were able
to test CDK on, CDK returns a worse median objective value than both ClusterWild! variants.

6

Conclusions and Future Directions

In this paper, we have presented two parallel algorithms for correlation clustering with nearly linear
speedups and provable approximation ratios. Overall, the two approaches support each other—when
C4 is relatively fast relative to ClusterWild!, we may prefer C4 for its guarantees of accuracy, and
when ClusterWild! is accurate relative to C4, we may prefer ClusterWild! for its speed.
In the future, we intend to implement our algorithms in the distributed environment, where synchronization and communication often account for the highest cost. Both C4 and ClusterWild! are
well-suited for the distributed environment, since they have polylogarithmic number of rounds.
8

References
[1] Ahmed K Elmagarmid, Panagiotis G Ipeirotis, and Vassilios S Verykios. Duplicate record detection: A survey. Knowledge and Data
Engineering, IEEE Transactions on, 19(1):1–16, 2007.
[2] Arvind Arasu, Christopher Ré, and Dan Suciu. Large-scale deduplication with constraints using dedupalog. In Data Engineering, 2009.
ICDE’09. IEEE 25th International Conference on, pages 952–963. IEEE, 2009.
[3] Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation clustering beyond ilp. In Proceedings of the
Workshop on Integer Linear Programming for Natural Langauge Processing, pages 19–27. Association for Computational Linguistics,
2009.
[4] Bilal Hussain, Oktie Hassanzadeh, Fei Chiang, Hyun Chul Lee, and Renée J Miller. An evaluation of clustering algorithms in duplicate
detection. Technical report, 2013.
[5] Francesco Bonchi, David Garcia-Soriano, and Edo Liberty. Correlation clustering: from theory to practice. In Proceedings of the 20th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1972–1972. ACM, 2014.
[6] Flavio Chierichetti, Nilesh Dalvi, and Ravi Kumar. Correlation clustering in mapreduce. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining, pages 641–650. ACM, 2014.
[7] Bo Yang, William K Cheung, and Jiming Liu. Community mining from signed social networks. Knowledge and Data Engineering,
IEEE Transactions on, 19(10):1333–1348, 2007.
[8] N Cesa-Bianchi, C Gentile, F Vitale, G Zappella, et al. A correlation clustering approach to link classification in signed networks. In
Annual Conference on Learning Theory, pages 34–1. Microtome, 2012.
[9] Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. Clustering gene expression patterns. Journal of computational biology, 6(3-4):281–297,
1999.
[10] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: ranking and clustering. Journal of the ACM
(JACM), 55(5):23, 2008.
[11] Xinghao Pan, Joseph E Gonzalez, Stefanie Jegelka, Tamara Broderick, and Michael Jordan. Optimistic concurrency control for distributed unsupervised learning. In Advances in Neural Information Processing Systems, pages 1403–1411, 2013.
[12] Guy E Blelloch, Jeremy T Fineman, and Julian Shun. Greedy sequential maximal independent set and matching are parallel on average.
In Proceedings of the twenty-fourth annual ACM symposium on Parallelism in algorithms and architectures, pages 308–317. ACM, 2012.
[13] Michael Krivelevich. The phase transition in site percolation on pseudo-random graphs. arXiv preprint arXiv:1404.5731, 2014.
[14] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. In 2013 IEEE 54th Annual Symposium on Foundations of
Computer Science, pages 238–238. IEEE Computer Society, 2002.
[15] Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative information. In Foundations of Computer
Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pages 524–533. IEEE, 2003.
[16] Erik D Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in general weighted graphs. Theoretical
Computer Science, 361(2):172–187, 2006.
[17] Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near optimal LP rounding algorithm for correlation
clustering on complete and complete k-partite graphs. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of
Computing, STOC ’15, pages 219–228, 2015.
[18] Chaitanya Swamy. Correlation clustering: maximizing agreements via semidefinite programming. In Proceedings of the fifteenth annual
ACM-SIAM symposium on Discrete algorithms, pages 526–527. Society for Industrial and Applied Mathematics, 2004.
[19] Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a fixed number of clusters. In Proceedings of the seventeenth
annual ACM-SIAM symposium on Discrete algorithm, pages 1167–1176. ACM, 2006.
[20] Moses Charikar and Anthony Wirth. Maximizing quadratic programs: extending grothendieck’s inequality. In Foundations of Computer
Science, 2004. Proceedings. 45th Annual IEEE Symposium on, pages 54–60. IEEE, 2004.
[21] Noga Alon, Konstantin Makarychev, Yury Makarychev, and Assaf Naor. Quadratic forms on graphs. Inventiones mathematicae, 163(3):
499–522, 2006.
[22] Francesco Bonchi, Aristides Gionis, Francesco Gullo, and Antti Ukkonen. Chromatic correlation clustering. In Proceedings of the 18th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1321–1329. ACM, 2012.
[23] Francesco Bonchi, Aristides Gionis, and Antti Ukkonen. Overlapping correlation clustering. In Data Mining (ICDM), 2011 IEEE 11th
International Conference on, pages 51–60. IEEE, 2011.
[24] Gregory J Puleo and Olgica Milenkovic. Correlation clustering with constrained cluster sizes and extended weights bounds. arXiv
preprint arXiv:1411.0547, 2014.
[25] P. Boldi and S. Vigna. The WebGraph framework I: Compression techniques. In WWW, 2004.
[26] P. Boldi, M. Rosa, M. Santini, and S. Vigna. Layered label propagation: A multiresolution coordinate-free ordering for compressing
social networks. In WWW. ACM Press, 2011.
[27] P. Boldi, B. Codenotti, M. Santini, and S. Vigna. Ubicrawler: A scalable fully distributed web crawler. Software: Practice & Experience,
34(8):711–726, 2004.

9

"
