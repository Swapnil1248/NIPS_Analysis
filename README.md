# Challenge: NIPS Data Analysis

## Get the data
We will be using the Neural Information Processing Systems (NIPS) 2015 conference papers as a text corpus. These can be obtained off Kaggle, which requires that you create a free account, at [this link](https://www.kaggle.com/benhamner/nips-2015-papers/version/2). An [overview](https://www.kaggle.com/benhamner/nips-2015-papers/version/2/home) talks about the data, which is available as a csv, in raw form from the PDFs, and also already stored into a sqlite db.

## Programming Language
All work should be completed in Python. **Please use Python 3.6+!!**

## Basic Info
Although we are only going to be working with the NIPS dataset, consider that this might be used later on to process a dataset much larger (eg an entire subreddit or all the articles from a news publication). Your ideas should be reusable and scaleable. Final work should be in commandline executable form and should allow the user to start with the files mentioned above and do all the following tasks.

## Tasks
### 1. Generate FastText word embeddings
One of the standard building blocks in NLP is the word embedding. The gold standard used to be word2vec embeddings, however we will be creating FastText embeddings instead due to their ability to deal with out-of-vocabulary words. Ensure that - 
1. `you can support a user-defined number of dimensions` 
2. `have the ability to turn on/off at least one text-preprocessing step`.

### 2. Generate a list of keywords
Within every text corpus, there are certain keywords that are specific to that corpus. Although keywords may normally be monograms, be sure that your methods can identify at least bigrams as well.

### 3. Group the documents
In addition to keywords, a corpus will generally have clusters of topics. Group the documents together (ie output groups of document titles) and identify how they are grouped.

## Additional Info
Logical organization and comments help to make code more understandable and maintainable. Keep this in mind as you write your code! Feel free to use any packages that you would like. Please check all your code into a publicly accessible Github repo and send me an email with link to your work.


### NOTE: Major Consideration
- Design Decisions
- Code pattern
- Documentation
- Testing - Using Visualization

## Details of Implementation

<img src="./images/pipeline.jpg" width="500">

Above diagram represent the data analysis pipeline. It consist of four major steps and an additional step. In the pipeline it can be seen that all the three key tasks can be accomplished independently without dependency on other task and can be independently scaled up or down. All the configurations can be controlled by user for the three tasks. Also, the results generated by one task can be reused in other tasks. There is no tight coupling in any part. Based on this design following tasks can be accomplished with the existing data and the pipeline:

- Text Mining - Text clustering using LDA in step 4
- Categorization - This can be done in step 5 using FastText, result from step 4 will serve as training data for text classification 
- Entity Extraction - Tokens stored in step 2 can be analyzed to indentify entities mentioned in (i.e. Extracting entities such as person, time, place etc.)
- Deep Linguistics - Identifying related words to an input word and finding closeness of the words in step 2
- Aspect Identification - Identifying keywords in the documents in step  in step 3

Following sections describe this pipeline in more details - 

### Input:
Input is in form of a csv file. The file has following colums - 

`['Id', 'Title', 'EventType', 'PdfName', 'Abstract',
       'PaperText', 'AbstractClean', 'PaperTextClean']`
       
Of these only four columns are valuable to current scope of work -
 
`['Id', 'Title', 'Abstract', 'PaperText']`

So I have removed rest of the columns.

### Step 1:
The first step after reading data is to clean the input. This pre-processing step is needed by all the later stages down the line. There are different actions that can be taken - removing punctuations, removing numbers, converting string to lower, removing html tags. User can decide what steps in this pipeline can be skipped by providing run-time arguements. In this implementation a user can enable or disable check for is_html (if data is html then clean html tags) and stemming.

### Step 2: 
After text data is clean, I created a word-vector to generate word-representations for any input word. Word vectorizer generates a fast-text model which is saved and can be reused. I have generated a list of tokens that can be used in down pipeline for `entity-extraction`. To validate the results generated I have genreated graph containing nearest word. This can be done for any set of words. A user can determine the dimension of word-vector with run-time arguement or the model can be used to generate real-time prediction to incoming stream of words. 

### Step 3:
To extract keywords from each text document, fist step is to find candidate keywords. Keywords are words that are noun(unigrams) or contain noun word (bigram). Identification of noun words take a linear scan of all tokens. Once the unigrams have been identified candidate unigrams are ones that occur more than the threshold (this threshold can be configured by user). For each document the unigrams and bigrams are being written in a json file.

There is an arrow from step 3 going back to step 2. This is to show that once the keywords have been identified for a document, the words can be passed to word-vector to see what other words are being used in the corpus for the keywords. This will help to correlate the keywords to similar words in entire corpus

### Step 4:
This step perform clustering of documents based on the common features of documents. Document clustering can be performed in multiple ways - Clutering (k-means, hierarchical) or  LDA. I have chosen LDA for the simplicity but the class can be extended to include k-mean clustering algorithm. The number of clusters are user defined. By default I am looking for 5 clusters and matching the documents based on the content of the paper. Similar documents occur in a cluster as they have similar topic on which they are presenting. The output of the step is the cluster of documents where key is the cluster id and corresponding to that there is a list of similar papers. Lower cluster number tend to cluster more uncommon paper in similar bucket so larger number of clusters (>10) are advised on a larger dataset.

### Step 5: 
This step is extension of step 4. Once the clusters are generated, tags can be assigned to clusters. Once all the documents have been tagged then this dataset can be used to build a fasttext document classifier like one mentioned here -  https://fasttext.cc/docs/en/supervised-tutorial.html . Once the classifier is created new documents can be classified in real-time basis.

## Design consideration - 
1. can extend the cleaning for html data
2. Can save model and reuse it - can perform various functions such as  - closeness of words, antonyms, grouping
3. Keywords identified in task 2 - these can be used to see related words in corpus, we can give these inputs to see related words from vectorizer
4. Clustering can use the words for generating the categories for document classification, in task 3 
5. Based on the common features, we can use the tags to identify related words: The Tags can be grouped under similar words - and these can be used to identify Group similar words
 
