{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import numpy.core.defchararray as npd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en')\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/Papers_sub.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['Id', 'Title', 'EventType', 'PdfName', 'Abstract',\n",
    "       'PaperText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>EventType</th>\n",
       "      <th>PdfName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5677</td>\n",
       "      <td>Double or Nothing: Multiplicative Incentive Me...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5677-double-or-nothing-multiplicative-incentiv...</td>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5941</td>\n",
       "      <td>Learning with Symmetric Label Noise: The Impor...</td>\n",
       "      <td>Spotlight</td>\n",
       "      <td>5941-learning-with-symmetric-label-noise-the-i...</td>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6019</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalization</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6019-algorithmic-stability-and-uniform-general...</td>\n",
       "      <td>One of the central questions in statistical le...</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6035</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6035-adaptive-low-complexity-sequential-infere...</td>\n",
       "      <td>We develop a sequential low-complexity inferen...</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                              Title  EventType  \\\n",
       "0  5677  Double or Nothing: Multiplicative Incentive Me...     Poster   \n",
       "1  5941  Learning with Symmetric Label Noise: The Impor...  Spotlight   \n",
       "2  6019   Algorithmic Stability and Uniform Generalization     Poster   \n",
       "3  6035  Adaptive Low-Complexity Sequential Inference f...     Poster   \n",
       "\n",
       "                                             PdfName  \\\n",
       "0  5677-double-or-nothing-multiplicative-incentiv...   \n",
       "1  5941-learning-with-symmetric-label-noise-the-i...   \n",
       "2  6019-algorithmic-stability-and-uniform-general...   \n",
       "3  6035-adaptive-low-complexity-sequential-infere...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "2  One of the central questions in statistical le...   \n",
       "3  We develop a sequential low-complexity inferen...   \n",
       "\n",
       "                                           PaperText  \n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...  \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...  \n",
       "2  Algorithmic Stability and Uniform Generalizati...  \n",
       "3  Adaptive Low-Complexity Sequential Inference f...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate FastText word embeddings\n",
    "1. support a user-defined number of dimensions \n",
    "2. have the ability to turn on/off at least one text-preprocessing step.\n",
    "\n",
    "### Step 1: Preprocessing data\n",
    "- data be in str format for python 3 \n",
    "space\n",
    "tab\n",
    "vertical tab\n",
    "carriage return\n",
    "formfeed\n",
    "the null character\n",
    "\n",
    "#### text preprocessing guidelines - \n",
    "https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#from pycontractions import Contractions\n",
    "import unicodedata\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(doc, html_stripping=False, accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    #normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    #for doc in corpus:\n",
    "    # strip HTML\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "\n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        doc = remove_accented_chars(doc)\n",
    "\n",
    "    # lowercase the text    \n",
    "    if text_lower_case:\n",
    "        doc = doc.lower()\n",
    "\n",
    "    # remove extra newlines\n",
    "    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "\n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        doc = lemmatize_text(doc)\n",
    "\n",
    "    # remove special characters and\\or digits    \n",
    "    if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "    # remove extra whitespace\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "    # remove stopwords\n",
    "    # http://www.cs.cornell.edu/~xanda/stopwords2017.pdf\n",
    "    if stopword_removal:\n",
    "        doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "\n",
    "    #normalized_corpus.append(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 3: Generate document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "class document_clustering:\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        self.lda_model=None\n",
    "        self.documents_cluster=dict()\n",
    "        self.cluster_word_map=dict()\n",
    "        \n",
    "    def get_document_cluster(self, topic_count=5):\n",
    "        # create a document corpus for LDA\n",
    "        documents =[(i,j) for i,j in zip(self.data.Title, self.data.PaperTextClean)]\n",
    "        \n",
    "        # Make sure to include words which have a minimum length of 3\n",
    "        # NOTE: this is to avoid cases where cleaning removes non alphabetic characters (pg13 -> pg)\n",
    "        document_updated= [[word for word in document[1].lower().split() if len(word)>3 ] for document in documents]\n",
    "\n",
    "        # create list of token\n",
    "        all_tokens = sum(document_updated, [])\n",
    "\n",
    "        # remove words that appear only once\n",
    "        tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "\n",
    "        texts = [[word for word in text if word not in tokens_once] for text in document_updated]\n",
    "\n",
    "        # Create Dictionary for word corpora\n",
    "        id2word = corpora.Dictionary(texts)\n",
    "\n",
    "        # Creates the Bag of Word corpus.\n",
    "        bag_of_words = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        # Trains the LDA models.\n",
    "        self.lda_model = models.ldamodel.LdaModel(corpus=bag_of_words, id2word=id2word, num_topics=topic_count, update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "        # Assigns the topics to the documents in corpus\n",
    "        lda_corpus = self.lda_model[bag_of_words]\n",
    "\n",
    "        # Find threshold for document to be part of cluster, threshold to be 1/#clusters,\n",
    "        # Average the sum of all probabilities:\n",
    "        scores = list(chain(*[[score for topic_id,score in topic] for topic in [doc for doc in lda_corpus]]))\n",
    "        threshold = sum(scores)/len(scores)\n",
    "\n",
    "        # saving the LDA Model\n",
    "        #self.lda_model.save(\"../models/lda_model\")\n",
    "        \n",
    "        # Generate document cluster for each topic\n",
    "        # Document cluster - {cluster_id: [list of documents]}\n",
    "        for i in range(len(document_updated)):\n",
    "            for j in lda_corpus[i]:\n",
    "                if j[1] > threshold:\n",
    "                    key=j[0]\n",
    "                    if key in self.documents_cluster:\n",
    "                        self.documents_cluster[j[0]].append(documents[i][0])\n",
    "                    else:\n",
    "                        self.documents_cluster[j[0]]=[documents[i][0]]      \n",
    "        \n",
    "        # Generate list of words corresponding to cluster\n",
    "        # {cluster_id: [(word, significance of word)]}\n",
    "        self.cluster_word_map={key: self.lda_model.show_topic(key, topn = 10) for key in range(self.lda_model.num_topics)}\n",
    "        #print(self.cluster_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: ['Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing'],\n",
       " 4: ['Learning with Symmetric Label Noise: The Importance of Being Unhinged',\n",
       "  'Planar Ultrametrics for Image Segmentation'],\n",
       " 2: ['Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models',\n",
       "  'Robust Portfolio Optimization',\n",
       "  'Expressing an Image Stream with a Sequence of Natural Sentences'],\n",
       " 1: ['Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling',\n",
       "  'Logarithmic Time Online Multiclass prediction'],\n",
       " 0: ['Parallel Correlation Clustering on Big Graphs']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df[:100]\n",
    "obj=document_clustering(df1)\n",
    "obj.get_document_cluster()\n",
    "obj.documents_cluster\n",
    "#obj.cluster_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
